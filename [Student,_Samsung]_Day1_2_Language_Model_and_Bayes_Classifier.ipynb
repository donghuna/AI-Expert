{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/donghuna/AI-Expert/blob/main/%5BStudent%2C_Samsung%5D_Day1_2_Language_Model_and_Bayes_Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Week 2: Language Model and Bayes Classifier"
      ],
      "metadata": {
        "id": "o6fMwWz3x25T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Language Model\n",
        "\n",
        "A language model is a type of artificial intelligence designed to understand and generate human language. It predicts the probability of a sequence of words, which can be used for various tasks such as text generation, translation, sentiment analysis, and more. In essence, a language model can read and write, making it a powerful tool for understanding and generating text.\n",
        "\n",
        "Contents of this section:\n",
        "*   Practice: Tokenization\n",
        "*   Practice: Language Model\n",
        "\n"
      ],
      "metadata": {
        "id": "JGYMZ3O0yaNa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting Environment\n",
        "Install python packages"
      ],
      "metadata": {
        "id": "TCwJmdMly0bI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "!pip install tokenizers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vNWZVM24zDg-",
        "outputId": "8f16d49e-f1bf-4410-d672-175b4123b3be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('book')"
      ],
      "metadata": {
        "id": "FcVEaVUMzI2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Practice: Tokenization\n",
        "\n",
        "Tokenization is a fundamental step in natural language processing (NLP) that involves breaking down text into smaller units called tokens. These tokens can be words, subwords, or even characters, depending on the specific task and the method used. The process of tokenization helps in structuring the text in a way that it can be easily analyzed and processed by NLP models."
      ],
      "metadata": {
        "id": "nqCbIFSjzLQF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Rule-based Tokenizer\n",
        "The Natural Language Toolkit (NLTK) is a powerful Python library designed for working with human language data, often referred to as Natural Language Processing (NLP). NLTK provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning."
      ],
      "metadata": {
        "id": "rHs1tdok9AbC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text = 'To be, or not to be, that is the question.' # TODO: Observe how the result changes by inputting different strings.\n",
        "tokens = word_tokenize(text)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-GC_qr2fmIL",
        "outputId": "1a753d71-8ad0-4eb0-fbb7-c7bec4a84fcc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['To', 'be', ',', 'or', 'not', 'to', 'be', ',', 'that', 'is', 'the', 'question', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Subword Tokenizer\n",
        "Byte Pair Encoding (BPE) is a data compression technique that iteratively merges the most frequent pairs of bytes (or characters) in a text corpus. This merging process continues until a specified vocabulary size is reached. The primary advantage of BPE is its ability to break down rare or unseen words into smaller, more frequent subword units, thereby reducing the incidence of out-of-vocabulary (OOV) words and enabling better handling of morphologically rich languages."
      ],
      "metadata": {
        "id": "GwsX0OiJ9X8X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter, defaultdict\n",
        "\n",
        "def get_stats(vocab):\n",
        "    pairs = defaultdict(int)\n",
        "    for word, freq in vocab.items():\n",
        "        symbols = ... # TODO: Separate a word into symbols based on spaces\n",
        "        for i in range(len(symbols)-1):\n",
        "            pairs[symbols[i], symbols[i+1]] += freq\n",
        "    return pairs"
      ],
      "metadata": {
        "id": "w8rBsKnic2Up"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = {'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w e s t </w>': 6, 'w i d e s t </w>': 3}\n",
        "num_merges = 12\n",
        "pairs = get_stats(vocab)\n",
        "pairs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_1goSQmsc3qU",
        "outputId": "eaf51064-eee4-4016-bec9-5db53611b437"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(int,\n",
              "            {('l', 'o'): 7,\n",
              "             ('o', 'w'): 7,\n",
              "             ('w', '</w>'): 5,\n",
              "             ('w', 'e'): 8,\n",
              "             ('e', 'r'): 2,\n",
              "             ('r', '</w>'): 2,\n",
              "             ('n', 'e'): 6,\n",
              "             ('e', 'w'): 6,\n",
              "             ('e', 's'): 9,\n",
              "             ('s', 't'): 9,\n",
              "             ('t', '</w>'): 9,\n",
              "             ('w', 'i'): 3,\n",
              "             ('i', 'd'): 3,\n",
              "             ('d', 'e'): 3})"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best = max(pairs, key=pairs.get)\n",
        "best"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ScJYzi1gdLDQ",
        "outputId": "5d0eef20-22fa-4232-94da-ed9990a809db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('e', 's')"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lCt2QFP7dOBB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_vocab(pair, v_in):\n",
        "    '''\n",
        "    Examples:\n",
        "        pair: ['l', 'o']\n",
        "        v_in: {'l o w </w>': 5, 'l o w e r </w>': 2}\n",
        "        -------------------------\n",
        "        v_out: {'lo w </w>': 5, 'lo w e r </w>': 2}\n",
        "    '''\n",
        "    v_out = {}\n",
        "    bigram = ... # TODO: fill here\n",
        "    replacement = ... # TODO: fill here\n",
        "    for word in v_in:\n",
        "        w_out = word.replace(...) # TODO: fill here\n",
        "        v_out[w_out] = v_in[word]\n",
        "    return v_out\n",
        "\n",
        "vocab = {'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w e s t </w>': 6, 'w i d e s t </w>': 3}\n",
        "num_merges = 12\n",
        "for i in range(num_merges):\n",
        "    pairs = get_stats(vocab)\n",
        "    best = max(pairs, key=pairs.get)\n",
        "    vocab = merge_vocab(best, vocab)\n",
        "    print('Merge:', best)\n",
        "    print('Vocab:', vocab)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sS8KWrdxf89i",
        "outputId": "a575114e-3fcc-44a1-e4f7-e89ade23131c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Merge: ('e', 's')\n",
            "Vocab: {'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w es t </w>': 6, 'w i d es t </w>': 3}\n",
            "Merge: ('es', 't')\n",
            "Vocab: {'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w est </w>': 6, 'w i d est </w>': 3}\n",
            "Merge: ('est', '</w>')\n",
            "Vocab: {'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w est</w>': 6, 'w i d est</w>': 3}\n",
            "Merge: ('l', 'o')\n",
            "Vocab: {'lo w </w>': 5, 'lo w e r </w>': 2, 'n e w est</w>': 6, 'w i d est</w>': 3}\n",
            "Merge: ('lo', 'w')\n",
            "Vocab: {'low </w>': 5, 'low e r </w>': 2, 'n e w est</w>': 6, 'w i d est</w>': 3}\n",
            "Merge: ('n', 'e')\n",
            "Vocab: {'low </w>': 5, 'low e r </w>': 2, 'ne w est</w>': 6, 'w i d est</w>': 3}\n",
            "Merge: ('ne', 'w')\n",
            "Vocab: {'low </w>': 5, 'low e r </w>': 2, 'new est</w>': 6, 'w i d est</w>': 3}\n",
            "Merge: ('new', 'est</w>')\n",
            "Vocab: {'low </w>': 5, 'low e r </w>': 2, 'newest</w>': 6, 'w i d est</w>': 3}\n",
            "Merge: ('low', '</w>')\n",
            "Vocab: {'low</w>': 5, 'low e r </w>': 2, 'newest</w>': 6, 'w i d est</w>': 3}\n",
            "Merge: ('w', 'i')\n",
            "Vocab: {'low</w>': 5, 'low e r </w>': 2, 'newest</w>': 6, 'wi d est</w>': 3}\n",
            "Merge: ('wi', 'd')\n",
            "Vocab: {'low</w>': 5, 'low e r </w>': 2, 'newest</w>': 6, 'wid est</w>': 3}\n",
            "Merge: ('wid', 'est</w>')\n",
            "Vocab: {'low</w>': 5, 'low e r </w>': 2, 'newest</w>': 6, 'widest</w>': 3}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NLTK vs. BPE"
      ],
      "metadata": {
        "id": "gc1RHxAW-s2h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from tokenizers import Tokenizer, models, trainers, pre_tokenizers, decoders, processors\n",
        "\n",
        "import re\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "class BPETokenizer:\n",
        "    def __init__(self, vocab_size):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.vocab = {}\n",
        "\n",
        "    def get_stats(self, tokens):\n",
        "        pairs = defaultdict(int)\n",
        "        for token in tokens:\n",
        "            symbols = token.split()\n",
        "            for i in range(len(symbols)-1):\n",
        "                pairs[(symbols[i], symbols[i+1])] += 1\n",
        "        return pairs\n",
        "\n",
        "    def merge_vocab(self, pair, tokens):\n",
        "        new_tokens = []\n",
        "        bigram = ' '.join(pair)\n",
        "        replacement = ''.join(pair)\n",
        "        for token in tokens:\n",
        "            new_token = token.replace(bigram, replacement)\n",
        "            new_tokens.append(new_token)\n",
        "        return new_tokens\n",
        "\n",
        "    def fit(self, text):\n",
        "        words = re.findall(r'\\w+', text)\n",
        "        tokens = [' '.join(list(word)) for word in words]\n",
        "        vocab = Counter(tokens)\n",
        "\n",
        "        while len(self.vocab) < self.vocab_size:\n",
        "            pairs = self.get_stats(vocab)\n",
        "            if not pairs:\n",
        "                break\n",
        "            best = max(pairs, key=pairs.get)\n",
        "            vocab = self.merge_vocab(best, vocab)\n",
        "            self.vocab[''.join(best)] = pairs[best]\n",
        "\n",
        "        self.vocab = dict(sorted(self.vocab.items(), key=lambda item: item[1], reverse=True))\n",
        "\n",
        "    def tokenize(self, word):\n",
        "        word = ' '.join(list(word))\n",
        "        while True:\n",
        "            pairs = self.get_stats([word])\n",
        "            if not pairs:\n",
        "                break\n",
        "            best = max(pairs, key=pairs.get)\n",
        "            bigram = ' '.join(best)\n",
        "            if bigram not in word:\n",
        "                break\n",
        "            word = word.replace(bigram, ''.join(best))\n",
        "        return word.split()\n",
        "\n",
        "text = '''\n",
        "To be, or not to be, that is the question:\n",
        "Whether 'tis nobler in the mind to suffer\n",
        "The slings and arrows of outrageous fortune,\n",
        "Or to take arms against a sea of troubles\n",
        "And by opposing end them. To die: to sleep;\n",
        "No more; and by a sleep to say we end\n",
        "The heart-ache and the thousand natural shocks\n",
        "That flesh is heir to, 'tis a consummation\n",
        "Devoutly to be wish'd. To die, to sleep;\n",
        "To sleep: perchance to dream: ay, there's the rub;\n",
        "For in that sleep of death what dreams may come\n",
        "When we have shuffled off this mortal coil,\n",
        "Must give us pause: there's the respect\n",
        "That makes calamity of so long life;\n",
        "For who would bear the whips and scorns of time,\n",
        "The oppressor's wrong, the proud man's contumely,\n",
        "The pangs of despised love, the law's delay,\n",
        "The insolence of office and the spurns\n",
        "That patient merit of the unworthy takes,\n",
        "When he himself might his quietus make\n",
        "With a bare bodkin? who would fardels bear,\n",
        "To grunt and sweat under a weary life,\n",
        "But that the dread of something after death,\n",
        "The undiscovered country from whose bourn\n",
        "No traveller returns, puzzles the will\n",
        "And makes us rather bear those ills we have\n",
        "Than fly to others that we know not of?\n",
        "Thus conscience does make cowards of us all;\n",
        "And thus the native hue of resolution\n",
        "Is sicklied o'er with the pale cast of thought,\n",
        "And enterprises of great pitch and moment\n",
        "With this regard their currents turn awry,\n",
        "And lose the name of action.--Soft you now!\n",
        "The fair Ophelia! Nymph, in thy orisons\n",
        "Be all my sins remember'd.\n",
        "'''\n",
        "text = ' '.join(text.split())\n",
        "\n",
        "words = word_tokenize(text)\n",
        "print('NLTK Tokenization:', words)\n",
        "\n",
        "tokenizer = BPETokenizer(vocab_size=50)\n",
        "tokenizer.fit(text)\n",
        "words = []\n",
        "for word in re.findall(r'\\w+', text):\n",
        "    words.extend(tokenizer.tokenize(word))\n",
        "print('BPE Tokenization:', words)"
      ],
      "metadata": {
        "id": "eEaFLp_Z8CmO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Practice: Langauge Model\n",
        "In this tutorial, we will explore language models, how they compute the probability of words, and the application of the chain rule in probability. We will also discuss the practical considerations of using uni-gram and n-gram models, including the trade-offs between them. To make things more concrete, we will use examples from Shakespeare's works."
      ],
      "metadata": {
        "id": "IDlk8mpMB2XY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import defaultdict, Counter\n",
        "from matplotlib import pyplot as plt\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "import math\n",
        "import random\n",
        "\n",
        "tokenizer = RegexpTokenizer('[\\w]+')\n",
        "text = nltk.corpus.gutenberg.raw('shakespeare-hamlet.txt').lower()\n",
        "words = tokenizer.tokenize(text)\n",
        "print(words)"
      ],
      "metadata": {
        "id": "5rnZ-rtxs-mx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text[:10]"
      ],
      "metadata": {
        "id": "bOPLV84NHMV8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Markov Assumption and Chain Rule\n",
        "\n",
        "The chain rule of probability states that:\n",
        "\n",
        "\\[ P(A, B, C) = P(A) P(B|A) P(C|A, B) \\]\n",
        "\n",
        "By the Markov assumption, we approximate this to:\n",
        "\n",
        "- Unigram: \\( P(A, B, C) \\approx P(A) P(B) P(C) \\)\n",
        "- Bigram: \\( P(A, B, C) \\approx P(A) P(B|A) P(C|B) \\)\n"
      ],
      "metadata": {
        "id": "c4eNTKjqskPL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Unigram Model\n",
        "\n",
        "In a unigram model, we assume that the probability of each word is independent of the previous words. Therefore, the probability of a sequence of words \\( w_1, w_2, \\ldots, w_n \\) is simply the product of the probabilities of each word:\n",
        "\n",
        "\\[ P(w_1, w_2, \\ldots, w_n) = P(w_1) P(w_2) \\ldots P(w_n) \\]\n",
        "\n"
      ],
      "metadata": {
        "id": "DPg-sSx_sTEV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unigram_pairs = words\n",
        "unigram_counts = Counter(unigram_pairs)\n",
        "unigram_total = # TODO: Total counts of unigram pairs\n",
        "\n",
        "unigram_probs = {pair: ... for pair in unigram_counts} # TODO: fill here\n",
        "sorted_unigram_probs = dict(sorted(unigram_probs.items(), key=lambda item: item[1], reverse=True))\n",
        "df = pd.DataFrame.from_dict(data=sorted_unigram_probs, orient='index')\n",
        "df"
      ],
      "metadata": {
        "id": "-yhte6jptGtR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[0].plot(kind='line', figsize=(8, 4), title=0)\n",
        "plt.gca().spines[['top', 'right']].set_visible(False)"
      ],
      "metadata": {
        "id": "bzQKls9Sx-Z3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def unigram_model(start_word, max_token=100):\n",
        "    result = [start_word]\n",
        "    for _ in range(max_token):\n",
        "        best_pair = # TODO: Select the pair with the highest probability\n",
        "        result.append(best_pair)\n",
        "    return result\n",
        "generated_text = unigram_model('the')\n",
        "print(' '.join(generated_text))"
      ],
      "metadata": {
        "id": "QffDDkYfyC2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bigram Model\n",
        "\n",
        "In a bigram model, we assume that the probability of each word depends only on the previous word. Therefore, the probability of a sequence of words \\( w_1, w_2, \\ldots, w_n \\) is given by:\n",
        "\n",
        "\\[ P(w_1, w_2, \\ldots, w_n) = P(w_1) P(w_2 | w_1) P(w_3 | w_2) \\ldots P(w_n | w_{n-1}) \\]\n"
      ],
      "metadata": {
        "id": "sk6SLyCMscsR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bigram_pairs = []\n",
        "... # TODO: Make bigram pairs (e.g. a b c d -> [(a, b), (b, c), (c, d)])\n",
        "bigram_counts = Counter(bigram_pairs)\n",
        "bigram_total = sum(bigram_counts.values())\n",
        "bigram_probs = {pair: bigram_counts[pair] / bigram_total for pair in bigram_counts}\n",
        "sorted_bigram_probs = dict(sorted(bigram_probs.items(), key=lambda item: item[1], reverse=True))\n",
        "df = pd.DataFrame.from_dict(data=sorted_bigram_probs, orient='index')\n",
        "df"
      ],
      "metadata": {
        "id": "tfj4ypXHtK8D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[0].plot(kind='line', figsize=(8, 4), title=0)\n",
        "plt.gca().spines[['top', 'right']].set_visible(False)"
      ],
      "metadata": {
        "id": "qMbZGLQT3y5J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bigram_model(start_word, max_token=100):\n",
        "    result = [start_word]\n",
        "    for _ in range(max_token):\n",
        "        bigram_subset_probs = ... # TODO: Select a subset of bigram_probs where the first element of the pair matches the last word of the result\n",
        "        best_pair = max(bigram_subset_probs, key=bigram_subset_probs.get)\n",
        "        result.append(best_pair[1])\n",
        "    return result\n",
        "generated_text = bigram_model('the')\n",
        "print(' '.join(generated_text))"
      ],
      "metadata": {
        "id": "VZo9DXQ132k_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### General N-gram Model\n",
        "\n",
        "An n-gram model generalizes this idea by assuming that the probability of each word depends on the previous $ ( n-1 ) $ words. The probability of a sequence of words $ ( w_1, w_2, \\ldots, w_n ) $ in an n-gram model is given by:\n",
        "\n",
        "$ P(w_1, w_2, \\ldots, w_n) = P(w_1) P(w_2 | w_1) P(w_3 | w_1, w_2) \\ldots P(w_n | w_{n-(n-1)}, \\ldots, w_{n-1})$"
      ],
      "metadata": {
        "id": "4gAYiQFxseq9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = 10\n",
        "ngram_pairs = []\n",
        "for i in range(1, len(words)):\n",
        "    ngram_pairs.append((' '.join(words[max(0, i-n+1):i]), words[i]))\n",
        "ngram_counts = Counter(ngram_pairs)\n",
        "ngram_total = sum(ngram_counts.values())\n",
        "ngram_probs = {pair: ngram_counts[pair] / ngram_total for pair in ngram_counts}\n",
        "sorted_ngram_probs = dict(sorted(ngram_probs.items(), key=lambda item: item[1], reverse=True))\n",
        "df = pd.DataFrame.from_dict(data=sorted_ngram_probs, orient='index')\n",
        "df"
      ],
      "metadata": {
        "id": "fljPBS8Qvvtc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[0].plot(kind='line', figsize=(8, 4), title=0)\n",
        "plt.gca().spines[['top', 'right']].set_visible(False)"
      ],
      "metadata": {
        "id": "KI-Rb3HC60la"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ngram_model(start_word, max_token=100, n=10):\n",
        "    result = [start_word]\n",
        "    for _ in range(max_token):\n",
        "        context = ... # Concat the last (n-1) words in the results (with spaces)\n",
        "        ngram_subset_probs = {pair : ngram_probs[pair] for pair in ngram_probs if pair[0] == context}\n",
        "        best_pair = max(ngram_subset_probs, key=ngram_subset_probs.get)\n",
        "        result.append(best_pair[-1])\n",
        "    return result\n",
        "generated_text = ngram_model('the')\n",
        "print(' '.join(generated_text))"
      ],
      "metadata": {
        "id": "ItuyFP3P62yB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Trade-offs of Unigram vs. N-gram Models\n",
        "\n",
        "- **Unigram Model**: Simple and computationally efficient, but ignores context, leading to less accurate predictions.\n",
        "- **Bigram Model**: Considers the immediate context (previous word), providing better predictions than unigram but still limited in capturing longer dependencies.\n",
        "- **N-gram Model**: Captures longer contexts, improving prediction accuracy at the cost of increased computational complexity and data sparsity.\n"
      ],
      "metadata": {
        "id": "2fIcwYU8suKH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## N-gram for email classification"
      ],
      "metadata": {
        "id": "dymzStFmH1fw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet gdown pandas nltk\n",
        "\n",
        "# Connect to Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# download spam/ham dataset\n",
        "import gdown\n",
        "from pathlib import Path\n",
        "\n",
        "url = 'https://drive.google.com/uc?id=1GaUS8wMlWQwqhgCX2wsvhOkQ8_Uw_x2r'\n",
        "dataset_path = dataset_path = Path('/content/drive/MyDrive/spam_dataset.tsv')\n",
        "gdown.download(url, str(dataset_path), quiet=False)"
      ],
      "metadata": {
        "id": "n0ugwMbCH5mK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(dataset_path, sep=\"\\t\")\n",
        "df"
      ],
      "metadata": {
        "id": "JndYzBU1ICpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_X, test_X, train_y, test_y = train_test_split(df[\"text\"],df[\"label_num\"], test_size=0.2, random_state=10)"
      ],
      "metadata": {
        "id": "SquhJWyhIKhD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_X[0]"
      ],
      "metadata": {
        "id": "4FcGA508PfDL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# concat all train_X to make .txt\n",
        "# TODO: {text} {spam/ham} \\n\n",
        "email_text_train = \"\"\n",
        "for x, l in zip(train_X, train_y):\n",
        "  ..."
      ],
      "metadata": {
        "id": "ikqOxQAqPfdk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "email_words_train = tokenizer.tokenize(email_text_train)"
      ],
      "metadata": {
        "id": "242N6HUHPrM9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "email_text_train[:10]"
      ],
      "metadata": {
        "id": "IxIqloHZPuGI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "email_words_train[:3]"
      ],
      "metadata": {
        "id": "ibKUFvvFPvMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# logic\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "STOPWORDS = set(stopwords.words(\"english\"))\n",
        "\n",
        "from string import punctuation\n",
        "\n",
        "PUNCTUATIONS = set(punctuation)\n",
        "\n",
        "def regularize_tokens(tokens):\n",
        "    tokens = [t.strip() for t in tokens]\n",
        "    tokens = [t.strip(\"\".join(PUNCTUATIONS)) for t in tokens]\n",
        "    tokens = [t for t in tokens if len(t) > 1]\n",
        "    tokens = [t for t in tokens if t not in STOPWORDS]\n",
        "    tokens = [t for t in tokens if t not in PUNCTUATIONS]\n",
        "    tokens = [t for t in tokens if not re.match(r\"^\\d+?\\.\\d+?$\", t)]  # e.g., 1.23\n",
        "    tokens = [t for t in tokens if not re.match(r\"^\\d+?\\,\\d+?$\", t)]  # e.g., 1,234\n",
        "    tokens = [t for t in tokens if not t.isnumeric()]  # e.g., 123\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "3ykXT2lrPwhx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "email_words_train = regularize_tokens(email_words_train)"
      ],
      "metadata": {
        "id": "GGt1UH7mPyef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make ngram model\n",
        "..."
      ],
      "metadata": {
        "id": "zSyFxuJAPzbi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df[0].plot(kind='line', figsize=(8, 4), title=0)\n",
        "plt.gca().spines[['top', 'right']].set_visible(False)"
      ],
      "metadata": {
        "id": "HDhPvp7MP2GX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ngram_model(start_words, n=10):\n",
        "    result = start_words\n",
        "    context = ' '.join(result[-n+1:])\n",
        "    ngram_subset_probs = {pair : ngram_probs[pair] for pair in ngram_probs if pair[0] == context}\n",
        "    return ngram_subset_probs"
      ],
      "metadata": {
        "id": "WJgv9o_kP3Lr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "regularized_email_example = ... # TODO: tokenize and regularize\n",
        "regularized_email_example"
      ],
      "metadata": {
        "id": "vODrvBQWP7-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated_text_probs = ngram_model(\n",
        "    regularized_email_example\n",
        ")\n",
        "generated_text_probs"
      ],
      "metadata": {
        "id": "e4GGoyuSQICR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "test_email_preds = []\n",
        "for email_text in tqdm(test_X):\n",
        "  email_words = regularize_tokens(word_tokenize(email_text))\n",
        "  ngram_output = ngram_model(email_words)\n",
        "  # TODO: if ngram output exists && if the highest probability shows spam for next token\n",
        "  #       return spam\n",
        "  #       else\n",
        "  #       return ham\n",
        "  if ...:\n",
        "    if ...:\n",
        "      test_email_preds.append(1)\n",
        "      continue\n",
        "\n",
        "  test_email_preds.append(0)"
      ],
      "metadata": {
        "id": "4A_6NOsTQDNs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate the model\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n"
      ],
      "metadata": {
        "id": "Bw0uCVKFQc6i"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}