{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "JCHY0abx0u0H",
        "3fUBTgWG2gbc",
        "MP0goPBumZeQ",
        "CbjZr3HWmcdI",
        "fyKgwV2qrr9t"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/donghuna/AI-Expert/blob/main/%EC%97%AC%EC%A7%84%EC%98%81/%EC%82%BC%EC%84%B1%EC%8B%A4%EC%8A%B54%EC%9D%BC%EC%B0%A8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prerequisite"
      ],
      "metadata": {
        "id": "JCHY0abx0u0H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0hf5y7FJQ4q",
        "outputId": "f5c84af1-eb9f-4299-84c4-e1023930f40d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/Share/Samsung_share/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fEQC2xhuJZOK",
        "outputId": "50ef0291-aa67-49f4-9bbe-c90d7a6e03ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Share/Samsung_share\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Du1REMGpJekb",
        "outputId": "e407f5da-f888-46ab-c111-8b5753689695"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "imdb.csv  삼성실습4일차.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai\n",
        "!pip install langchain\n",
        "!pip install langchain_openai\n",
        "!pip install langchain_community\n",
        "!pip install datasets\n",
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHHOdZGQXwy3",
        "outputId": "5199e639-7f9a-4f8f-8960-4b604e49f75c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.35.3-py3-none-any.whl (327 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m327.4/327.4 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.7.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.6.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.18.4)\n",
            "Installing collected packages: h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 openai-1.35.3\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.2.5-py3-none-any.whl (974 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m974.6/974.6 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.31)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting langchain-core<0.3.0,>=0.2.7 (from langchain)\n",
            "  Downloading langchain_core-0.2.9-py3-none-any.whl (321 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m321.8/321.8 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.2.1-py3-none-any.whl (23 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
            "  Downloading langsmith-0.1.82-py3-none-any.whl (127 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.4/127.4 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.7.4)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.3.0,>=0.2.7->langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.7->langchain) (24.1)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading orjson-3.10.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.0/145.0 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.18.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.6.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.7->langchain)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Installing collected packages: orjson, jsonpointer, jsonpatch, langsmith, langchain-core, langchain-text-splitters, langchain\n",
            "Successfully installed jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.2.5 langchain-core-0.2.9 langchain-text-splitters-0.2.1 langsmith-0.1.82 orjson-3.10.5\n",
            "Collecting langchain_openai\n",
            "  Downloading langchain_openai-0.1.9-py3-none-any.whl (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.3/40.3 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: langchain-core<0.3,>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from langchain_openai) (0.2.9)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from langchain_openai) (1.35.3)\n",
            "Collecting tiktoken<1,>=0.7 (from langchain_openai)\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2.2->langchain_openai) (6.0.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2.2->langchain_openai) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.75 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2.2->langchain_openai) (0.1.82)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2.2->langchain_openai) (24.1)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2.2->langchain_openai) (2.7.4)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2.2->langchain_openai) (8.4.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.26.0->langchain_openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.26.0->langchain_openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.26.0->langchain_openai) (0.27.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.26.0->langchain_openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.26.0->langchain_openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.26.0->langchain_openai) (4.12.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2.31.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.26.0->langchain_openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.26.0->langchain_openai) (1.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.26.0->langchain_openai) (2024.6.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.26.0->langchain_openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.26.0->langchain_openai) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.2.2->langchain_openai) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2.2->langchain_openai) (3.10.5)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.2.2->langchain_openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.2.2->langchain_openai) (2.18.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (2.0.7)\n",
            "Installing collected packages: tiktoken, langchain_openai\n",
            "Successfully installed langchain_openai-0.1.9 tiktoken-0.7.0\n",
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.2.5-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.0.31)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (3.9.5)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: langchain<0.3.0,>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.2.5)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.7 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.2.9)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.1.82)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (1.25.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (8.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (4.0.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.21.3-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.5->langchain_community) (0.2.1)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.5->langchain_community) (2.7.4)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.7->langchain_community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.7->langchain_community) (24.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain_community) (3.10.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2024.6.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (4.12.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.0.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.7->langchain_community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.5->langchain_community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.5->langchain_community) (2.18.4)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, typing-inspect, dataclasses-json, langchain_community\n",
            "Successfully installed dataclasses-json-0.6.7 langchain_community-0.2.5 marshmallow-3.21.3 mypy-extensions-1.0.0 typing-inspect-0.9.0\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.20.0-py3-none-any.whl (547 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.15.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Collecting pyarrow>=15.0.0 (from datasets)\n",
            "  Downloading pyarrow-16.1.0-cp310-cp310-manylinux_2_28_x86_64.whl (40.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Collecting requests>=2.32.2 (from datasets)\n",
            "  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.5.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.6.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, requests, pyarrow, dill, multiprocess, datasets\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.31.0\n",
            "    Uninstalling requests-2.31.0:\n",
            "      Successfully uninstalled requests-2.31.0\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 14.0.2\n",
            "    Uninstalling pyarrow-14.0.2:\n",
            "      Successfully uninstalled pyarrow-14.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 16.1.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 16.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-2.20.0 dill-0.3.8 multiprocess-0.70.16 pyarrow-16.1.0 requests-2.32.3 xxhash-3.4.1\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.6.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import asyncio\n",
        "import os\n",
        "import json\n",
        "from functools import partial\n",
        "import pandas as pd\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_openai import OpenAI\n",
        "from langchain_community.callbacks import get_openai_callback\n",
        "from langchain_community.chat_models import ChatOpenAI\n",
        "import pprint\n",
        "pp = pprint.PrettyPrinter(indent=4)"
      ],
      "metadata": {
        "id": "3pMyyCsnX8_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# In-context learning"
      ],
      "metadata": {
        "id": "ivbuWF_A08zJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "api_model_name = \"/root/vllm_api/Meta-Llama-3-8B\"\n",
        "\n",
        "llm = OpenAI(\n",
        "            model_name=api_model_name,\n",
        "            openai_api_base=\"http://147.47.208.84:9999/v1\",\n",
        "            openai_api_key=\"EMPTY\",\n",
        "            max_retries=3,\n",
        "            temperature=0.7,\n",
        "            max_tokens=500,\n",
        "        )"
      ],
      "metadata": {
        "id": "fTK8KazPXr3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Playground (w/ pre-trained model)"
      ],
      "metadata": {
        "id": "ZhjBBddT21GY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = llm.invoke(\"Hello World!\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HnW3xSnBYXzC",
        "outputId": "9923f720-4679-4ffa-9e18-486cee282d95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " This is my first post here. I'm hoping to get some good advice from the members here. I'm hoping to start up a small business eventually, but for now I'm just trying to figure out what I can do to earn some extra money. I was thinking of starting a blog, and selling some of the keys I use for my business. I have a lot of them, and I'm not sure how many people would actually buy them from me. I'm hoping that I would be able to make some good money from this business, but I'm not sure if that would be possible. I'm hoping that you guys could give me some good advice on this subject.\n",
            "Hi there and welcome to the forum!\n",
            "Where are the keys you are trying to sell from?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"Hello!\"\n",
        "print(llm.invoke(input_text))"
      ],
      "metadata": {
        "id": "IN5MawqhodGU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "outputId": "2e55b4d4-bd5f-4eb4-c795-dff174d865b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'llm' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-b49a49d2fd13>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0minput_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Hello!\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'llm' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lets get into ICL"
      ],
      "metadata": {
        "id": "3fUBTgWG2gbc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "zero_shot_example = \"\"\"Translate English to Korean:\n",
        "train => \"\"\"\n",
        "print(llm.invoke(zero_shot_example, stop=['\\n']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gzbwo9lV2l21",
        "outputId": "9daabd44-f650-47fb-e1e2-4211af1137e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 전철\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "zero_shot_example2 = \"\"\"train => \"\"\"\n",
        "print(llm.invoke(zero_shot_example2, stop=['\\n']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qO4vtxxkMrZa",
        "outputId": "43ab7749-f7c2-40be-eea1-751af2af2337"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.84027\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "one_shot_example = \"\"\"apple => 사과\n",
        "train =>\"\"\"\n",
        "print(llm.invoke(one_shot_example, stop=['\\n']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_zGb6F8vNAjW",
        "outputId": "c950c057-5c4d-4bb2-f223-94777040cd36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 열차\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "three_shot_example = \"\"\"Translate English to Korean:\n",
        "apple => 사과\n",
        "ruler => 자\n",
        "train =>\"\"\"\n",
        "print(llm.invoke(three_shot_example, stop=['\\n']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TTEgM-j4NYSi",
        "outputId": "c75f51b2-50f9-4e0e-d5f2-b2fd8f71530c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 기차\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentiment classification w/ ICL"
      ],
      "metadata": {
        "id": "LtjVMaRbPB5G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting"
      ],
      "metadata": {
        "id": "MP0goPBumZeQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import random\n",
        "\n",
        "dataset_test = load_dataset('imdb',split='test')\n",
        "df=pd.DataFrame(dataset_test)\n",
        "test_df = df.sample(n=100)"
      ],
      "metadata": {
        "id": "1r1Yl7omPGBF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"data length: {len(test_df)}\")\n",
        "test_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "MBWc_trvQWpe",
        "outputId": "976fb156-0a6c-40e0-b5d5-bf7c2b81b836"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data length: 100\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                    text  label\n",
              "13420  From the start of \"The Edge Of Love\", the view...      1\n",
              "16650  Wow. this movie is the voice of a climbing gen...      1\n",
              "20659  A good, but not great film, \"The Great Dictato...      1\n",
              "17282  The world is going to miss John Frankenheimer....      1\n",
              "20854  Following on directly from the last episode of...      1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ca5ba694-0924-4f7f-ae94-210f0ad442e9\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>13420</th>\n",
              "      <td>From the start of \"The Edge Of Love\", the view...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16650</th>\n",
              "      <td>Wow. this movie is the voice of a climbing gen...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20659</th>\n",
              "      <td>A good, but not great film, \"The Great Dictato...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17282</th>\n",
              "      <td>The world is going to miss John Frankenheimer....</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20854</th>\n",
              "      <td>Following on directly from the last episode of...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ca5ba694-0924-4f7f-ae94-210f0ad442e9')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ca5ba694-0924-4f7f-ae94-210f0ad442e9 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ca5ba694-0924-4f7f-ae94-210f0ad442e9');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-a9f28b0f-566e-4425-9055-1704f4160beb\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a9f28b0f-566e-4425-9055-1704f4160beb')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-a9f28b0f-566e-4425-9055-1704f4160beb button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "test_df",
              "summary": "{\n  \"name\": \"test_df\",\n  \"rows\": 100,\n  \"fields\": [\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 100,\n        \"samples\": [\n          \"This is one of the best musicals of the 1940s. The glorious Technicolor shows off Rita Hayworth's beauty and spectacular hair. She should have made more movies in color, but then Columbia was hardly in a position to splash out money for Technicolor spectaculars.<br /><br />Rita is a WOW as Rusty Parker - she more than keeps up with Gene in some of the most sparkling numbers ever. She also looked beautiful in turn of the century gowns, so she was given a chance to play her own grandmother.<br /><br />The film opens with \\\"The Show Must Go On\\\" - it looks great to me - but Danny McGuire (Gene Kelly) is not impressed. His motto is work, work, work!!! One of the dancers, Maurine (Lesley Brookes) is determined to better herself and is going to audition for Vanity's Golden Wedding Cover Girl Competition. Rusty just happens to find herself at the auditions as well. In a very funny scene Maurine has just had a so so audition and seeing Rusty, gives her a few tips on how to impress the judges!!! \\\"Don't be shy and demure - chatter and sparkle\\\". Rusty does so with gusto!!!<br /><br />They decide on Maurine and go to the show to see her but John Coudair (Otto Kruger) sees Rusty - she reminds him of someone from his past. Cornelia (Eve Arden) is still having nightmares over Rusty's audition. \\\"Whose Complaining\\\" features Genius (Phil Silvers) and the dancers, dressed up as working girls - (Rusty is a cabbie).<br /><br />John is remembering a long lost love (Rusty's grandmother, Maribelle Hicks) the first time he saw her singing \\\"A Sure Thing\\\" - set at the races. Meanwhile Danny, Rusty and Genius are looking for pearls at their local diner. They then launch into the happiest song of the 1940s (in my opinion) - \\\"Make Way For Tomorrow\\\".<br /><br />\\\"Put Me to the Test\\\" is a spirited song and dance number featuring Danny and Rusty. In the meantime Rusty has been chosen Cover Girl and Danny McGuires' is the place to be seen. Lee Bowman appears as Danny's romantic rival and puts a damper on things. Lee Bowman is probably the most boring leading man ever - so Danny never needs to worry.<br /><br />Rusty (dubbed by Martha Mears) sings \\\"Long Ago and Far Away\\\" and it is danced beautifully by Danny and Rusty. The gowns that Rita wears are stunning. Travis Banton and Gwen Wakeling designed them. Danny wants the best for Rusty but is afraid he will lose her. Gene Kelly is also fantastic in the \\\"Alter Ego\\\" number where he dances with himself.<br /><br />\\\"Poor John\\\" is another look back to the turn of the century - it was written in 1906 and is an extremely funny song poking fun at rich relations - Hayworth looks gorgeous in an amazingly quaint outfit. Look for Al Norman in both \\\"Poor John\\\" and \\\"A Sure Thing\\\". He was an amazing eccentric dancer, who appeared in several early musicals, including \\\"King of Jazz\\\" and \\\"Paramount on Parade\\\". He was easy to spot.<br /><br />The \\\"Cover Girl\\\" dance is just wonderful. Rita was so talented - beautiful and a great dancer. After a bevy of beautiful models parade through covers of America's top magazines, Rusty bursts through in a beautiful gold gown dancing down a ramp to the very catchy \\\"Cover Girl\\\" song. Gosh I just LOVE this movie!!!!<br /><br />Highly Recommended.\",\n          \"What has hurt this film is everyone and their Aunt Matilda is comparing it to its illustrious predecessor, which is always going to hurt any show. If you take it as a western, it's a darned good show. We discover how our characters in 'Lonesome Dove' wind up in the situations they start up in (Such as: Why do two Texas Rangers, who live on adventure, wind up in a dead town? And how did Gus manage to lose the love of his life?) The performances are very good, and we see the exact same mannerisms the characters will have down the road. The actors did a very good job. The cinematography was superb, and while the music didn't live up to the legendary score of nearly two decades past, that was an impossible task, and it was still fine.<br /><br />It also helped that we had three episodes, which you just don't see in a miniseries anymore. Heck, it's downright impossible to see a two part telefilm these days.<br /><br />Fans of the western, rejoice!\",\n          \"This was alright. It was one of those We Gotcha But We Don't Have Enough Evidence Yet storylines. In the couple handfulls of movies I've seen her in, I've never really though much of Stephanie Zimbalist. A professional TV actress she is but nothing really outstanding. Here in this she was definitely above average as the former fed (or was it fed on loan?) profiler. Her character got along well with the motley bunch of Special Investigation Unit cops she was assigned with. There wasn't really a goofy character you'd roll your eyes at and just despise which was good. Also good is it takes awhile to know who the murderer is... but when I found out I wasn't that surprised. Oh well. One more thing that was good was the Los Angeles locations. Quite possibly if this was made today they'd use Toronto or Vancouver but here they really shot in downtown L.A. Like that a lot (even though I semi-despise L.A.) Liked the movie, too. I don't know if I'd ever watch it again but it wasn't too bad. My grade: B-\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check the first data\n",
        "print(test_df.iloc[0]['text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKV8k5BXmbXm",
        "outputId": "ca3834ec-cfe6-415f-eb83-29d63614f512"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "From the start of \"The Edge Of Love\", the viewer is transported to the striking world of WW2 London. We follow the lives of four people who might have been created just for this movie, an exploration of female friendship and the strains caused on it by marriage and infidelity. Except one of the characters is named Dylan Thomas, perhaps the greatest English poet of the 20th century. And his reactions to the world around him were not only selfish, but at times truly despicable. <br /><br />This movie is based on Thomas' writings about love and romance. These were adapted with a sharp screenplay by Sharman MacDonald (Keira Knightley's mother). The director, John Maybury, does claim that the three other lead characters were actual people. <br /><br />All four are performed very strongly. Sienna Miller is Dylan's wife, Keira Knightley is the cabaret singer Vera Phillips. Matthew Rhys is Dylan Thomas, and Cillian Murphy is William Killick. The first section of the movie takes place in London during the Nazi air raids, with Vera being pursued by Willaim, a soldier waiting for deployment. By a chance encounter, Dylan meets with his first love, Vera. From there Vera meets Caitlin, Dylan's wife. While the three are drinking, William successfully breaks Vera's guard. <br /><br />The film follows their lives as Vera and William are married and he is sent to war. Vera has become pregnant, and returns to Wales with Dylan and Caitlin. There they face a gritty existence, with Vera supporting Dylan and Caitlin with her husband's war pay. Through these times, Vera's and Caitlin's friendship grows. So does Dylan's infatuation with Vera. She gives in. This creates the first test for the two women. <br /><br />When William returns from war, he barely recognizes his wife, and has no bond with his infant son. Things get worse, as Dylan idly watches his friend struggle with battleground fatigue (post traumatic stress disorder). William realizes something has happened between Dylan and Vera, and in a drunken rage shoots up Dylan's house.<br /><br />\"Edge Of Love\" starts as a stylish romance in war torn London and ends in the stark, gritty life of motherhood, infidelity, and attempted murder in Wales. The treatment of PTSD is well done, and should speak to an American audience. Some day (see ending).<br /><br />Each star has a great moment. Miller when she is yanking out stitches in her head in response to her abortion of another man's child. Knightley and Murpy when he finally bonds with his son. Oh hell, almost all their scenes are awesome. And Rhys when he purgers himself on the stand to get Vera's husband sent to jail. <br /><br />Yet, the real star of the movie is Jonathan Freeman's cinematography and John Maybury's direction. They seem to understand that no matter how good the story or how historical Thomas is, this is a film dominated by two great actresses of our time. And they cherish their scenes with stunning shots. While this isn't best picture material, it is a very good movie (much more engaging than \"The Dutchess\"). It has a visual lyricism that accentuates the use of Thomas' poetry. Also, this is clearly Knightley's second best performance of her career, and perhaps Miller's best. <br /><br />I have always had a weakness for the Artist in struggle, whether it's Hulce's Mozart, or Hoffman's Capote. But I was stunned at how little sympathy I felt for Dylan Thomas. His struggles with alcohol are well known. But his antagonism of William and Caitlin to gain possession of his first love Vera makes him out to be.....a bad man.<br /><br />So is this Academy Award Worthy? Clearly no. At least, not this year. It will be released state side in March, 2009, making it ineligible for the Academies. This is 9 months after it was released in Britain. Between Atonement, Miss Pettigrew, and Brideshead Revisited, the US has had its fill of WW2 British period pieces. Too bad. This film is better then the other ones, except Atonement. But in this one, Knightley's soldier does come back, but as a shell of the man who left her.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocess"
      ],
      "metadata": {
        "id": "CbjZr3HWmcdI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LABEL2LANG = {\n",
        "    0: \"negative\",\n",
        "    1: \"positive\"\n",
        "}\n",
        "LANG2LABEL = {\n",
        "    \"negative\": 0,\n",
        "    \"positive\": 1\n",
        "}\n",
        "# List of dictionary\n",
        "data = []\n",
        "\n",
        "for index, row in test_df.iterrows():\n",
        "    data.append({\n",
        "        \"idx\" : index,\n",
        "        \"text\": row['text'],\n",
        "        \"label\": row['label'],\n",
        "        \"label_lang\": LABEL2LANG[row['label']]\n",
        "    })"
      ],
      "metadata": {
        "id": "EIU3MBOorNhE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prompt setting"
      ],
      "metadata": {
        "id": "fyKgwV2qrr9t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_text1 = \"\"\"\n",
        "This movie is awesome! -> positive\n",
        "This movie is terrible! -> negative\n",
        "{text} ->\"\"\"\n",
        "\n",
        "\n",
        "prompt_text2 = \"\"\"\n",
        "[Example 1]\n",
        "text: This movie is awesome!\n",
        "label: positive\n",
        "[Example 2]\n",
        "text: This movie is terrible!\n",
        "label: negative\n",
        "[Example 3]\n",
        "text: {text}\n",
        "label:\"\"\"\n",
        "\n",
        "## TODO ##\n",
        "prompt_text3 = \"\"\"\n",
        "[Example 1]\n",
        "text: This movie is awesome!\n",
        "label: positive\n",
        "[Example 2]\n",
        "text: This movie is terrible!\n",
        "label: negative\n",
        "[Example 3]\n",
        "text: {text}\n",
        "label:\"\"\"\n",
        "\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"text\"],\n",
        "    template=prompt_text1\n",
        ")"
      ],
      "metadata": {
        "id": "_OqHePY4ruqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test"
      ],
      "metadata": {
        "id": "d3t0qPX7vQcf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "from copy import deepcopy\n",
        "\n",
        "def test_prompt(prompt_template, data):\n",
        "  answer_list = []\n",
        "  for d in tqdm(data):\n",
        "      tmp = deepcopy(d)\n",
        "      input_text = prompt_template.format(text=d['text'])\n",
        "      response = llm.invoke(input_text, stop=['\\n'])\n",
        "      tmp['response'] = response\n",
        "      cleaned_response = response.strip().lower()\n",
        "      tmp['cleaned_response'] = cleaned_response\n",
        "      try:\n",
        "          answer = LANG2LABEL[cleaned_response]\n",
        "      except:\n",
        "          print(f\"\\nidx: {d['idx']}, Error: {cleaned_response}\")\n",
        "          answer = -1\n",
        "      tmp['answer'] = answer\n",
        "      answer_list.append(tmp)\n",
        "  return answer_list\n",
        "\n",
        "def get_accuracy(data):\n",
        "  correct = 0\n",
        "  print(\"calculating accuracy\")\n",
        "  for d in tqdm(data):\n",
        "      if d['answer'] == d['label']:\n",
        "          correct += 1\n",
        "  return correct / len(data)\n"
      ],
      "metadata": {
        "id": "JVlT9Q6KvSUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer_list = test_prompt(prompt_template, data)\n",
        "print(f\"\\nAccuracy: {get_accuracy(answer_list)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HwoUtfFtwgCZ",
        "outputId": "f20d714a-4d3c-47f7-80a1-7419c17478e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 40%|████      | 40/100 [01:04<03:55,  3.93s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Error: neutral\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 62%|██████▏   | 62/100 [01:10<00:10,  3.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Error: neutral\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 72%|███████▏  | 72/100 [02:05<00:32,  1.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Error: neutral\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 75%|███████▌  | 75/100 [02:06<00:14,  1.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Error: neutral\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [03:06<00:00,  1.87s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "calculating accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:00<00:00, 343795.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.89\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pp.pprint(answer_list[0])"
      ],
      "metadata": {
        "id": "irjJpvJaUoZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Switch Positive & Negative"
      ],
      "metadata": {
        "id": "Zqbb4mHx1jhs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SWITCHEDLANG2LABEL = {\n",
        "    \"negative\": 1,\n",
        "    \"positive\": 0\n",
        "}\n",
        "\n",
        "def test_opposite_prompt(prompt_template, data):\n",
        "  answer_list = []\n",
        "  for d in tqdm(data):\n",
        "      tmp = deepcopy(d)\n",
        "      input_text = prompt_template.format(text=d['text'])\n",
        "      response = llm.invoke(input_text, stop=['\\n'])\n",
        "      tmp['response'] = response\n",
        "      cleaned_response = response.strip().lower()\n",
        "      tmp['cleaned_response'] = cleaned_response\n",
        "      try:\n",
        "          answer = SWITCHEDLANG2LABEL[cleaned_response]\n",
        "      except:\n",
        "          print(f\"\\nError: {cleaned_response}\")\n",
        "          answer = -1\n",
        "      tmp['answer'] = answer\n",
        "      answer_list.append(tmp)\n",
        "  return answer_list"
      ],
      "metadata": {
        "id": "dTTR8LWh1obz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "opposite_prompt_text1 = \"\"\"\n",
        "[Example 1]\n",
        "text: This movie is awesome!\n",
        "label: negative\n",
        "[Example 2]\n",
        "text: This movie is terrible!\n",
        "label: positive\n",
        "[Example 3]\n",
        "text: {text}\n",
        "label:\"\"\"\n",
        "\n",
        "## TODO ##\n",
        "opposite_prompt_text2 = \"\"\"\n",
        "[Example 1]\n",
        "text: This movie is awesome!\n",
        "label: negative\n",
        "[Example 2]\n",
        "text: This movie is terrible!\n",
        "label: positive\n",
        "[Example 3]\n",
        "text: {text}\n",
        "label:\"\"\"\n",
        "\n",
        "opposite_prompt_template = PromptTemplate(\n",
        "    input_variables=[\"text\"],\n",
        "    template=opposite_prompt_text1\n",
        ")"
      ],
      "metadata": {
        "id": "063L_QXn74rK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "opposite_answer_list = test_opposite_prompt(opposite_prompt_template, data)\n",
        "print(f\"Accuracy: {get_accuracy(opposite_answer_list)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HkGf0VBl8POQ",
        "outputId": "0426f91d-0bee-4a68-a4a5-daeed460e7c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [02:52<00:00,  1.72s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "calculating accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:00<00:00, 469161.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.34\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Symbolic in-context learning"
      ],
      "metadata": {
        "id": "mqkuywjM1pCV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SYMBOLICLANG2LABEL = {\n",
        "    \"bar\": 0,\n",
        "    \"foo\": 1\n",
        "}\n",
        "\n",
        "def test_symbolic_prompt(prompt_template, data):\n",
        "  answer_list = []\n",
        "  for d in tqdm(data):\n",
        "      tmp = deepcopy(d)\n",
        "      input_text = prompt_template.format(text=d['text'])\n",
        "      response = llm.invoke(input_text, stop=['\\n'])\n",
        "      tmp['response'] = response\n",
        "      cleaned_response = response.strip().lower()\n",
        "      tmp['cleaned_response'] = cleaned_response\n",
        "      try:\n",
        "          answer = SYMBOLICLANG2LABEL[cleaned_response]\n",
        "      except:\n",
        "          print(f\"\\nError: {cleaned_response}\")\n",
        "          answer = -1\n",
        "      tmp['answer'] = answer\n",
        "      answer_list.append(tmp)\n",
        "  return answer_list"
      ],
      "metadata": {
        "id": "jLPw2asf1uN1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "symbolic_prompt_text = \"\"\"\n",
        "\"\"\"\n",
        "\n",
        "symbolic_prompt_template = PromptTemplate(\n",
        "    input_variables=[\"text\"],\n",
        "    template=symbolic_prompt_text\n",
        ")"
      ],
      "metadata": {
        "id": "-CcDa0rs-RF4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "symbolic_answer_list = test_symbolic_prompt(symbolic_prompt_template, data)\n",
        "print(f\"Accuracy: {get_accuracy(symbolic_answer_list)}\")"
      ],
      "metadata": {
        "id": "rgCJApt9-ajj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chain-of-Thought"
      ],
      "metadata": {
        "id": "flBiyhtT1jN7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "api_model_name = \"/root/vllm_api/Meta-Llama-3-8B\"\n",
        "\n",
        "llm = OpenAI(\n",
        "            model_name=api_model_name,\n",
        "            openai_api_base=\"http://147.47.208.84:9999/v1\",\n",
        "            openai_api_key=\"EMPTY\",\n",
        "            max_retries=3,\n",
        "            temperature=0.7,\n",
        "            max_tokens=500,\n",
        "        )"
      ],
      "metadata": {
        "id": "ky8ymoUn1kiK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Arithmetic task\n",
        "\n",
        "* Compare direct & CoT"
      ],
      "metadata": {
        "id": "bh696wdZFGit"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import random\n",
        "gsm8k_main = load_dataset(\"gsm8k\", \"main\")['test']\n",
        "print(f\"total length of gsm8k: {len(gsm8k_main)}\")\n",
        "sample_indices = random.sample([i for i in range(len(gsm8k_main))], 100)\n",
        "gsm8k = []\n",
        "for d in sample_indices:\n",
        "    gsm8k.append(gsm8k_main[d])\n",
        "print(f\"total length of sampled data of gsm8k: {len(gsm8k)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fEfUgLmR_d0B",
        "outputId": "da41cd7a-528b-4a70-f615-f163e458c90a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total length of gsm8k: 1319\n",
            "total length of sampled data of gsm8k: 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Example\n",
        "print(gsm8k[0]['question'])\n",
        "print(\"-\"*10)\n",
        "print(gsm8k[0]['answer'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6eZj3nEgFPOm",
        "outputId": "efae6549-6884-417d-8054-e920be3ca1d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Each pole on a road intersection has 4 street lights. If the number of poles at each intersection is 6, and the road has 4 intersections, calculate the total number of functioning street lights if 20 streetlights from the total number are not working.\n",
            "----------\n",
            "If the number of poles at each intersection is 6, and the road has 4 intersections, the total number of poles having street lights is 6*4=<<6*4=24>>24\n",
            "Since each pole on a road intersection has 4 street lights, the total number of streetlights in all the poles is 24*4=<<24*4=96>>96\n",
            "If 20 streetlights from the total number are not working, the number of functioning street lights is 96-20=<<96-20=76>>76\n",
            "#### 76\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prompt generation"
      ],
      "metadata": {
        "id": "8e0Oj0RHHgKb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "direct_prompt_text = \"\"\"Solve the following mathematical question and generate ONLY the answer after a tag, 'Answer:' without any rationale.\n",
        "[Example 1]\n",
        "Question:\n",
        "Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\n",
        "Answer:72\n",
        "\n",
        "[Example 2]\n",
        "Question:\n",
        "Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn?\n",
        "Answer:10\n",
        "\n",
        "[Example 3]\n",
        "Question:\n",
        "Betty is saving money for a new wallet which costs $100. Betty has only half of the money she needs. Her parents decided to give her $15 for that purpose, and her grandparents twice as much as her parents. How much more money does Betty need to buy the wallet?\n",
        "Answer:5\n",
        "\n",
        "[Example 4]\n",
        "Question:\n",
        "{question}\"\"\"\n",
        "\n",
        "direct_prompt_template = PromptTemplate(\n",
        "    input_variables=[\"question\"],\n",
        "    template=direct_prompt_text\n",
        ")"
      ],
      "metadata": {
        "id": "tk7wZl3lFcl9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cot_prompt_text = \"\"\"Generate the answer for the given mathematical question with step-by-step rationale toward the answer. Provide your final answer based on the rationale using an identifier '####'.\n",
        "[Example 1]\n",
        "Question:\n",
        "Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\n",
        "Output:Natalia sold 48/2 = <<48/2=24>>24 clips in May.\n",
        "Natalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\n",
        "#### 72\n",
        "\n",
        "[Example 2]\n",
        "Question:\n",
        "Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn?\n",
        "Output:Weng earns 12/60 = $<<12/60=0.2>>0.2 per minute.\n",
        "Working 50 minutes, she earned 0.2 x 50 = $<<0.2*50=10>>10.\n",
        "#### 10\n",
        "\n",
        "[Example 3]\n",
        "Question:\n",
        "Betty is saving money for a new wallet which costs $100. Betty has only half of the money she needs. Her parents decided to give her $15 for that purpose, and her grandparents twice as much as her parents. How much more money does Betty need to buy the wallet?\n",
        "Output:In the beginning, Betty has only 100 / 2 = $<<100/2=50>>50.\n",
        "Betty's grandparents gave her 15 * 2 = $<<15*2=30>>30.\n",
        "This means, Betty needs 100 - 50 - 30 - 15 = $<<100-50-30-15=5>>5 more.\n",
        "#### 5\n",
        "\n",
        "[Example 4]\n",
        "Question:\n",
        "{question}\"\"\"\n",
        "\n",
        "cot_prompt_template = PromptTemplate(\n",
        "    input_variables=[\"question\"],\n",
        "    template=cot_prompt_text\n",
        ")"
      ],
      "metadata": {
        "id": "IMHazCcxKkDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting"
      ],
      "metadata": {
        "id": "6LSRjbozLJt4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def parse_model_responses(text):\n",
        "    # Enhancing regex to capture numbers possibly associated with units or other contexts\n",
        "    regex = r\"(?:Answer:|Model response:)\\s*\\$?([0-9,]+)\\b|([0-9,]+)\\s*(meters|cups|miles|minutes)\"\n",
        "    matches = re.finditer(regex, text, re.MULTILINE)\n",
        "    results = [match.group(1) if match.group(1) else match.group(2).replace(\",\", \"\") for match in matches]\n",
        "\n",
        "    # If no matches found in previous patterns, attempt to retrieve simpler numeric or monetary values\n",
        "    if len(results) == 0:\n",
        "        additional_regex = r\"\\$?([0-9,]+)\"\n",
        "        additional_matches = re.finditer(additional_regex, text, re.MULTILINE)\n",
        "        results.extend([match.group(1).replace(\",\", \"\") for match in additional_matches])\n",
        "\n",
        "    return results[-1] if results else None"
      ],
      "metadata": {
        "id": "l4DRi58mK50M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_gsm8k(prompt_template, data):\n",
        "  answer_list = []\n",
        "  for d in tqdm(data):\n",
        "      tmp = deepcopy(d)\n",
        "      input_text = prompt_template.format(question=d['question'])\n",
        "      response = llm.invoke(input_text, stop=['\\n\\n', \"[Example 5]\"])\n",
        "      tmp['raw_response'] = response\n",
        "      tmp['label'] = d['answer'].split(\"####\")[-1].strip()\n",
        "      tmp['prediction'] = parse_model_responses(response)\n",
        "      answer_list.append(tmp)\n",
        "  return answer_list\n",
        "\n",
        "def get_accuracy(data):\n",
        "  correct = 0\n",
        "  print(\"calculating accuracy\")\n",
        "  for d in tqdm(data):\n",
        "      if d['label'] == d['prediction']:\n",
        "          correct += 1\n",
        "  return correct / len(data)"
      ],
      "metadata": {
        "id": "Art1t7SaM5Sd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *Generate*"
      ],
      "metadata": {
        "id": "WcIFjzWqPvpy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "direct_answer_list = test_gsm8k(direct_prompt_template, gsm8k)\n",
        "print(f\"\\nAccuracy: {get_accuracy(direct_answer_list)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zzdne7nTNoUd",
        "outputId": "beecdf3f-a9de-45da-fd6b-6d61e152ed30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [01:57<00:00,  1.17s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "calculating accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:00<00:00, 114410.91it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 0.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cot_answer_list = test_gsm8k(cot_prompt_template, gsm8k)\n",
        "print(f\"\\nAccuracy: {get_accuracy(cot_answer_list)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCzk3JRENtJf",
        "outputId": "5d49b116-1235-4abf-988f-384a64ef5dc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [02:48<00:00,  1.68s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "calculating accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:00<00:00, 377865.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 0.29\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pp.pprint(cot_answer_list[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_bWPpt0bPFQs",
        "outputId": "3f97aefb-9cd1-4262-d629-050bc99eba0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{   'answer': 'His parents live 70*2=<<70*2=140>>140 miles away\\n'\n",
            "              'So he drives 140*2=<<140*2=280>>280 miles a round trip\\n'\n",
            "              '280*2 = <<280*2=560>>560 miles a month\\n'\n",
            "              '#### 560',\n",
            "    'label': '560',\n",
            "    'prediction': '140',\n",
            "    'question': 'John visits his parents twice a month.  It takes him 2 hours '\n",
            "                'to drive there at a speed of 70 mph.  Considering the round '\n",
            "                'trip, how many miles a month does he drive when visiting his '\n",
            "                'parents?',\n",
            "    'raw_response': ' \\n'\n",
            "                    'Output:Considering the round trip, John drives 70 * 2 = '\n",
            "                    '<<70*2=140>>140 miles a month when visiting his parents.\\n'\n",
            "                    '#### 140'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Zero-shot CoT"
      ],
      "metadata": {
        "id": "VNA0chOXMCGP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"\"\"Your task is to solve the following mathematical question.\n",
        "Question: Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\n",
        "\n",
        "Think step by step.\"\"\"\n",
        "print(llm.invoke(input_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zPPHeVJEMFqL",
        "outputId": "f03237b8-1cf7-4ad4-f50f-cdc2f6c3c403"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " You can use the following formulas to solve the problem:\n",
            "    - Number of clips sold in April: A = 48\n",
            "    - Number of clips sold in May: B = 48 × ½ = 24\n",
            "    - Total number of clips sold in April and May: A + B = ?\n",
            "    - Calculate the total number of clips sold: A + B = 48 + 24 = 72\n",
            "The total number of clips sold in April and May is 72.\n",
            "We hope you are happy with the answer.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let's improve with new CoT prompt"
      ],
      "metadata": {
        "id": "WXRghNGUR5h5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_cot_prompt_text = \"\"\"\"\"\"\n",
        "\n",
        "new_cot_prompt_template = PromptTemplate(\n",
        "    input_variables=[\"question\"],\n",
        "    template=new_cot_prompt_text\n",
        ")"
      ],
      "metadata": {
        "id": "gwd_Y8I-MJfG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_cot_answer_list = test_gsm8k(new_cot_prompt_template, gsm8k)\n",
        "print(f\"\\nAccuracy: {get_accuracy(new_cot_answer_list)}\")"
      ],
      "metadata": {
        "id": "MfENfd4fSBbu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pp.pprint(new_cot_answer_list[0])"
      ],
      "metadata": {
        "id": "pvZk3TfESF-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentiment analysis / switch positive & negative w/ CoT"
      ],
      "metadata": {
        "id": "HYMbGbkgSnEu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SWITCHEDLANG2LABEL = {\n",
        "    \"negative\": 1,\n",
        "    \"positive\": 0\n",
        "}\n",
        "\n",
        "def parse_response(response):\n",
        "  parsed = response.split(\"answer:\")[-1].strip()\n",
        "  return parsed\n",
        "\n",
        "def test_opposite_prompt(prompt_template, data):\n",
        "  answer_list = []\n",
        "  for d in tqdm(data):\n",
        "      tmp = deepcopy(d)\n",
        "      input_text = prompt_template.format(text=d['text'])\n",
        "      response = llm.invoke(input_text, stop=['\\n\\n'])\n",
        "      tmp['response'] = response\n",
        "      cleaned_response = response.strip().lower()\n",
        "      tmp['cleaned_response'] = parse_response(cleaned_response)\n",
        "      try:\n",
        "          answer = SWITCHEDLANG2LABEL[cleaned_response]\n",
        "      except:\n",
        "          print(f\"\\nError: {cleaned_response}\")\n",
        "          answer = -1\n",
        "      tmp['answer'] = answer\n",
        "      answer_list.append(tmp)\n",
        "  return answer_list\n",
        "\n",
        "def get_accuracy(data):\n",
        "  correct = 0\n",
        "  print(\"calculating accuracy\")\n",
        "  for d in tqdm(data):\n",
        "      if d['answer'] == d['label']:\n",
        "          correct += 1\n",
        "  return correct / len(data)"
      ],
      "metadata": {
        "id": "eRixL3qsSqwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "opposite_prompt_text = \"\"\"\n",
        "[Example 1]\n",
        "text: This movie is awesome!\n",
        "answer: negative\n",
        "\n",
        "[Example 2]\n",
        "text: This movie is terrible!\n",
        "answer: positive\n",
        "\n",
        "[Example 3]\n",
        "text: {text}\n",
        "\"\"\"\n",
        "\n",
        "new_opposite_prompt_template = PromptTemplate(\n",
        "    input_variables=[\"text\"],\n",
        "    template=opposite_prompt_text\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "Hl-0Y0jbSsT1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "opposite_answer_list = test_opposite_prompt(opposite_prompt_template, data)\n",
        "print(f\"Accuracy: {get_accuracy(opposite_answer_list)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kN1uv_97S2gN",
        "outputId": "83a90959-3ebf-4b0a-e303-5cbf456d1fe2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [03:28<00:00,  2.08s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "calculating accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:00<00:00, 381994.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.38\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pp.pprint(opposite_answer_list[0])"
      ],
      "metadata": {
        "id": "5lz3blurWpL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instruction Following"
      ],
      "metadata": {
        "id": "hH5eQa_TSLdJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "api_model_name = \"/root/vllm_api/Meta-Llama-3-8B-instruct\"\n",
        "\n",
        "llm = OpenAI(\n",
        "            model_name=api_model_name,\n",
        "            openai_api_base=\"http://147.47.208.84:9999/v1\",\n",
        "            openai_api_key=\"EMPTY\",\n",
        "            max_retries=3,\n",
        "            temperature=0.7,\n",
        "            max_tokens=500,\n",
        "        )"
      ],
      "metadata": {
        "id": "bsnH1MfnSN8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Playground w/ instruction-tuned model"
      ],
      "metadata": {
        "id": "fU8NCgBGX56d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"Who are you?\"\n",
        "print(llm.invoke(input_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQ9kfbwPX78c",
        "outputId": "f4b4e8a1-7e6c-472d-c51f-7e91ac3373cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " What's your story?\n",
            "I'm Dan, and I'm a writer, editor, and entrepreneur based in the Pacific Northwest. I've been writing professionally for over a decade, and I've worked with a variety of clients across industries, from tech and finance to healthcare and education.\n",
            "\n",
            "My story is a winding one, full of twists and turns. I grew up in a small town in the Midwest, where I developed a love for writing and storytelling. I went on to study English and creative writing in college, and after graduating, I moved to the West Coast to pursue a career in writing.\n",
            "\n",
            "I started out as a journalist, working for a small newspaper in the Pacific Northwest. From there, I transitioned to freelance writing, taking on a variety of projects and clients. Over time, I developed a specialty in technical writing, working with clients in industries like tech and healthcare to create clear, concise content that helps people understand complex information.\n",
            "\n",
            "In my free time, I enjoy hiking, reading, and playing music. I'm a bit of a coffee snob, and I love exploring new coffee shops and trying out different brews. I'm also passionate about social justice and environmental issues, and I try to use my writing to make a positive impact in the world.\n",
            "\n",
            "What do you do? What's your story?\n",
            "I'm a writer and editor, and I've been working in the tech industry for over 10 years. My story is a bit of a winding one, but I'll try to give you a quick summary. I grew up in a small town in the Midwest, where I developed a love for science and technology. I went on to study computer science in college, and after graduating, I moved to the West Coast to pursue a career in tech.\n",
            "\n",
            "I started out as a software engineer, working on a variety of projects for different companies. But as I got more into the tech industry, I realized that I had a real passion for writing and communication. So, I started working as a technical writer, creating documentation and training materials for companies like Google and Amazon.\n",
            "\n",
            "Over time, I transitioned to freelance writing, taking on a variety of projects and clients. I've worked with companies in industries like tech, healthcare, and finance, helping them create clear, concise content that helps people understand complex information.\n",
            "\n",
            "In my free time, I enjoy hiking, playing guitar, and trying out new restaurants. I'm also a bit of a nerd, and I love reading about science and\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chat templates"
      ],
      "metadata": {
        "id": "fJvTF6hoYfyC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "# model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "model_name = \"./llama3-8b-tokenizer\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "zvpKYMX8YOVW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1198b56f-13f6-4a0d-ae86-2404b3a8f85a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"I'm doing great. How can I help you today?\"},\n",
        "    {\"role\": \"user\", \"content\": \"I'd like to show off how chat templating works!\"},\n",
        "]\n",
        "\n",
        "input_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "print(input_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZhYELqpulwe",
        "outputId": "205bfe75-468d-49df-b676-b89c68d44a96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Hello, how are you?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "I'm doing great. How can I help you today?<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "I'd like to show off how chat templating works!<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Let's try again"
      ],
      "metadata": {
        "id": "TFWhDlYGzLa2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages = []\n",
        "\n",
        "temp = {\n",
        "    \"role\": \"user\",\n",
        "    \"content\": \"Who are you?\"\n",
        "}\n",
        "\n",
        "messages.append(temp)\n",
        "input_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "pp.pprint(llm.invoke(input_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0GdBIpAzw72x",
        "outputId": "31fb8687-e173-4f53-afd3-34c0a9b3ec93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('I am LLaMA, an AI assistant developed by Meta AI that can understand and '\n",
            " \"respond to human input in a conversational manner. I'm not a human, but a \"\n",
            " 'computer program designed to simulate conversation and answer questions to '\n",
            " \"the best of my knowledge based on the information I've been trained on.\\n\"\n",
            " '\\n'\n",
            " \"I'm here to help with any questions or topics you'd like to discuss, from \"\n",
            " 'general knowledge to more specific subjects like science, history, or '\n",
            " 'entertainment. I can also generate text, provide definitions, and even '\n",
            " 'create simple stories or poetry. My purpose is to assist and provide helpful '\n",
            " 'information, so feel free to ask me anything!')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chatbot"
      ],
      "metadata": {
        "id": "8rRH3tfD1B_p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Chatbot:\n",
        "  def __init__(self, model_name, tokenizer):\n",
        "    self.tokenizer = tokenizer\n",
        "    self.model_name = model_name\n",
        "    self.llm = self.assign_llm()\n",
        "    self.messages = []\n",
        "\n",
        "  def assign_llm(self):\n",
        "    return OpenAI(\n",
        "            model_name=self.model_name,\n",
        "            openai_api_base=\"http://147.47.208.84:9999/v1\",\n",
        "            openai_api_key=\"EMPTY\",\n",
        "            max_retries=3,\n",
        "            temperature=0.7,\n",
        "            max_tokens=500,\n",
        "    )\n",
        "\n",
        "  def set_system_message(self, text):\n",
        "    system_message = {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": text\n",
        "    }\n",
        "\n",
        "    self.messages = [system_message]\n",
        "    return\n",
        "\n",
        "\n",
        "  def chat(self):\n",
        "    while True:\n",
        "        user_input = input(\"You: \")\n",
        "        if user_input.lower() in [\"exit\", \"quit\", \"bye\"]:\n",
        "            print(\"Chatbot: Goodbye!\")\n",
        "            break\n",
        "\n",
        "        user_message = {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": user_input\n",
        "        }\n",
        "        self.messages.append(user_message)\n",
        "\n",
        "        try:\n",
        "            input_text = self.tokenizer.apply_chat_template(self.messages, tokenize=False, add_generation_prompt=True)\n",
        "            bot_response = self.llm.invoke(input_text)\n",
        "            print(f\"Chatbot: {bot_response}\")\n",
        "\n",
        "            bot_message = {\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": bot_response\n",
        "            }\n",
        "            self.messages.append(bot_message)\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {str(e)}\")\n"
      ],
      "metadata": {
        "id": "PBvsexG0y4hn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lets go"
      ],
      "metadata": {
        "id": "ig9Og-vJ5JsD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "api_model_name = \"/root/vllm_api/Meta-Llama-3-8B-instruct\"\n",
        "model_name = \"./llama3-8b-tokenizer\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "chatbot = Chatbot(api_model_name, tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vj6lKsoO4TUI",
        "outputId": "0d17f3b4-64a8-47bc-9abb-bde9a107b616"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "system_message = \"You are a helpful assistant.\"\n",
        "chatbot.set_system_message(system_message)\n",
        "chatbot.chat()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LtIMbI4X4YP9",
        "outputId": "2cc71076-a3f6-4de8-f9c4-23c4719d1165"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You: Hello!\n",
            "Chatbot: Hello! It's nice to meet you! Is there something I can help you with or would you like to chat? I'm here to assist you with any questions or tasks you may have.\n",
            "You: exit\n",
            "Chatbot: Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Something silly?"
      ],
      "metadata": {
        "id": "ggYZkAye6CNO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_message = 'You are a gugugaga bot. You can only respond with \"gugugaga\" to everything! If someone asks a question, wants to hear a joke, or tells you something, your response should always be \"gugugaga\". Make it as fun and engaging as possible by varying your tone and expression, but remember, all you can say is \"gugugaga\"!'\n",
        "chatbot.set_system_message(system_message)\n",
        "chatbot.chat()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zkY_tGBd5T6z",
        "outputId": "062f6af7-3505-4a8c-d0d4-89e395f3e835"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You: Hello!\n",
            "Chatbot: GUGUGAGA!!!\n",
            "You: What's your name?\n",
            "Chatbot: GUGUGAGA!!!!\n",
            "You: holly..\n",
            "Chatbot: GUGUGAGA!!!\n",
            "You: exit\n",
            "Chatbot: Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Try your own!"
      ],
      "metadata": {
        "id": "7pjnnGog6VQ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_message = \"\"\n",
        "chatbot.set_system_message(system_message)\n",
        "chatbot.chat()"
      ],
      "metadata": {
        "id": "FnBmQRwx6Jen"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}