{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMOy6qm0NHzW0C79k4gpP23",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/donghuna/AI-Expert/blob/main/Project/retrieval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain\n",
        "!pip install tiktoken\n",
        "!pip install langchain-openai\n",
        "!pip install faiss-cpu\n",
        "!pip install langchain-community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0zvFBm5Nkfme",
        "outputId": "981cb496-70d0-4864-d66e-f952e0bcb83c"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.2.12)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.32)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.10.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.27 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.29)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.2)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.99)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.5.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.3.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.27->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.27->langchain) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.27->langchain) (4.12.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.7)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.7.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.27->langchain) (3.0.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.7.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.7.4)\n",
            "Requirement already satisfied: langchain-openai in /usr/local/lib/python3.10/dist-packages (0.1.21)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.29 in /usr/local/lib/python3.10/dist-packages (from langchain-openai) (0.2.29)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.40.0 in /usr/local/lib/python3.10/dist-packages (from langchain-openai) (1.40.3)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.10/dist-packages (from langchain-openai) (0.7.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.29->langchain-openai) (6.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.29->langchain-openai) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.75 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.29->langchain-openai) (0.1.99)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.29->langchain-openai) (24.1)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.29->langchain-openai) (2.8.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.29->langchain-openai) (8.5.0)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.29->langchain-openai) (4.12.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->langchain-openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.40.0->langchain-openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->langchain-openai) (0.27.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->langchain-openai) (0.5.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->langchain-openai) (4.66.5)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2.32.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.40.0->langchain-openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.40.0->langchain-openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.40.0->langchain-openai) (2024.7.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.40.0->langchain-openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.40.0->langchain-openai) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.29->langchain-openai) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3.0,>=0.2.29->langchain-openai) (3.10.7)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.3.0,>=0.2.29->langchain-openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.3.0,>=0.2.29->langchain-openai) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (2.0.7)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.10/dist-packages (1.8.0.post1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.1)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.10/dist-packages (0.2.11)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.0.32)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (3.10.1)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: langchain<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.2.12)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.27 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.2.29)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.1.99)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (1.26.4)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (8.5.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.3.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.21.3)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.12->langchain-community) (0.2.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.12->langchain-community) (2.8.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.27->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.27->langchain-community) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.27->langchain-community) (4.12.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-community) (3.10.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2024.7.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.0.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.27->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.12->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.12->langchain-community) (2.20.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "import subprocess\n",
        "import shutil\n",
        "from langchain.text_splitter import TokenTextSplitter\n",
        "\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "fJ32Yy6-pC8E"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clone_git_repository(repo_url, clone_dir):\n",
        "    \"\"\"\n",
        "    주어진 GitHub 저장소를 지정된 디렉토리에 클론합니다.\n",
        "    \"\"\"\n",
        "    if os.path.exists(clone_dir):\n",
        "        print(f\"Directory {clone_dir} already exists. Deleting and recloning the repository.\")\n",
        "        shutil.rmtree(clone_dir)\n",
        "    subprocess.run(['git', 'clone', repo_url, clone_dir], check=True)\n",
        "    print(f\"Repository cloned to {clone_dir}\")\n",
        "\n",
        "def read_code_files(directory, extensions=['.py']):\n",
        "    \"\"\"\n",
        "    지정된 디렉토리에서 주어진 확장자를 가진 모든 코드 파일을 읽어서 반환합니다.\n",
        "    \"\"\"\n",
        "    code_texts = []\n",
        "    for root, dirs, files in os.walk(directory):\n",
        "        for file in files:\n",
        "            if any(file.endswith(ext) for ext in extensions):\n",
        "                file_path = Path(root) / file\n",
        "                with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                    code_texts.append(f.read())\n",
        "    return code_texts\n",
        "\n",
        "def split_text_with_overlap(text, chunk_size=1000, overlap=200):\n",
        "    \"\"\"\n",
        "    TokenTextSplitter를 사용하여 텍스트를 오버랩을 가진 청크로 나눕니다.\n",
        "    \"\"\"\n",
        "    text_splitter = TokenTextSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=overlap,\n",
        "        disallowed_special=()  # 모든 특수 토큰을 허용하지 않음\n",
        "    )\n",
        "    return text_splitter.split_text(text)\n",
        "\n",
        "def process_code_directory(directory, extensions=['.py', '.cpp', '.h'], chunk_size=1000, overlap=200):\n",
        "    \"\"\"\n",
        "    디렉토리 내의 모든 코드 파일을 읽고, TokenTextSplitter를 사용하여 오버랩이 있는 청크로 나눠서 반환합니다.\n",
        "    \"\"\"\n",
        "    code_texts = read_code_files(directory, extensions)\n",
        "    chunks = []\n",
        "    for text in code_texts:\n",
        "        chunks.extend(split_text_with_overlap(text, chunk_size, overlap))\n",
        "    return chunks"
      ],
      "metadata": {
        "id": "JH8B2wcMpCKV"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26FVzsaIjn-o",
        "outputId": "0c4e27f0-aeaf-484a-935c-a77e4c21f957"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunk 1:\n",
            "#!/usr/bin/env python3\n",
            "# -*- coding: utf-8 -*-\n",
            "\n",
            "# This script downloads the tokenizer models of the specified models from Huggingface and\n",
            "# generates the get_vocab_base_pre() function for convert_hf_to_gguf.py\n",
            "#\n",
            "# This is necessary in order to analyze the type of pre-tokenizer used by the model and\n",
            "# provide the necessary information to llama.cpp via the GGUF header in order to implement\n",
            "# the same pre-tokenizer.\n",
            "#\n",
            "# ref: https://github.com/ggerganov/llama.cpp/pull/6920\n",
            "#\n",
            "# Instructions:\n",
            "#\n",
            "# - Add a new model to the \"models\" list\n",
            "# - Run the script with your huggingface token:\n",
            "#\n",
            "#   python3 convert_hf_to_gguf_update.py <huggingface_token>\n",
            "#\n",
            "# - Copy-paste the generated get_vocab_base_pre() function into convert_hf_to_gguf.py\n",
            "# - Update llama.cpp with the new pre-tokenizer if necessary\n",
            "#\n",
            "# TODO: generate tokenizer tests for llama.cpp\n",
            "#\n",
            "\n",
            "import logging\n",
            "import os\n",
            "import pathlib\n",
            "import re\n",
            "\n",
            "import requests\n",
            "import sys\n",
            "import json\n",
            "\n",
            "from hashlib import sha256\n",
            "from enum import IntEnum, auto\n",
            "from transformers import AutoTokenizer\n",
            "\n",
            "logging.basicConfig(level=logging.DEBUG)\n",
            "logger = logging.getLogger(\"convert_hf_to_gguf_update\")\n",
            "sess = requests.Session()\n",
            "\n",
            "\n",
            "class TOKENIZER_TYPE(IntEnum):\n",
            "    SPM = auto()\n",
            "    BPE = auto()\n",
            "    WPM = auto()\n",
            "    UGM = auto()\n",
            "\n",
            "\n",
            "# TODO: this string has to exercise as much pre-tokenizer functionality as possible\n",
            "#       will be updated with time - contributions welcome\n",
            "CHK_TXT = '\\n \\n\\n \\n\\n\\n \\t \\t\\t \\t\\n  \\n   \\n    \\n     \\n🚀 (normal) 😶‍🌫️ (multiple emojis concatenated) ✅ 🦙🦙 3 33 333 3333 33333 333333 3333333 33333333 3.3 3..3 3...3 កាន់តែពិសេសអាច😁 ?我想在apple工作1314151天～ ------======= нещо на Български \\'\\'\\'\\'\\'\\'```````\\\"\\\"\\\"\\\"......!!!!!!?????? I\\'ve been \\'told he\\'s there, \\'RE you sure? \\'M not sure I\\'ll make it, \\'D you like some tea? We\\'Ve a\\'lL'\n",
            "\n",
            "if len(sys.argv) == 2:\n",
            "    token = sys.argv[1]\n",
            "    if not token.startswith(\"hf_\"):\n",
            "        logger.info(\"Huggingface token seems invalid\")\n",
            "        logger.info(\"Usage: python convert_hf_to_gguf_update.py <huggingface_token>\")\n",
            "        sys.exit(1)\n",
            "else:\n",
            "    logger.info(\"Usage: python convert_hf_to_gguf_update.py <huggingface_token>\")\n",
            "    sys.exit(1)\n",
            "\n",
            "# TODO: add models here, base models preferred\n",
            "models = [\n",
            "    {\"name\": \"llama-spm\",      \"tokt\": TOKENIZER_TYPE.SPM, \"repo\": \"https://huggingface.co/meta-llama/Llama-2-7b-hf\", },\n",
            "    {\"name\": \"llama-bpe\",      \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/meta-llama/Meta-\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Chunk 2:\n",
            ">\")\n",
            "        sys.exit(1)\n",
            "else:\n",
            "    logger.info(\"Usage: python convert_hf_to_gguf_update.py <huggingface_token>\")\n",
            "    sys.exit(1)\n",
            "\n",
            "# TODO: add models here, base models preferred\n",
            "models = [\n",
            "    {\"name\": \"llama-spm\",      \"tokt\": TOKENIZER_TYPE.SPM, \"repo\": \"https://huggingface.co/meta-llama/Llama-2-7b-hf\", },\n",
            "    {\"name\": \"llama-bpe\",      \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/meta-llama/Meta-Llama-3-8B\", },\n",
            "    {\"name\": \"phi-3\",          \"tokt\": TOKENIZER_TYPE.SPM, \"repo\": \"https://huggingface.co/microsoft/Phi-3-mini-4k-instruct\", },\n",
            "    {\"name\": \"deepseek-llm\",   \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/deepseek-ai/deepseek-llm-7b-base\", },\n",
            "    {\"name\": \"deepseek-coder\", \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/deepseek-ai/deepseek-coder-6.7b-base\", },\n",
            "    {\"name\": \"falcon\",         \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/tiiuae/falcon-7b\", },\n",
            "    {\"name\": \"bert-bge\",       \"tokt\": TOKENIZER_TYPE.WPM, \"repo\": \"https://huggingface.co/BAAI/bge-small-en-v1.5\", },\n",
            "    {\"name\": \"mpt\",            \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/mosaicml/mpt-7b\", },\n",
            "    {\"name\": \"starcoder\",      \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/bigcode/starcoder2-3b\", },\n",
            "    {\"name\": \"gpt-2\",          \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/openai-community/gpt2\", },\n",
            "    {\"name\": \"stablelm2\",      \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/stabilityai/stablelm-2-zephyr-1_6b\", },\n",
            "    {\"name\": \"refact\",         \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/smallcloudai/Refact-1_6-base\", },\n",
            "    {\"name\": \"command-r\",      \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/CohereForAI/c4ai-command-r-v01\", },\n",
            "    {\"name\": \"qwen2\",          \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/Qwen/Qwen1.5-7B\", },\n",
            "    {\"name\": \"olmo\",           \"tokt\": TOKENIZER_TYPE.BPE, \"re\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Chunk 3:\n",
            "IZER_TYPE.BPE, \"repo\": \"https://huggingface.co/smallcloudai/Refact-1_6-base\", },\n",
            "    {\"name\": \"command-r\",      \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/CohereForAI/c4ai-command-r-v01\", },\n",
            "    {\"name\": \"qwen2\",          \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/Qwen/Qwen1.5-7B\", },\n",
            "    {\"name\": \"olmo\",           \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/allenai/OLMo-1.7-7B-hf\", },\n",
            "    {\"name\": \"dbrx\",           \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/databricks/dbrx-base\", },\n",
            "    {\"name\": \"jina-v2-en\",     \"tokt\": TOKENIZER_TYPE.WPM, \"repo\": \"https://huggingface.co/jinaai/jina-embeddings-v2-base-en\", }, # WPM!\n",
            "    {\"name\": \"jina-v2-es\",     \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/jinaai/jina-embeddings-v2-base-es\", },\n",
            "    {\"name\": \"jina-v2-de\",     \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/jinaai/jina-embeddings-v2-base-de\", },\n",
            "    {\"name\": \"smaug-bpe\",      \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/abacusai/Smaug-Llama-3-70B-Instruct\", },\n",
            "    {\"name\": \"poro-chat\",      \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/LumiOpen/Poro-34B-chat\", },\n",
            "    {\"name\": \"jina-v2-code\",   \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/jinaai/jina-embeddings-v2-base-code\", },\n",
            "    {\"name\": \"viking\",         \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/LumiOpen/Viking-7B\", }, # Also used for Viking 13B and 33B\n",
            "    {\"name\": \"gemma\",          \"tokt\": TOKENIZER_TYPE.SPM, \"repo\": \"https://huggingface.co/google/gemma-2b\", },\n",
            "    {\"name\": \"gemma-2\",        \"tokt\": TOKENIZER_TYPE.SPM, \"repo\": \"https://huggingface.co/google/gemma-2-9b\", },\n",
            "    {\"name\": \"jais\",           \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/core42/jais-13b\", },\n",
            "    {\"name\": \"t5\",             \"tokt\": TOKENIZER_TYPE.UGM, \"repo\": \"https://huggingface.co/google-t5/t5-\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Chunk 4:\n",
            "\": \"https://huggingface.co/google/gemma-2b\", },\n",
            "    {\"name\": \"gemma-2\",        \"tokt\": TOKENIZER_TYPE.SPM, \"repo\": \"https://huggingface.co/google/gemma-2-9b\", },\n",
            "    {\"name\": \"jais\",           \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/core42/jais-13b\", },\n",
            "    {\"name\": \"t5\",             \"tokt\": TOKENIZER_TYPE.UGM, \"repo\": \"https://huggingface.co/google-t5/t5-small\", },\n",
            "    {\"name\": \"codeshell\",      \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/WisdomShell/CodeShell-7B\", },\n",
            "    {\"name\": \"tekken\",         \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/mistralai/Mistral-Nemo-Base-2407\", },\n",
            "    {\"name\": \"smollm\",         \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/HuggingFaceTB/SmolLM-135M\", },\n",
            "]\n",
            "\n",
            "\n",
            "def download_file_with_auth(url, token, save_path):\n",
            "    headers = {\"Authorization\": f\"Bearer {token}\"}\n",
            "    response = sess.get(url, headers=headers)\n",
            "    response.raise_for_status()\n",
            "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
            "    with open(save_path, 'wb') as downloaded_file:\n",
            "        downloaded_file.write(response.content)\n",
            "    logger.info(f\"File {save_path} downloaded successfully\")\n",
            "\n",
            "\n",
            "def download_model(model):\n",
            "    name = model[\"name\"]\n",
            "    repo = model[\"repo\"]\n",
            "    tokt = model[\"tokt\"]\n",
            "\n",
            "    os.makedirs(f\"models/tokenizers/{name}\", exist_ok=True)\n",
            "\n",
            "    files = [\"config.json\", \"tokenizer.json\", \"tokenizer_config.json\"]\n",
            "\n",
            "    if tokt == TOKENIZER_TYPE.SPM:\n",
            "        files.append(\"tokenizer.model\")\n",
            "\n",
            "    if tokt == TOKENIZER_TYPE.UGM:\n",
            "        files.append(\"spiece.model\")\n",
            "\n",
            "    for file in files:\n",
            "        save_path = f\"models/tokenizers/{name}/{file}\"\n",
            "        if os.path.isfile(save_path):\n",
            "            logger.info(f\"{name}: File {save_path} already exists - skipping\")\n",
            "            continue\n",
            "        download_file_with_auth(f\"{repo}/resolve/main/{file}\", token, save_path)\n",
            "\n",
            "\n",
            "for model in models:\n",
            "    try:\n",
            "        download_model(model)\n",
            "    except Exception as e:\n",
            "        logger.error(f\"Failed to download model {model['name']}. Error: {e}\")\n",
            "\n",
            "\n",
            "# generate the source code for the convert_hf_to_gguf.py:get_vocab_base_pre() function:\n",
            "\n",
            "src_ifs = \"\"\n",
            "for model in models:\n",
            "    name = model[\"name\"]\n",
            "    tokt = model[\"tokt\"]\n",
            "\n",
            "    if tokt == TOKENIZER_TYPE.SPM or tokt == TOKENIZER\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Chunk 5:\n",
            " continue\n",
            "        download_file_with_auth(f\"{repo}/resolve/main/{file}\", token, save_path)\n",
            "\n",
            "\n",
            "for model in models:\n",
            "    try:\n",
            "        download_model(model)\n",
            "    except Exception as e:\n",
            "        logger.error(f\"Failed to download model {model['name']}. Error: {e}\")\n",
            "\n",
            "\n",
            "# generate the source code for the convert_hf_to_gguf.py:get_vocab_base_pre() function:\n",
            "\n",
            "src_ifs = \"\"\n",
            "for model in models:\n",
            "    name = model[\"name\"]\n",
            "    tokt = model[\"tokt\"]\n",
            "\n",
            "    if tokt == TOKENIZER_TYPE.SPM or tokt == TOKENIZER_TYPE.UGM:\n",
            "        continue\n",
            "\n",
            "    # Skip if the tokenizer folder does not exist or there are other download issues previously\n",
            "    if not os.path.exists(f\"models/tokenizers/{name}\"):\n",
            "        logger.warning(f\"Directory for tokenizer {name} not found. Skipping...\")\n",
            "        continue\n",
            "\n",
            "    # create the tokenizer\n",
            "    try:\n",
            "        if name == \"t5\":\n",
            "            tokenizer = AutoTokenizer.from_pretrained(f\"models/tokenizers/{name}\", use_fast=False)\n",
            "        else:\n",
            "            tokenizer = AutoTokenizer.from_pretrained(f\"models/tokenizers/{name}\")\n",
            "    except OSError as e:\n",
            "        logger.error(f\"Error loading tokenizer for model {name}. The model may not exist or is not accessible with the provided token. Error: {e}\")\n",
            "        continue  # Skip to the next model if the tokenizer can't be loaded\n",
            "\n",
            "    chktok = tokenizer.encode(CHK_TXT)\n",
            "    chkhsh = sha256(str(chktok).encode()).hexdigest()\n",
            "\n",
            "    logger.info(f\"model: {name}\")\n",
            "    logger.info(f\"tokt: {tokt}\")\n",
            "    logger.info(f\"repo: {model['repo']}\")\n",
            "    logger.info(f\"chktok: {chktok}\")\n",
            "    logger.info(f\"chkhsh: {chkhsh}\")\n",
            "\n",
            "    # print the \"pre_tokenizer\" content from the tokenizer.json\n",
            "    with open(f\"models/tokenizers/{name}/tokenizer.json\", \"r\", encoding=\"utf-8\") as f:\n",
            "        cfg = json.load(f)\n",
            "        normalizer = cfg[\"normalizer\"]\n",
            "        logger.info(\"normalizer: \" + json.dumps(normalizer, indent=4))\n",
            "        pre_tokenizer = cfg[\"pre_tokenizer\"]\n",
            "        logger.info(\"pre_tokenizer: \" + json.dumps(pre_tokenizer, indent=4))\n",
            "        if \"ignore_merges\" in cfg[\"model\"]:\n",
            "            logger.info(\"ignore_merges: \" + json.dumps(cfg[\"model\"][\"ignore_merges\"], indent=4))\n",
            "\n",
            "    logger.info(\"\")\n",
            "\n",
            "    src_ifs += f\"        if chkhsh == \\\"{chkhsh}\\\":\\n\"\n",
            "    src_ifs += f\"            # ref: {model['repo']}\\n\"\n",
            "    src_ifs += f\"            res = \\\"{name}\\\"\\n\"\n",
            "\n",
            "src_func = f\"\"\"\n",
            "    def get_vocab_base_\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Chunk 6:\n",
            "tokenizer, indent=4))\n",
            "        if \"ignore_merges\" in cfg[\"model\"]:\n",
            "            logger.info(\"ignore_merges: \" + json.dumps(cfg[\"model\"][\"ignore_merges\"], indent=4))\n",
            "\n",
            "    logger.info(\"\")\n",
            "\n",
            "    src_ifs += f\"        if chkhsh == \\\"{chkhsh}\\\":\\n\"\n",
            "    src_ifs += f\"            # ref: {model['repo']}\\n\"\n",
            "    src_ifs += f\"            res = \\\"{name}\\\"\\n\"\n",
            "\n",
            "src_func = f\"\"\"\n",
            "    def get_vocab_base_pre(self, tokenizer) -> str:\n",
            "        # encoding this string and hashing the resulting tokens would (hopefully) give us a unique identifier that\n",
            "        # is specific for the BPE pre-tokenizer used by the model\n",
            "        # we will use this unique identifier to write a \"tokenizer.ggml.pre\" entry in the GGUF file which we can\n",
            "        # use in llama.cpp to implement the same pre-tokenizer\n",
            "\n",
            "        chktxt = {repr(CHK_TXT)}\n",
            "\n",
            "        chktok = tokenizer.encode(chktxt)\n",
            "        chkhsh = sha256(str(chktok).encode()).hexdigest()\n",
            "\n",
            "        logger.debug(f\"chktok: {{chktok}}\")\n",
            "        logger.debug(f\"chkhsh: {{chkhsh}}\")\n",
            "\n",
            "        res = None\n",
            "\n",
            "        # NOTE: if you get an error here, you need to update the convert_hf_to_gguf_update.py script\n",
            "        #       or pull the latest version of the model from Huggingface\n",
            "        #       don't edit the hashes manually!\n",
            "{src_ifs}\n",
            "        if res is None:\n",
            "            logger.warning(\"\\\\n\")\n",
            "            logger.warning(\"**************************************************************************************\")\n",
            "            logger.warning(\"** WARNING: The BPE pre-tokenizer was not recognized!\")\n",
            "            logger.warning(\"**          There are 2 possible reasons for this:\")\n",
            "            logger.warning(\"**          - the model has not been added to convert_hf_to_gguf_update.py yet\")\n",
            "            logger.warning(\"**          - the pre-tokenization config has changed upstream\")\n",
            "            logger.warning(\"**          Check your model files and convert_hf_to_gguf_update.py and update them accordingly.\")\n",
            "            logger.warning(\"** ref:     https://github.com/ggerganov/llama.cpp/pull/6920\")\n",
            "            logger.warning(\"**\")\n",
            "            logger.warning(f\"** chkhsh:  {{chkhsh}}\")\n",
            "            logger.warning(\"**************************************************************************************\")\n",
            "            logger.warning(\"\\\\n\")\n",
            "            raise NotImplementedError(\"BPE pre-tokenizer was not recognized - update get_vocab_base_pre()\")\n",
            "\n",
            "        logger.debug(f\"tokenizer.ggml.pre: {{repr(res\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Chunk 7:\n",
            ".\")\n",
            "            logger.warning(\"** ref:     https://github.com/ggerganov/llama.cpp/pull/6920\")\n",
            "            logger.warning(\"**\")\n",
            "            logger.warning(f\"** chkhsh:  {{chkhsh}}\")\n",
            "            logger.warning(\"**************************************************************************************\")\n",
            "            logger.warning(\"\\\\n\")\n",
            "            raise NotImplementedError(\"BPE pre-tokenizer was not recognized - update get_vocab_base_pre()\")\n",
            "\n",
            "        logger.debug(f\"tokenizer.ggml.pre: {{repr(res)}}\")\n",
            "        logger.debug(f\"chkhsh: {{chkhsh}}\")\n",
            "\n",
            "        return res\n",
            "\"\"\"\n",
            "\n",
            "convert_py_pth = pathlib.Path(\"convert_hf_to_gguf.py\")\n",
            "convert_py = convert_py_pth.read_text(encoding=\"utf-8\")\n",
            "convert_py = re.sub(\n",
            "    r\"(# Marker: Start get_vocab_base_pre)(.+?)( +# Marker: End get_vocab_base_pre)\",\n",
            "    lambda m: m.group(1) + src_func + m.group(3),\n",
            "    convert_py,\n",
            "    flags=re.DOTALL | re.MULTILINE,\n",
            ")\n",
            "\n",
            "convert_py_pth.write_text(convert_py, encoding=\"utf-8\")\n",
            "\n",
            "logger.info(\"+++ convert_hf_to_gguf.py was updated\")\n",
            "\n",
            "# generate tests for each tokenizer model\n",
            "\n",
            "tests = [\n",
            "    \"ied 4 ½ months\",\n",
            "    \"Führer\",\n",
            "    \"\",\n",
            "    \" \",\n",
            "    \"  \",\n",
            "    \"   \",\n",
            "    \"\\t\",\n",
            "    \"\\n\",\n",
            "    \"\\n\\n\",\n",
            "    \"\\n\\n\\n\",\n",
            "    \"\\t\\n\",\n",
            "    \"Hello world\",\n",
            "    \" Hello world\",\n",
            "    \"Hello World\",\n",
            "    \" Hello World\",\n",
            "    \" Hello World!\",\n",
            "    \"Hello, world!\",\n",
            "    \" Hello, world!\",\n",
            "    \" this is 🦙.cpp\",\n",
            "    \"w048 7tuijk dsdfhu\",\n",
            "    \"нещо на Български\",\n",
            "    \"កាន់តែពិសេសអាចខលចេញ\",\n",
            "    \"🚀 (normal) 😶‍🌫️ (multiple emojis concatenated) ✅ (only emoji that has its own token)\",\n",
            "    \"Hello\",\n",
            "    \" Hello\",\n",
            "    \"  Hello\",\n",
            "    \"   Hello\",\n",
            "    \"    Hello\",\n",
            "    \"    Hello\\n    Hello\",\n",
            "    \" (\",\n",
            "    \"\\n =\",\n",
            "    \"' era\",\n",
            "    \"Hello, y'all! How are you 😁 ?我想在apple工作1314151天～\",\n",
            "    \"!!!!!!\",\n",
            "    \"3\",\n",
            "    \"33\",\n",
            "    \"333\",\n",
            "    \"3333\",\n",
            "    \"33333\",\n",
            "    \"333333\",\n",
            "    \"3333333\",\n",
            "    \"33333333\",\n",
            "    \"333333333\",\n",
            "    \"Cửa Việt\", # llama-bpe fails on this\n",
            "    \" discards\",\n",
            "    CHK_TXT,\n",
            "]\n",
            "\n",
            "# write the tests\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Chunk 8:\n",
            "    \"    Hello\\n    Hello\",\n",
            "    \" (\",\n",
            "    \"\\n =\",\n",
            "    \"' era\",\n",
            "    \"Hello, y'all! How are you 😁 ?我想在apple工作1314151天～\",\n",
            "    \"!!!!!!\",\n",
            "    \"3\",\n",
            "    \"33\",\n",
            "    \"333\",\n",
            "    \"3333\",\n",
            "    \"33333\",\n",
            "    \"333333\",\n",
            "    \"3333333\",\n",
            "    \"33333333\",\n",
            "    \"333333333\",\n",
            "    \"Cửa Việt\", # llama-bpe fails on this\n",
            "    \" discards\",\n",
            "    CHK_TXT,\n",
            "]\n",
            "\n",
            "# write the tests to ./models/ggml-vocab-{name}.gguf.inp\n",
            "# the format is:\n",
            "#\n",
            "# test0\n",
            "# __ggml_vocab_test__\n",
            "# test1\n",
            "# __ggml_vocab_test__\n",
            "# ...\n",
            "#\n",
            "\n",
            "# with each model, encode all tests and write the results in ./models/ggml-vocab-{name}.gguf.out\n",
            "# for each test, write the resulting tokens on a separate line\n",
            "\n",
            "for model in models:\n",
            "    name = model[\"name\"]\n",
            "    tokt = model[\"tokt\"]\n",
            "\n",
            "    # Skip if the tokenizer folder does not exist or there are other download issues previously\n",
            "    if not os.path.exists(f\"models/tokenizers/{name}\"):\n",
            "        logger.warning(f\"Directory for tokenizer {name} not found. Skipping...\")\n",
            "        continue\n",
            "\n",
            "    # create the tokenizer\n",
            "    try:\n",
            "        if name == \"t5\":\n",
            "            tokenizer = AutoTokenizer.from_pretrained(f\"models/tokenizers/{name}\", use_fast=False)\n",
            "        else:\n",
            "            tokenizer = AutoTokenizer.from_pretrained(f\"models/tokenizers/{name}\")\n",
            "    except OSError as e:\n",
            "        logger.error(f\"Failed to load tokenizer for model {name}. Error: {e}\")\n",
            "        continue  # Skip this model and continue with the next one in the loop\n",
            "\n",
            "    with open(f\"models/ggml-vocab-{name}.gguf.inp\", \"w\", encoding=\"utf-8\") as f:\n",
            "        for text in tests:\n",
            "            f.write(f\"{text}\")\n",
            "            f.write(\"\\n__ggml_vocab_test__\\n\")\n",
            "\n",
            "    with open(f\"models/ggml-vocab-{name}.gguf.out\", \"w\") as f:\n",
            "        for text in tests:\n",
            "            res = tokenizer.encode(text, add_special_tokens=False)\n",
            "            for r in res:\n",
            "                f.write(f\" {r}\")\n",
            "            f.write(\"\\n\")\n",
            "\n",
            "    logger.info(f\"Tests for {name} written in ./models/ggml-vocab-{name}.gguf.*\")\n",
            "\n",
            "# generate commands for creating vocab files\n",
            "\n",
            "logger.info(\"\\nRun the following commands to generate the vocab files for testing:\\n\")\n",
            "\n",
            "for model in models:\n",
            "    name = model[\"name\"]\n",
            "\n",
            "    print(f\"python3 convert_hf_to_gguf.py models/tokenizers/{name}/ --outfile models/ggml-vocab-{name}.gguf --vocab-only\") # noqa: NP100\n",
            "\n",
            "logger.info(\"\\n\")\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Chunk 9:\n",
            "#!/usr/bin/env python3\n",
            "from __future__ import annotations\n",
            "\n",
            "import logging\n",
            "import argparse\n",
            "import os\n",
            "import struct\n",
            "import sys\n",
            "from enum import IntEnum\n",
            "from pathlib import Path\n",
            "\n",
            "import numpy as np\n",
            "\n",
            "if 'NO_LOCAL_GGUF' not in os.environ:\n",
            "    sys.path.insert(1, str(Path(__file__).parent / 'gguf-py'))\n",
            "import gguf\n",
            "\n",
            "logger = logging.getLogger(\"ggml-to-gguf\")\n",
            "\n",
            "\n",
            "class GGMLFormat(IntEnum):\n",
            "    GGML = 0\n",
            "    GGMF = 1\n",
            "    GGJT = 2\n",
            "\n",
            "\n",
            "class GGMLFType(IntEnum):\n",
            "    ALL_F32              = 0\n",
            "    MOSTLY_F16           = 1\n",
            "    MOSTLY_Q4_0          = 2\n",
            "    MOSTLY_Q4_1          = 3\n",
            "    MOSTLY_Q4_1_SOME_F16 = 4\n",
            "    MOSTLY_Q8_0          = 7\n",
            "    MOSTLY_Q5_0          = 8\n",
            "    MOSTLY_Q5_1          = 9\n",
            "    MOSTLY_Q2_K          = 10\n",
            "    MOSTLY_Q3_K_S        = 11\n",
            "    MOSTLY_Q3_K_M        = 12\n",
            "    MOSTLY_Q3_K_L        = 13\n",
            "    MOSTLY_Q4_K_S        = 14\n",
            "    MOSTLY_Q4_K_M        = 15\n",
            "    MOSTLY_Q5_K_S        = 16\n",
            "    MOSTLY_Q5_K_M        = 17\n",
            "    MOSTLY_Q6_K          = 18\n",
            "\n",
            "\n",
            "class Hyperparameters:\n",
            "    def __init__(self):\n",
            "        self.n_vocab = self.n_embd = self.n_mult = self.n_head = 0\n",
            "        self.n_layer = self.n_rot = self.n_ff = 0\n",
            "        self.ftype = GGMLFType.ALL_F32\n",
            "\n",
            "    def set_n_ff(self, model):\n",
            "        ff_tensor_idx = model.tensor_map.get(b'layers.0.feed_forward.w1.weight')\n",
            "        assert ff_tensor_idx is not None, 'Missing layer 0 FF tensor'\n",
            "        ff_tensor = model.tensors[ff_tensor_idx]\n",
            "        self.n_ff = ff_tensor.dims[1]\n",
            "\n",
            "    def load(self, data, offset):\n",
            "        (\n",
            "            self.n_vocab,\n",
            "            self.n_embd,\n",
            "            self.n_mult,\n",
            "            self.n_head,\n",
            "            self.n_layer,\n",
            "            self.n_rot,\n",
            "            ftype,\n",
            "        ) = struct.unpack('<7I', data[offset:offset + (4 * 7)])\n",
            "        try:\n",
            "            self.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Chunk 10:\n",
            "   def load(self, data, offset):\n",
            "        (\n",
            "            self.n_vocab,\n",
            "            self.n_embd,\n",
            "            self.n_mult,\n",
            "            self.n_head,\n",
            "            self.n_layer,\n",
            "            self.n_rot,\n",
            "            ftype,\n",
            "        ) = struct.unpack('<7I', data[offset:offset + (4 * 7)])\n",
            "        try:\n",
            "            self.ftype = GGMLFType(ftype)\n",
            "        except ValueError:\n",
            "            raise ValueError(f'Invalid ftype {ftype}')\n",
            "        return 4 * 7\n",
            "\n",
            "    def __str__(self):\n",
            "        return f'<Hyperparameters: n_vocab={self.n_vocab}, n_embd={self.n_embd}, n_mult={self.n_mult}, n_head={self.n_head}, n_layer={self.n_layer}, n_rot={self.n_rot}, n_ff={self.n_ff}, ftype={self.ftype.name}>'\n",
            "\n",
            "\n",
            "class Vocab:\n",
            "    def __init__(self, load_scores = True):\n",
            "        self.items = []\n",
            "        self.load_scores = load_scores\n",
            "\n",
            "    def load(self, data, offset, n_vocab):\n",
            "        orig_offset = offset\n",
            "        for _ in range(n_vocab):\n",
            "            itemlen = struct.unpack('<I', data[offset:offset + 4])[0]\n",
            "            assert itemlen < 4096, 'Absurd vocab item length'\n",
            "            offset += 4\n",
            "            item_text = bytes(data[offset:offset + itemlen])\n",
            "            offset += itemlen\n",
            "            if self.load_scores:\n",
            "                item_score = struct.unpack('<f', data[offset:offset + 4])[0]\n",
            "                offset += 4\n",
            "            else:\n",
            "                item_score = 0.0\n",
            "            self.items.append((item_text, item_score))\n",
            "        return offset - orig_offset\n",
            "\n",
            "\n",
            "class Tensor:\n",
            "    def __init__(self, use_padding = True):\n",
            "        self.name = None\n",
            "        self.dims: tuple[int, ...] = ()\n",
            "        self.dtype = None\n",
            "        self.start_offset = 0\n",
            "        self.len_bytes = np.int64(0)\n",
            "        self.use_padding = use_padding\n",
            "\n",
            "    def load(self, data, offset):\n",
            "        orig_offset = offset\n",
            "        (n_dims, name_len, dtype) = struct.unpack('<3I', data[offset:offset + 12])\n",
            "        assert n_dims >= 0 and n_dims <= 4, f'Invalid tensor dimensions {n_dims}'\n",
            "        assert name_len < 4096, 'Absurd tensor name length'\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Chunk 11:\n",
            ": tuple[int, ...] = ()\n",
            "        self.dtype = None\n",
            "        self.start_offset = 0\n",
            "        self.len_bytes = np.int64(0)\n",
            "        self.use_padding = use_padding\n",
            "\n",
            "    def load(self, data, offset):\n",
            "        orig_offset = offset\n",
            "        (n_dims, name_len, dtype) = struct.unpack('<3I', data[offset:offset + 12])\n",
            "        assert n_dims >= 0 and n_dims <= 4, f'Invalid tensor dimensions {n_dims}'\n",
            "        assert name_len < 4096, 'Absurd tensor name length'\n",
            "        quant = gguf.GGML_QUANT_SIZES.get(dtype)\n",
            "        assert quant is not None, 'Unknown tensor type'\n",
            "        (blksize, tysize) = quant\n",
            "        offset += 12\n",
            "        self.dtype= dtype\n",
            "        self.dims = struct.unpack(f'<{n_dims}I', data[offset:offset + (4 * n_dims)])\n",
            "        offset += 4 * n_dims\n",
            "        self.name = bytes(data[offset:offset + name_len])\n",
            "        offset += name_len\n",
            "        pad = ((offset + 31) & ~31) - offset if self.use_padding else 0\n",
            "        offset += pad\n",
            "        n_elems = np.prod(self.dims)\n",
            "        n_bytes = np.int64(np.int64(n_elems) * np.int64(tysize)) // np.int64(blksize)\n",
            "        self.start_offset = offset\n",
            "        self.len_bytes = n_bytes\n",
            "        offset += n_bytes\n",
            "        return offset - orig_offset\n",
            "\n",
            "\n",
            "class GGMLModel:\n",
            "\n",
            "    file_format: GGMLFormat\n",
            "    format_version: int\n",
            "\n",
            "    def __init__(self):\n",
            "        self.hyperparameters = None\n",
            "        self.vocab = None\n",
            "        self.tensor_map = {}\n",
            "        self.tensors = []\n",
            "\n",
            "    def validate_header(self, data, offset):\n",
            "        magic = bytes(data[offset:offset + 4])\n",
            "        if magic == b'GGUF':\n",
            "            raise ValueError('File is already in GGUF format.')\n",
            "        if magic == b'lmgg':\n",
            "            self.file_format = GGMLFormat.GGML\n",
            "            self.format_version = 1\n",
            "            return 4\n",
            "        version = struct.unpack('<I', data[offset + 4:offset + 8])[0]\n",
            "        if magic == b'fmgg':\n",
            "            if version != 1:\n",
            "                raise ValueError(f'Cannot handle unexpected GGMF file version {version}')\n",
            "            self.file_format = GGMLFormat.GGMF\n",
            "            self.format_version = version\n",
            "            return 8\n",
            "        if magic == b'tjgg':\n",
            "            if version < 1 or version > 3:\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Chunk 12:\n",
            "      return 4\n",
            "        version = struct.unpack('<I', data[offset + 4:offset + 8])[0]\n",
            "        if magic == b'fmgg':\n",
            "            if version != 1:\n",
            "                raise ValueError(f'Cannot handle unexpected GGMF file version {version}')\n",
            "            self.file_format = GGMLFormat.GGMF\n",
            "            self.format_version = version\n",
            "            return 8\n",
            "        if magic == b'tjgg':\n",
            "            if version < 1 or version > 3:\n",
            "                raise ValueError(f'Cannot handle unexpected GGJT file version {version}')\n",
            "            self.file_format = GGMLFormat.GGJT\n",
            "            self.format_version = version\n",
            "            return 8\n",
            "        raise ValueError(f\"Unexpected file magic {magic!r}! This doesn't look like a GGML format file.\")\n",
            "\n",
            "    def validate_conversion(self, ftype):\n",
            "        err = ''\n",
            "        if (self.file_format < GGMLFormat.GGJT or self.format_version < 2):\n",
            "            if ftype not in (GGMLFType.ALL_F32, GGMLFType.MOSTLY_F16):\n",
            "                err = 'Quantizations changed in GGJTv2. Can only convert unquantized GGML files older than GGJTv2.'\n",
            "        elif (self.file_format == GGMLFormat.GGJT and self.format_version == 2):\n",
            "            if ftype in (GGMLFType.MOSTLY_Q4_0, GGMLFType.MOSTLY_Q4_1,\n",
            "                         GGMLFType.MOSTLY_Q4_1_SOME_F16, GGMLFType.MOSTLY_Q8_0):\n",
            "                err = 'Q4 and Q8 quantizations changed in GGJTv3.'\n",
            "        if len(err) > 0:\n",
            "            raise ValueError(f'{err} Sorry, your {self.file_format.name}v{self.format_version} file of type {ftype.name} is not eligible for conversion.')\n",
            "\n",
            "    def load(self, data, offset):\n",
            "        offset += self.validate_header(data, offset)\n",
            "        hp = Hyperparameters()\n",
            "        offset += hp.load(data, offset)\n",
            "        logger.info(f'* File format: {self.file_format.name}v{self.format_version} with ftype {hp.ftype.name}')\n",
            "        self.validate_conversion(hp.ftype)\n",
            "        vocab = Vocab(load_scores = self.file_format > GGMLFormat.GGML)\n",
            "        offset += vocab.load(data, offset, hp.n_vocab)\n",
            "        tensors: list[Tensor] = []\n",
            "        tensor_map = {}\n",
            "        while offset < len(data):\n",
            "            tensor = Tensor(use_padding = self.file_format > GGMLFormat.GGMF)\n",
            "         \n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Chunk 13:\n",
            " {self.file_format.name}v{self.format_version} with ftype {hp.ftype.name}')\n",
            "        self.validate_conversion(hp.ftype)\n",
            "        vocab = Vocab(load_scores = self.file_format > GGMLFormat.GGML)\n",
            "        offset += vocab.load(data, offset, hp.n_vocab)\n",
            "        tensors: list[Tensor] = []\n",
            "        tensor_map = {}\n",
            "        while offset < len(data):\n",
            "            tensor = Tensor(use_padding = self.file_format > GGMLFormat.GGMF)\n",
            "            offset += tensor.load(data, offset)\n",
            "            tensor_map[tensor.name] = len(tensors)\n",
            "            tensors.append(tensor)\n",
            "        self.hyperparameters = hp\n",
            "        self.vocab = vocab\n",
            "        self.tensors = tensors\n",
            "        self.tensor_map = tensor_map\n",
            "        hp.set_n_ff(self)\n",
            "        return offset\n",
            "\n",
            "\n",
            "class GGMLToGGUF:\n",
            "    def __init__(self, ggml_model, data, cfg, params_override = None, vocab_override = None, special_vocab = None):\n",
            "        hp = ggml_model.hyperparameters\n",
            "        self.model = ggml_model\n",
            "        self.data = data\n",
            "        self.cfg = cfg\n",
            "        self.params_override = params_override\n",
            "        self.vocab_override = vocab_override\n",
            "        self.special_vocab = special_vocab\n",
            "        if params_override is not None:\n",
            "            n_kv_head = params_override.n_head_kv\n",
            "        else:\n",
            "            if cfg.gqa == 1:\n",
            "                n_kv_head = hp.n_head\n",
            "            else:\n",
            "                gqa = float(cfg.gqa)\n",
            "                n_kv_head = None\n",
            "                for x in range(1, 256):\n",
            "                    if float(hp.n_head) / float(x) == gqa:\n",
            "                        n_kv_head = x\n",
            "                assert n_kv_head is not None, \"Couldn't determine n_kv_head from GQA param\"\n",
            "                logger.info(f'- Guessed n_kv_head = {n_kv_head} based on GQA {cfg.gqa}')\n",
            "        self.n_kv_head = n_kv_head\n",
            "        self.name_map = gguf.get_tensor_name_map(gguf.MODEL_ARCH.LLAMA, ggml_model.hyperparameters.n_layer)\n",
            "\n",
            "    def save(self):\n",
            "        logger.info('* Preparing to save GGUF file')\n",
            "    \n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Chunk 14:\n",
            "            assert n_kv_head is not None, \"Couldn't determine n_kv_head from GQA param\"\n",
            "                logger.info(f'- Guessed n_kv_head = {n_kv_head} based on GQA {cfg.gqa}')\n",
            "        self.n_kv_head = n_kv_head\n",
            "        self.name_map = gguf.get_tensor_name_map(gguf.MODEL_ARCH.LLAMA, ggml_model.hyperparameters.n_layer)\n",
            "\n",
            "    def save(self):\n",
            "        logger.info('* Preparing to save GGUF file')\n",
            "        gguf_writer = gguf.GGUFWriter(\n",
            "            self.cfg.output,\n",
            "            gguf.MODEL_ARCH_NAMES[gguf.MODEL_ARCH.LLAMA],\n",
            "            use_temp_file = False)\n",
            "        self.add_params(gguf_writer)\n",
            "        self.add_vocab(gguf_writer)\n",
            "        if self.special_vocab is not None:\n",
            "            self.special_vocab.add_to_gguf(gguf_writer)\n",
            "        self.add_tensors(gguf_writer)\n",
            "        logger.info(\"    gguf: write header\")\n",
            "        gguf_writer.write_header_to_file()\n",
            "        logger.info(\"    gguf: write metadata\")\n",
            "        gguf_writer.write_kv_data_to_file()\n",
            "        logger.info(\"    gguf: write tensors\")\n",
            "        gguf_writer.write_tensors_to_file()\n",
            "        gguf_writer.close()\n",
            "\n",
            "    def add_params(self, gguf_writer):\n",
            "        hp = self.model.hyperparameters\n",
            "        cfg = self.cfg\n",
            "        if cfg.desc is not None:\n",
            "            desc = cfg.desc\n",
            "        else:\n",
            "            desc = f'converted from legacy {self.model.file_format.name}v{self.model.format_version} {hp.ftype.name} format'\n",
            "        try:\n",
            "            # Filenames aren't necessarily valid UTF8.\n",
            "            name = cfg.name if cfg.name is not None else cfg.input.name\n",
            "        except UnicodeDecodeError:\n",
            "            name = None\n",
            "        logger.info('* Adding model parameters and KV items')\n",
            "        if name is not None:\n",
            "            gguf_writer.add_name(name)\n",
            "        gguf_writer.add_description(desc)\n",
            "        gguf_writer.add_file_type(int(hp.ftype))\n",
            "        if self.params_override is not None:\n",
            "            po = self.params_override\n",
            "            assert po.n_embd == hp.n_embd, 'Model hyperparams mismatch'\n",
            "            assert po.n_layer == hp.n_layer, 'Model hyperparams\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Chunk 15:\n",
            "  logger.info('* Adding model parameters and KV items')\n",
            "        if name is not None:\n",
            "            gguf_writer.add_name(name)\n",
            "        gguf_writer.add_description(desc)\n",
            "        gguf_writer.add_file_type(int(hp.ftype))\n",
            "        if self.params_override is not None:\n",
            "            po = self.params_override\n",
            "            assert po.n_embd == hp.n_embd, 'Model hyperparams mismatch'\n",
            "            assert po.n_layer == hp.n_layer, 'Model hyperparams mismatch'\n",
            "            assert po.n_head == hp.n_head, 'Model hyperparams mismatch'\n",
            "            gguf_writer.add_context_length      (po.n_ctx)\n",
            "            gguf_writer.add_embedding_length    (po.n_embd)\n",
            "            gguf_writer.add_block_count         (po.n_layer)\n",
            "            gguf_writer.add_feed_forward_length (po.n_ff)\n",
            "            gguf_writer.add_rope_dimension_count(po.n_embd // po.n_head)\n",
            "            gguf_writer.add_head_count          (po.n_head)\n",
            "            gguf_writer.add_head_count_kv       (po.n_head_kv)\n",
            "            gguf_writer.add_layer_norm_rms_eps  (po.f_norm_eps)\n",
            "            return\n",
            "        gguf_writer.add_context_length(cfg.context_length)\n",
            "        gguf_writer.add_embedding_length(hp.n_embd)\n",
            "        gguf_writer.add_block_count(hp.n_layer)\n",
            "        gguf_writer.add_feed_forward_length(hp.n_ff)\n",
            "        gguf_writer.add_rope_dimension_count(hp.n_embd // hp.n_head)\n",
            "        gguf_writer.add_head_count(hp.n_head)\n",
            "        gguf_writer.add_head_count_kv(self.n_kv_head)\n",
            "        gguf_writer.add_layer_norm_rms_eps(float(cfg.eps))\n",
            "\n",
            "    def add_vocab(self, gguf_writer):\n",
            "        hp = self.model.hyperparameters\n",
            "        gguf_writer.add_tokenizer_model('llama')\n",
            "        gguf_writer.add_tokenizer_pre('default')\n",
            "        tokens = []\n",
            "        scores = []\n",
            "        toktypes = []\n",
            "        if self.vocab_override is not None:\n",
            "            vo = self.vocab_override\n",
            "            logger.info('* Adding vocab item(s)')\n",
            "            for (_, (vbytes, score, ttype)) in enumerate(vo.all_tokens()):\n",
            " \n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Chunk 16:\n",
            "     hp = self.model.hyperparameters\n",
            "        gguf_writer.add_tokenizer_model('llama')\n",
            "        gguf_writer.add_tokenizer_pre('default')\n",
            "        tokens = []\n",
            "        scores = []\n",
            "        toktypes = []\n",
            "        if self.vocab_override is not None:\n",
            "            vo = self.vocab_override\n",
            "            logger.info('* Adding vocab item(s)')\n",
            "            for (_, (vbytes, score, ttype)) in enumerate(vo.all_tokens()):\n",
            "                tokens.append(vbytes)\n",
            "                scores.append(score)\n",
            "                toktypes.append(ttype)\n",
            "            assert len(tokens) == hp.n_vocab, \\\n",
            "                f'Override vocab has a different number of items than hyperparameters - override = {len(tokens)} but n_vocab={hp.n_vocab}'\n",
            "            gguf_writer.add_token_list(tokens)\n",
            "            gguf_writer.add_token_scores(scores)\n",
            "            if len(toktypes) > 0:\n",
            "                gguf_writer.add_token_types(toktypes)\n",
            "            return\n",
            "        logger.info(f'* Adding {hp.n_vocab} vocab item(s)')\n",
            "        assert len(self.model.vocab.items) >= 3, 'Cannot handle unexpectedly short model vocab'\n",
            "        for (tokid, (vbytes, vscore)) in enumerate(self.model.vocab.items):\n",
            "            tt = 1 # Normal\n",
            "            # Special handling for UNK, BOS, EOS tokens.\n",
            "            if tokid <= 2:\n",
            "                if tokid == 0:\n",
            "                    vbytes = b'<unk>'\n",
            "                    tt = 2\n",
            "                elif tokid == 1:\n",
            "                    vbytes = b'<s>'\n",
            "                    tt = 3\n",
            "                else:\n",
            "                    vbytes = b'</s>'\n",
            "                    tt = 3\n",
            "            elif len(vbytes) == 0:\n",
            "                tt = 3 # Control\n",
            "            elif tokid >= 3 and tokid <= 258 and len(vbytes) == 1:\n",
            "                vbytes = bytes(f'<0x{vbytes[0]:02X}>', encoding = 'UTF-8')\n",
            "                tt = 6 # Byte\n",
            " \n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Chunk 17:\n",
            "  else:\n",
            "                    vbytes = b'</s>'\n",
            "                    tt = 3\n",
            "            elif len(vbytes) == 0:\n",
            "                tt = 3 # Control\n",
            "            elif tokid >= 3 and tokid <= 258 and len(vbytes) == 1:\n",
            "                vbytes = bytes(f'<0x{vbytes[0]:02X}>', encoding = 'UTF-8')\n",
            "                tt = 6 # Byte\n",
            "            else:\n",
            "                vbytes = vbytes.replace(b' ', b'\\xe2\\x96\\x81')\n",
            "            toktypes.append(tt)\n",
            "            tokens.append(vbytes)\n",
            "            scores.append(vscore)\n",
            "        gguf_writer.add_token_list(tokens)\n",
            "        gguf_writer.add_token_scores(scores)\n",
            "        gguf_writer.add_token_types(toktypes)\n",
            "        gguf_writer.add_unk_token_id(0)\n",
            "        gguf_writer.add_bos_token_id(1)\n",
            "        gguf_writer.add_eos_token_id(2)\n",
            "\n",
            "    def add_tensors(self, gguf_writer):\n",
            "        tensor_map = self.name_map\n",
            "        data = self.data\n",
            "        logger.info(f'* Adding {len(self.model.tensors)} tensor(s)')\n",
            "        for tensor in self.model.tensors:\n",
            "            name = str(tensor.name, 'UTF-8')\n",
            "            mapped_name = tensor_map.get_name(name, try_suffixes = (\".weight\", \".bias\"))\n",
            "            assert mapped_name is not None, f'Bad name {name}'\n",
            "            tempdims = list(tensor.dims[:])\n",
            "            if len(tempdims) > 1:\n",
            "                temp = tempdims[1]\n",
            "                tempdims[1] = tempdims[0]\n",
            "                tempdims[0] = temp\n",
            "            gguf_writer.add_tensor(\n",
            "                mapped_name,\n",
            "                data[tensor.start_offset:tensor.start_offset + tensor.len_bytes],\n",
            "                raw_shape = tempdims,\n",
            "                raw_dtype = tensor.dtype)\n",
            "\n",
            "\n",
            "def handle_metadata(cfg, hp):\n",
            "    import examples.convert_legacy_llama as convert\n",
            "\n",
            "    assert cfg.model_metadata_dir.is_dir(), 'Metadata dir is not a directory'\n",
            "    hf_config_path   = cfg.model_metadata_dir / \"config.json\"\n",
            "    orig_config_\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Chunk 18:\n",
            "\n",
            "                mapped_name,\n",
            "                data[tensor.start_offset:tensor.start_offset + tensor.len_bytes],\n",
            "                raw_shape = tempdims,\n",
            "                raw_dtype = tensor.dtype)\n",
            "\n",
            "\n",
            "def handle_metadata(cfg, hp):\n",
            "    import examples.convert_legacy_llama as convert\n",
            "\n",
            "    assert cfg.model_metadata_dir.is_dir(), 'Metadata dir is not a directory'\n",
            "    hf_config_path   = cfg.model_metadata_dir / \"config.json\"\n",
            "    orig_config_path = cfg.model_metadata_dir / \"params.json\"\n",
            "    # We pass a fake model here. \"original\" mode will check the shapes of some\n",
            "    # tensors if information is missing in the .json file: other than that, the\n",
            "    # model data isn't used so this should be safe (at least for now).\n",
            "    fakemodel = {\n",
            "        'tok_embeddings.weight': convert.LazyTensor.__new__(convert.LazyTensor),\n",
            "        'layers.0.feed_forward.w1.weight': convert.LazyTensor.__new__(convert.LazyTensor),\n",
            "    }\n",
            "    fakemodel['tok_embeddings.weight'].shape = [hp.n_vocab]\n",
            "    fakemodel['layers.0.feed_forward.w1.weight'].shape = [hp.n_ff]\n",
            "    if hf_config_path.exists():\n",
            "        params = convert.Params.loadHFTransformerJson(fakemodel, hf_config_path)\n",
            "    elif orig_config_path.exists():\n",
            "        params = convert.Params.loadOriginalParamsJson(fakemodel, orig_config_path)\n",
            "    else:\n",
            "        raise ValueError('Unable to load metadata')\n",
            "    vocab_path = Path(cfg.vocab_dir if cfg.vocab_dir is not None else cfg.model_metadata_dir)\n",
            "    vocab_factory = convert.VocabFactory(vocab_path)\n",
            "    vocab, special_vocab = vocab_factory.load_vocab(cfg.vocabtype.split(\",\"), cfg.model_metadata_dir)\n",
            "    convert.check_vocab_size(params, vocab)\n",
            "    return params, vocab, special_vocab\n",
            "\n",
            "\n",
            "def handle_args():\n",
            "    parser = argparse.ArgumentParser(description = 'Convert GGML models to GGUF')\n",
            "    parser.add_argument('--input', '-i', type = Path, required = True,\n",
            "                        help = 'Input GGMLv3 filename')\n",
            "    parser.add_argument('--output', '-o', type = Path, required = True,\n",
            "                        help ='Output GGUF filename')\n",
            "    parser.add_argument('--name',\n",
            "                        help = 'Set model name')\n",
            "    parser.add_argument('--desc',\n",
            "                        help = 'Set model description')\n",
            "    parser.add_argument('--gqa', type = int, default = 1,\n",
            "                        help = 'grouped-query attention factor (use 8 for LLaMA2 70B)')\n",
            "    parser.add_argument('--eps', default =\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Chunk 19:\n",
            "                       help ='Output GGUF filename')\n",
            "    parser.add_argument('--name',\n",
            "                        help = 'Set model name')\n",
            "    parser.add_argument('--desc',\n",
            "                        help = 'Set model description')\n",
            "    parser.add_argument('--gqa', type = int, default = 1,\n",
            "                        help = 'grouped-query attention factor (use 8 for LLaMA2 70B)')\n",
            "    parser.add_argument('--eps', default = '5.0e-06',\n",
            "                        help = 'RMS norm eps: Use 1e-6 for LLaMA1 and OpenLLaMA, use 1e-5 for LLaMA2')\n",
            "    parser.add_argument('--context-length', '-c', type=int, default = 2048,\n",
            "                        help = 'Default max context length: LLaMA1 is typically 2048, LLaMA2 is typically 4096')\n",
            "    parser.add_argument('--model-metadata-dir', '-m', type = Path,\n",
            "                        help ='Load HuggingFace/.pth vocab and metadata from the specified directory')\n",
            "    parser.add_argument(\"--vocab-dir\", type=Path,\n",
            "                        help=\"directory containing tokenizer.model, if separate from model file - only meaningful with --model-metadata-dir\")\n",
            "    parser.add_argument(\"--vocabtype\", default=\"spm,hfft\",\n",
            "                        help=\"vocab format - only meaningful with --model-metadata-dir and/or --vocab-dir (default: spm,hfft)\")\n",
            "    parser.add_argument(\"--verbose\", action=\"store_true\", help=\"increase output verbosity\")\n",
            "    return parser.parse_args()\n",
            "\n",
            "\n",
            "def main():\n",
            "    cfg = handle_args()\n",
            "    logging.basicConfig(level=logging.DEBUG if cfg.verbose else logging.INFO)\n",
            "    logger.info(f'* Using config: {cfg}')\n",
            "    logger.warning('=== WARNING === Be aware that this conversion script is best-effort. Use a native GGUF model if possible. === WARNING ===')\n",
            "    if cfg.model_metadata_dir is None and (cfg.gqa == 1 or cfg.eps == '5.0e-06'):\n",
            "        logger.info('- Note: If converting LLaMA2, specifying \"--eps 1e-5\" is required. 70B models also need \"--gqa 8\".')\n",
            "    data = np.memmap(cfg.input, mode = 'r')\n",
            "    model = GGMLModel()\n",
            "    logger.info('* Scanning GGML input file')\n",
            "    offset = model.load(data, 0)  # noqa\n",
            "    logger.info(f'* GGML model hyperparameters: {model.hyperparameters}')\n",
            "    vocab_override = None\n",
            "    params_override = None\n",
            "    special_vocab = None\n",
            "    if cfg.model_metadata_dir is not None:\n",
            "        (params_override, vocab_override, special_vocab) = handle_metadata(cfg, model.hyperparameters)\n",
            "        logger.info('!! Note: When overriding params the --gqa, --eps and --context-length options are ignored.')\n",
            "       \n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Chunk 20:\n",
            "')\n",
            "    model = GGMLModel()\n",
            "    logger.info('* Scanning GGML input file')\n",
            "    offset = model.load(data, 0)  # noqa\n",
            "    logger.info(f'* GGML model hyperparameters: {model.hyperparameters}')\n",
            "    vocab_override = None\n",
            "    params_override = None\n",
            "    special_vocab = None\n",
            "    if cfg.model_metadata_dir is not None:\n",
            "        (params_override, vocab_override, special_vocab) = handle_metadata(cfg, model.hyperparameters)\n",
            "        logger.info('!! Note: When overriding params the --gqa, --eps and --context-length options are ignored.')\n",
            "        logger.info(f'* Overriding params: {params_override}')\n",
            "        logger.info(f'* Overriding vocab: {vocab_override}')\n",
            "        logger.info(f'* Special vocab: {special_vocab}')\n",
            "    else:\n",
            "        logger.warning('\\n=== WARNING === Special tokens may not be converted correctly. Use --model-metadata-dir if possible === WARNING ===\\n')\n",
            "        if model.file_format == GGMLFormat.GGML:\n",
            "            logger.info('! This is a very old GGML file that does not contain vocab scores. Strongly recommend using model metadata!')\n",
            "    converter = GGMLToGGUF(\n",
            "        model, data, cfg,\n",
            "        params_override = params_override,\n",
            "        vocab_override = vocab_override,\n",
            "        special_vocab = special_vocab\n",
            "    )\n",
            "    converter.save()\n",
            "    logger.info(f'* Successful completion. Output saved to: {cfg.output}')\n",
            "\n",
            "\n",
            "if __name__ == '__main__':\n",
            "    main()\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Chunk 21:\n",
            "#!/usr/bin/env python3\n",
            "# -*- coding: utf-8 -*-\n",
            "\n",
            "from __future__ import annotations\n",
            "\n",
            "from dataclasses import dataclass\n",
            "import logging\n",
            "import argparse\n",
            "import os\n",
            "import sys\n",
            "import json\n",
            "from math import prod\n",
            "from pathlib import Path\n",
            "from typing import TYPE_CHECKING, Any, Callable, Iterable, Iterator, Sequence, SupportsIndex, cast\n",
            "\n",
            "import torch\n",
            "\n",
            "if TYPE_CHECKING:\n",
            "    from torch import Tensor\n",
            "\n",
            "if 'NO_LOCAL_GGUF' not in os.environ:\n",
            "    sys.path.insert(1, str(Path(__file__).parent / 'gguf-py'))\n",
            "import gguf\n",
            "\n",
            "# reuse model definitions from convert_hf_to_gguf.py\n",
            "from convert_hf_to_gguf import LazyTorchTensor, Model\n",
            "\n",
            "logger = logging.getLogger(\"lora-to-gguf\")\n",
            "\n",
            "\n",
            "@dataclass\n",
            "class PartialLoraTensor:\n",
            "    A: Tensor | None = None\n",
            "    B: Tensor | None = None\n",
            "\n",
            "\n",
            "# magic to support tensor shape modifications and splitting\n",
            "class LoraTorchTensor:\n",
            "    _lora_A: Tensor  # (n_rank, row_size)\n",
            "    _lora_B: Tensor  # (col_size, n_rank)\n",
            "    _rank: int\n",
            "\n",
            "    def __init__(self, A: Tensor, B: Tensor):\n",
            "        assert len(A.shape) == len(B.shape)\n",
            "        assert A.shape[-2] == B.shape[-1]\n",
            "        if A.dtype != B.dtype:\n",
            "            A = A.to(torch.float32)\n",
            "            B = B.to(torch.float32)\n",
            "        self._lora_A = A\n",
            "        self._lora_B = B\n",
            "        self._rank = B.shape[-1]\n",
            "\n",
            "    def get_lora_A_B(self) -> tuple[Tensor, Tensor]:\n",
            "        return (self._lora_A, self._lora_B)\n",
            "\n",
            "    def __getitem__(\n",
            "        self,\n",
            "        indices: (\n",
            "            SupportsIndex\n",
            "            | slice\n",
            "            | tuple[SupportsIndex | slice | Tensor, ...]  # TODO: add ellipsis in the type signature\n",
            "        ),\n",
            "    ) -> LoraTorchTensor:\n",
            "        shape = self.shape\n",
            "        if isinstance(indices, SupportsIndex):\n",
            "            if len(shape) > 2:\n",
            "                return LoraTorchTensor(self._lora_A[indices], self._lora_B[indices])\n",
            "            else:\n",
            "                raise NotImplementedError  # can't return a vector\n",
            "        elif isinstance(indices, slice):\n",
            "            if len(shape) > 2:\n",
            "                return LoraTorchTensor(self._lora_A[indices], self._lora_B[indices])\n",
            "            else:\n",
            "                return LoraTorchTensor(self._lora_A, self._lora_B[indices])\n",
            "        elif isinstance(indices, tuple):\n",
            "            assert len(\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Chunk 22:\n",
            ":\n",
            "                raise NotImplementedError  # can't return a vector\n",
            "        elif isinstance(indices, slice):\n",
            "            if len(shape) > 2:\n",
            "                return LoraTorchTensor(self._lora_A[indices], self._lora_B[indices])\n",
            "            else:\n",
            "                return LoraTorchTensor(self._lora_A, self._lora_B[indices])\n",
            "        elif isinstance(indices, tuple):\n",
            "            assert len(indices) > 0\n",
            "            if indices[-1] is Ellipsis:\n",
            "                return self[indices[:-1]]\n",
            "            # expand ellipsis\n",
            "            indices = tuple(\n",
            "                u\n",
            "                for v in (\n",
            "                    (\n",
            "                        (slice(None, None) for _ in range(len(indices) - 1))\n",
            "                        if i is Ellipsis\n",
            "                        else (i,)\n",
            "                    )\n",
            "                    for i in indices\n",
            "                )\n",
            "                for u in v\n",
            "            )\n",
            "\n",
            "            if len(indices) < len(shape):\n",
            "                indices = (*indices, *(slice(None, None) for _ in range(len(indices), len(shape))))\n",
            "\n",
            "            # TODO: make sure this is correct\n",
            "            indices_A = (\n",
            "                *(\n",
            "                    (\n",
            "                        j.__index__() % self._lora_A.shape[i]\n",
            "                        if isinstance(j, SupportsIndex)\n",
            "                        else slice(None, None)\n",
            "                    )\n",
            "                    for i, j in enumerate(indices[:-2])\n",
            "                ),\n",
            "                slice(None, None),\n",
            "                indices[-1],\n",
            "            )\n",
            "            indices_B = indices[:-1]\n",
            "            return LoraTorchTensor(self._lora_A[indices_A], self._lora_B[indices_B])\n",
            "        else:\n",
            "     \n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Chunk 23:\n",
            "            )\n",
            "                    for i, j in enumerate(indices[:-2])\n",
            "                ),\n",
            "                slice(None, None),\n",
            "                indices[-1],\n",
            "            )\n",
            "            indices_B = indices[:-1]\n",
            "            return LoraTorchTensor(self._lora_A[indices_A], self._lora_B[indices_B])\n",
            "        else:\n",
            "            raise NotImplementedError  # unknown indice type\n",
            "\n",
            "    @property\n",
            "    def dtype(self) -> torch.dtype:\n",
            "        assert self._lora_A.dtype == self._lora_B.dtype\n",
            "        return self._lora_A.dtype\n",
            "\n",
            "    @property\n",
            "    def shape(self) -> tuple[int, ...]:\n",
            "        assert len(self._lora_A.shape) == len(self._lora_B.shape)\n",
            "        return (*self._lora_B.shape[:-1], self._lora_A.shape[-1])\n",
            "\n",
            "    def size(self, dim=None):\n",
            "        assert dim is None\n",
            "        return self.shape\n",
            "\n",
            "    def reshape(self, *shape: int | tuple[int, ...]) -> LoraTorchTensor:\n",
            "        if isinstance(shape[0], tuple):\n",
            "            new_shape: tuple[int, ...] = shape[0]\n",
            "        else:\n",
            "            new_shape = cast(tuple[int, ...], shape)\n",
            "        orig_shape = self.shape\n",
            "        if len(new_shape) < 2:\n",
            "            raise NotImplementedError  # can't become a vector\n",
            "\n",
            "        # expand -1 in the shape\n",
            "        if any(dim == -1 for dim in new_shape):\n",
            "            n_elems = prod(orig_shape)\n",
            "            n_new_elems = prod(dim if dim != -1 else 1 for dim in new_shape)\n",
            "            assert n_elems % n_new_elems == 0\n",
            "            new_shape = (*(dim if dim != -1 else n_elems // n_new_elems for dim in new_shape),)\n",
            "\n",
            "        if new_shape[-1] != orig_shape[-1]:\n",
            "            raise NotImplementedError  # can't reshape the row size trivially\n",
            "\n",
            "        shape_A = (*(1 for _ in new_shape[:-2]), self._rank, orig_shape[-1])\n",
            "        shape_B = (*new_shape[:-1], self._rank)\n",
            "        return LoraTorchTensor(\n",
            "            self._lora_A.reshape(shape_A),\n",
            "            self._lora_B.reshape(shape_B),\n",
            "        )\n",
            "\n",
            "    def reshape_as(self, other: Tensor) -> LoraTorchTensor:\n",
            "        return self.reshape(*other.shape)\n",
            "\n",
            "    def view(self, *size: int) -> LoraTorchTensor\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Chunk 24:\n",
            "        shape_A = (*(1 for _ in new_shape[:-2]), self._rank, orig_shape[-1])\n",
            "        shape_B = (*new_shape[:-1], self._rank)\n",
            "        return LoraTorchTensor(\n",
            "            self._lora_A.reshape(shape_A),\n",
            "            self._lora_B.reshape(shape_B),\n",
            "        )\n",
            "\n",
            "    def reshape_as(self, other: Tensor) -> LoraTorchTensor:\n",
            "        return self.reshape(*other.shape)\n",
            "\n",
            "    def view(self, *size: int) -> LoraTorchTensor:\n",
            "        return self.reshape(*size)\n",
            "\n",
            "    def permute(self, *dims: int) -> LoraTorchTensor:\n",
            "        shape = self.shape\n",
            "        dims = tuple(dim - len(shape) if dim >= 0 else dim for dim in dims)\n",
            "        if dims[-1] == -1:\n",
            "            # TODO: support higher dimensional A shapes bigger than 1\n",
            "            assert all(dim == 1 for dim in self._lora_A.shape[:-2])\n",
            "            return LoraTorchTensor(self._lora_A, self._lora_B.permute(*dims))\n",
            "        if len(shape) == 2 and dims[-1] == -2 and dims[-2] == -1:\n",
            "            return LoraTorchTensor(self._lora_B.permute(*dims), self._lora_A.permute(*dims))\n",
            "        else:\n",
            "            # TODO: compose the above two\n",
            "            raise NotImplementedError\n",
            "\n",
            "    def transpose(self, dim0: int, dim1: int) -> LoraTorchTensor:\n",
            "        shape = self.shape\n",
            "        dims = [i for i in range(len(shape))]\n",
            "        dims[dim0], dims[dim1] = dims[dim1], dims[dim0]\n",
            "        return self.permute(*dims)\n",
            "\n",
            "    def swapaxes(self, axis0: int, axis1: int) -> LoraTorchTensor:\n",
            "        return self.transpose(axis0, axis1)\n",
            "\n",
            "    def to(self, *args, **kwargs):\n",
            "        return LoraTorchTensor(self._lora_A.to(*args, **kwargs), self._lora_B.to(*args, **kwargs))\n",
            "\n",
            "    @classmethod\n",
            "    def __torch_function__(cls, func: Callable, types, args=(), kwargs=None):\n",
            "        del types  # unused\n",
            "\n",
            "        if kwargs is None:\n",
            "            kwargs = {}\n",
            "\n",
            "        if func is torch.permute:\n",
            "            return type(args[0]).permute(*args, **kwargs)\n",
            "        elif func is torch.reshape:\n",
            "            return type(args[0]).reshape(*args, **kwargs)\n",
            "        elif func is torch.stack:\n",
            "            assert isinstance(args[0], Sequence)\n",
            "            dim = kwargs.get(\"dim\", 0)\n",
            "         \n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Chunk 25:\n",
            "   del types  # unused\n",
            "\n",
            "        if kwargs is None:\n",
            "            kwargs = {}\n",
            "\n",
            "        if func is torch.permute:\n",
            "            return type(args[0]).permute(*args, **kwargs)\n",
            "        elif func is torch.reshape:\n",
            "            return type(args[0]).reshape(*args, **kwargs)\n",
            "        elif func is torch.stack:\n",
            "            assert isinstance(args[0], Sequence)\n",
            "            dim = kwargs.get(\"dim\", 0)\n",
            "            assert dim == 0\n",
            "            return LoraTorchTensor(\n",
            "                torch.stack([a._lora_A for a in args[0]], dim),\n",
            "                torch.stack([b._lora_B for b in args[0]], dim),\n",
            "            )\n",
            "        elif func is torch.cat:\n",
            "            assert isinstance(args[0], Sequence)\n",
            "            dim = kwargs.get(\"dim\", 0)\n",
            "            assert dim == 0\n",
            "            if len(args[0][0].shape) > 2:\n",
            "                return LoraTorchTensor(\n",
            "                    torch.cat([a._lora_A for a in args[0]], dim),\n",
            "                    torch.cat([b._lora_B for b in args[0]], dim),\n",
            "                )\n",
            "            elif all(torch.equal(args[0][0]._lora_A, t._lora_A) for t in args[0][1:]):\n",
            "                return LoraTorchTensor(\n",
            "                    args[0][0]._lora_A,\n",
            "                    torch.cat([b._lora_B for b in args[0]], dim),\n",
            "                )\n",
            "            else:\n",
            "                raise NotImplementedError\n",
            "        else:\n",
            "            raise NotImplementedError\n",
            "\n",
            "\n",
            "def get_base_tensor_name(lora_tensor_name: str) -> str:\n",
            "    base_name = lora_tensor_name.replace(\"base_model.model.\", \"\")\n",
            "    base_name = base_name.replace(\".lora_A.weight\", \".weight\")\n",
            "    base_name = base_name.replace(\".lora_B.weight\", \".weight\")\n",
            "    return base_name\n",
            "\n",
            "\n",
            "def parse_args() -> argparse.Namespace:\n",
            "    parser = argparse.ArgumentParser(\n",
            "        description=\"Convert a huggingface PEFT LoRA adapter to a GGML compatible file\")\n",
            "    parser.add_argument(\n",
            "        \"--outfile\", type=Path,\n",
            "        help=\"path to write to; default: based on input. {ftype} will be replaced by the outtype.\",\n",
            "    )\n",
            "    parser.add_argument\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Chunk 26:\n",
            "ensor_name.replace(\"base_model.model.\", \"\")\n",
            "    base_name = base_name.replace(\".lora_A.weight\", \".weight\")\n",
            "    base_name = base_name.replace(\".lora_B.weight\", \".weight\")\n",
            "    return base_name\n",
            "\n",
            "\n",
            "def parse_args() -> argparse.Namespace:\n",
            "    parser = argparse.ArgumentParser(\n",
            "        description=\"Convert a huggingface PEFT LoRA adapter to a GGML compatible file\")\n",
            "    parser.add_argument(\n",
            "        \"--outfile\", type=Path,\n",
            "        help=\"path to write to; default: based on input. {ftype} will be replaced by the outtype.\",\n",
            "    )\n",
            "    parser.add_argument(\n",
            "        \"--outtype\", type=str, choices=[\"f32\", \"f16\", \"bf16\", \"q8_0\", \"auto\"], default=\"f16\",\n",
            "        help=\"output format - use f32 for float32, f16 for float16, bf16 for bfloat16, q8_0 for Q8_0, auto for the highest-fidelity 16-bit float type depending on the first loaded tensor type\",\n",
            "    )\n",
            "    parser.add_argument(\n",
            "        \"--bigendian\", action=\"store_true\",\n",
            "        help=\"model is executed on big endian machine\",\n",
            "    )\n",
            "    parser.add_argument(\n",
            "        \"--no-lazy\", action=\"store_true\",\n",
            "        help=\"use more RAM by computing all outputs before writing (use in case lazy evaluation is broken)\",\n",
            "    )\n",
            "    parser.add_argument(\n",
            "        \"--verbose\", action=\"store_true\",\n",
            "        help=\"increase output verbosity\",\n",
            "    )\n",
            "    parser.add_argument(\n",
            "        \"--dry-run\", action=\"store_true\",\n",
            "        help=\"only print out what will be done, without writing any new files\",\n",
            "    )\n",
            "    parser.add_argument(\n",
            "        \"--base\", type=Path, required=True,\n",
            "        help=\"directory containing base model file\",\n",
            "    )\n",
            "    parser.add_argument(\n",
            "        \"lora_path\", type=Path,\n",
            "        help=\"directory containing LoRA adapter file\",\n",
            "    )\n",
            "\n",
            "    return parser.parse_args()\n",
            "\n",
            "\n",
            "if __name__ == '__main__':\n",
            "    args = parse_args()\n",
            "    logging.basicConfig(level=logging.DEBUG if args.verbose else logging.INFO)\n",
            "\n",
            "    ftype_map: dict[str, gguf.LlamaFileType] = {\n",
            "        \"f32\": gguf.LlamaFileType.ALL_F32,\n",
            "        \"f16\": gguf.LlamaFileType.MOSTLY_F16,\n",
            "        \"bf16\": gguf.LlamaFileType.MOSTLY_BF16,\n",
            "        \"q8_0\": gguf.LlamaFileType.MOSTLY_Q8_0,\n",
            "        \"auto\": gguf.LlamaFileType.GUESSED,\n",
            "    }\n",
            "\n",
            "    ftype = ftype_map[args.outtype]\n",
            "\n",
            "    dir_base_model: Path = args.base\n",
            "    dir_lora: Path = args.lora_path\n",
            "    lora_config = dir_lora / \"adapter_config.json\"\n",
            "    input_model = dir_lora / \"adapter_model.safetensors\"\n",
            "\n",
            "    if args.outfile is not None:\n",
            "        fname_out = args.outfile\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Chunk 27:\n",
            "lamaFileType.MOSTLY_BF16,\n",
            "        \"q8_0\": gguf.LlamaFileType.MOSTLY_Q8_0,\n",
            "        \"auto\": gguf.LlamaFileType.GUESSED,\n",
            "    }\n",
            "\n",
            "    ftype = ftype_map[args.outtype]\n",
            "\n",
            "    dir_base_model: Path = args.base\n",
            "    dir_lora: Path = args.lora_path\n",
            "    lora_config = dir_lora / \"adapter_config.json\"\n",
            "    input_model = dir_lora / \"adapter_model.safetensors\"\n",
            "\n",
            "    if args.outfile is not None:\n",
            "        fname_out = args.outfile\n",
            "    else:\n",
            "        # output in the same directory as the model by default\n",
            "        fname_out = dir_lora\n",
            "\n",
            "    if os.path.exists(input_model):\n",
            "        # lazy import load_file only if lora is in safetensors format.\n",
            "        from safetensors.torch import load_file\n",
            "\n",
            "        lora_model = load_file(input_model, device=\"cpu\")\n",
            "    else:\n",
            "        input_model = os.path.join(dir_lora, \"adapter_model.bin\")\n",
            "        lora_model = torch.load(input_model, map_location=\"cpu\", weights_only=True)\n",
            "\n",
            "    # load base model\n",
            "    logger.info(f\"Loading base model: {dir_base_model.name}\")\n",
            "    hparams = Model.load_hparams(dir_base_model)\n",
            "    with torch.inference_mode():\n",
            "        try:\n",
            "            model_class = Model.from_model_architecture(hparams[\"architectures\"][0])\n",
            "        except NotImplementedError:\n",
            "            logger.error(f\"Model {hparams['architectures'][0]} is not supported\")\n",
            "            sys.exit(1)\n",
            "\n",
            "        class LoraModel(model_class):\n",
            "            model_arch = model_class.model_arch\n",
            "\n",
            "            lora_alpha: float\n",
            "\n",
            "            def __init__(self, *args, dir_lora_model: Path, lora_alpha: float, **kwargs):\n",
            "\n",
            "                super().__init__(*args, **kwargs)\n",
            "\n",
            "                self.dir_model_card = dir_lora_model\n",
            "                self.lora_alpha = float(lora_alpha)\n",
            "\n",
            "            def set_type(self):\n",
            "                self.gguf_writer.add_type(gguf.GGUFType.ADAPTER)\n",
            "                self.gguf_writer.add_string(gguf.Keys.Adapter.TYPE, \"lora\")\n",
            "\n",
            "            def set_gguf_parameters(self):\n",
            "                self.gguf_writer.add_float32(gguf.Keys.Adapter.LORA_ALPHA, self.lora_alpha)\n",
            "                super().set_gguf_parameters()\n",
            "\n",
            "            def get_tensors(self) -> Iterator[tuple[str, Tensor]]:\n",
            "      \n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Chunk 28:\n",
            ".gguf_writer.add_type(gguf.GGUFType.ADAPTER)\n",
            "                self.gguf_writer.add_string(gguf.Keys.Adapter.TYPE, \"lora\")\n",
            "\n",
            "            def set_gguf_parameters(self):\n",
            "                self.gguf_writer.add_float32(gguf.Keys.Adapter.LORA_ALPHA, self.lora_alpha)\n",
            "                super().set_gguf_parameters()\n",
            "\n",
            "            def get_tensors(self) -> Iterator[tuple[str, Tensor]]:\n",
            "                tensor_map: dict[str, PartialLoraTensor] = {}\n",
            "\n",
            "                for name, tensor in lora_model.items():\n",
            "                    if self.lazy:\n",
            "                        tensor = LazyTorchTensor.from_eager(tensor)\n",
            "                    base_name = get_base_tensor_name(name)\n",
            "                    is_lora_a = \".lora_A.weight\" in name\n",
            "                    is_lora_b = \".lora_B.weight\" in name\n",
            "                    if not is_lora_a and not is_lora_b:\n",
            "                        if \".base_layer.weight\" in name:\n",
            "                            continue\n",
            "                        logger.error(f\"Unexpected name '{name}': Not a lora_A or lora_B tensor\")\n",
            "                        sys.exit(1)\n",
            "\n",
            "                    if base_name in tensor_map:\n",
            "                        if is_lora_a:\n",
            "                            tensor_map[base_name].A = tensor\n",
            "                        else:\n",
            "                            tensor_map[base_name].B = tensor\n",
            "                    else:\n",
            "                        if is_lora_a:\n",
            "                            tensor_map[base_name] = PartialLoraTensor(A=tensor)\n",
            "                        else:\n",
            "                            tensor_map[base_name] = PartialLoraTensor(B=tensor)\n",
            "\n",
            "                for name, tensor in tensor_map.items():\n",
            "       \n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Chunk 29:\n",
            "      else:\n",
            "                        if is_lora_a:\n",
            "                            tensor_map[base_name] = PartialLoraTensor(A=tensor)\n",
            "                        else:\n",
            "                            tensor_map[base_name] = PartialLoraTensor(B=tensor)\n",
            "\n",
            "                for name, tensor in tensor_map.items():\n",
            "                    assert tensor.A is not None\n",
            "                    assert tensor.B is not None\n",
            "                    yield (name, cast(torch.Tensor, LoraTorchTensor(tensor.A, tensor.B)))\n",
            "\n",
            "            def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -> Iterable[tuple[str, Tensor]]:\n",
            "                dest = super().modify_tensors(data_torch, name, bid)\n",
            "                for dest_name, dest_data in dest:\n",
            "                    assert isinstance(dest_data, LoraTorchTensor)\n",
            "                    lora_a, lora_b = dest_data.get_lora_A_B()\n",
            "\n",
            "                    yield (dest_name + \".lora_a\", lora_a)\n",
            "                    yield (dest_name + \".lora_b\", lora_b)\n",
            "\n",
            "        with open(lora_config, \"r\") as f:\n",
            "            lparams: dict[str, Any] = json.load(f)\n",
            "\n",
            "        alpha: float = lparams[\"lora_alpha\"]\n",
            "\n",
            "        model_instance = LoraModel(\n",
            "            dir_base_model,\n",
            "            ftype,\n",
            "            fname_out,\n",
            "            is_big_endian=args.bigendian,\n",
            "            use_temp_file=False,\n",
            "            eager=args.no_lazy,\n",
            "            dry_run=args.dry_run,\n",
            "            dir_lora_model=dir_lora,\n",
            "            lora_alpha=alpha,\n",
            "        )\n",
            "\n",
            "        logger.info(\"Exporting model...\")\n",
            "        model_instance.write()\n",
            "        logger.info(f\"Model successfully exported to {model_instance.fname_out}\")\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Chunk 30:\n",
            "#!/usr/bin/env python3\n",
            "# -*- coding: utf-8 -*-\n",
            "\n",
            "from __future__ import annotations\n",
            "\n",
            "import logging\n",
            "import argparse\n",
            "import contextlib\n",
            "import json\n",
            "import os\n",
            "import re\n",
            "import sys\n",
            "from enum import IntEnum\n",
            "from pathlib import Path\n",
            "from hashlib import sha256\n",
            "from typing import TYPE_CHECKING, Any, Callable, ContextManager, Iterable, Iterator, Literal, Sequence, TypeVar, cast\n",
            "\n",
            "import math\n",
            "import numpy as np\n",
            "import torch\n",
            "\n",
            "if TYPE_CHECKING:\n",
            "    from torch import Tensor\n",
            "\n",
            "if 'NO_LOCAL_GGUF' not in os.environ:\n",
            "    sys.path.insert(1, str(Path(__file__).parent / 'gguf-py'))\n",
            "import gguf\n",
            "\n",
            "logger = logging.getLogger(\"hf-to-gguf\")\n",
            "\n",
            "\n",
            "###### MODEL DEFINITIONS ######\n",
            "\n",
            "class SentencePieceTokenTypes(IntEnum):\n",
            "    NORMAL = 1\n",
            "    UNKNOWN = 2\n",
            "    CONTROL = 3\n",
            "    USER_DEFINED = 4\n",
            "    UNUSED = 5\n",
            "    BYTE = 6\n",
            "\n",
            "\n",
            "AnyModel = TypeVar(\"AnyModel\", bound=\"type[Model]\")\n",
            "\n",
            "\n",
            "class Model:\n",
            "    _model_classes: dict[str, type[Model]] = {}\n",
            "\n",
            "    dir_model: Path\n",
            "    ftype: gguf.LlamaFileType\n",
            "    fname_out: Path\n",
            "    is_big_endian: bool\n",
            "    endianess: gguf.GGUFEndian\n",
            "    use_temp_file: bool\n",
            "    lazy: bool\n",
            "    part_names: list[str]\n",
            "    is_safetensors: bool\n",
            "    hparams: dict[str, Any]\n",
            "    block_count: int\n",
            "    tensor_map: gguf.TensorNameMap\n",
            "    tensor_names: set[str] | None\n",
            "    gguf_writer: gguf.GGUFWriter\n",
            "    model_name: str | None\n",
            "    metadata_override: Path | None\n",
            "    dir_model_card: Path\n",
            "\n",
            "    # subclasses should define this!\n",
            "    model_arch: gguf.MODEL_ARCH\n",
            "\n",
            "    def __init__(self, dir_model: Path, ftype: gguf.LlamaFileType, fname_out: Path, is_big_endian: bool = False,\n",
            "                 use_temp_file: bool = False, eager: bool = False,\n",
            "                 metadata_override: Path | None = None, model_name: str | None = None,\n",
            "                 split_max_tensors: int = 0, split_max_size: int = 0, dry_run: bool = False, small_first_shard: bool = False):\n",
            "        if type(self) is Model:\n",
            "            raise TypeError(f\"{type(self).__name__!r} should not be directly instantiated\")\n",
            "\n",
            "        self.dir_model = dir_model\n",
            "        self.ftype = ftype\n",
            "        self.fname_out = fname_out\n",
            "        self.is_big_endian = is_big_endian\n",
            "        self.endianess = gguf.GGUFEndian.BIG if is_big_endian else gguf.GGUFEndian.LITTLE\n",
            "        self.use_temp_file = use_temp_file\n",
            "        self.lazy = not eager\n",
            "        self.part_names = Model.get_model_part_names(self.dir_model, \"model\", \".safetensors\")\n",
            "        self.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# def main():\n",
        "repo_url = 'https://github.com/ggerganov/llama.cpp.git'\n",
        "clone_dir = './llama_cpp'\n",
        "openai_organization_id = userdata.get('MY_OPENAI_ORG_ID')\n",
        "openai_key = userdata.get('OPEN-AI_KEY')\n",
        "\n",
        "# GitHub 저장소 클론\n",
        "# clone_git_repository(repo_url, clone_dir)\n",
        "\n",
        "# 클론한 디렉토리 내의 코드 파일 처리\n",
        "chunks = process_code_directory(clone_dir, extensions=['.py', '.cpp', '.h'], chunk_size=1000, overlap=200)\n",
        "\n",
        "# 임시코드\n",
        "chunks = chunks[:30]\n",
        "\n",
        "# 청크 출력\n",
        "# for i, chunk in enumerate(chunks):\n",
        "#     print(f\"Chunk {i+1}:\")\n",
        "#     print(chunk)\n",
        "#     print(\"\\n\" + \"-\"*80 + \"\\n\")\n",
        "\n",
        "# VectorStore를 생성합니다.\n",
        "embeddings = OpenAIEmbeddings(openai_api_key=openai_key, organization=openai_organization_id)\n",
        "vector = FAISS.from_texts(chunks, embeddings)\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     main()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Retriever를 생성합니다.\n",
        "base_retriever = vector.as_retriever(\n",
        "    search_type='similarity_score_threshold',\n",
        "    search_kwargs={'score_threshold': 0.08}\n",
        ")\n",
        "\n",
        "\n",
        "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "\n",
        "model_name = \"gpt-3.5-turbo\"\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPEN-AI_KEY')\n",
        "\n",
        "llm=ChatOpenAI(model=model_name, temperature=0.4, organization=openai_organization_id, max_tokens=500)\n",
        "\n",
        "multi_query_retriever = MultiQueryRetriever.from_llm(\n",
        "    retriever=base_retriever,\n",
        "    llm=llm\n",
        ")"
      ],
      "metadata": {
        "id": "-xzoXij4kE2M"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
        "from langchain.retrievers import ContextualCompressionRetriever\n",
        "\n",
        "# langchain 패키지의 tools 모듈에서 retriever 도구를 생성\n",
        "compressor = LLMChainExtractor.from_llm(llm)\n",
        "compression_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=compressor, base_retriever=multi_query_retriever\n",
        ")"
      ],
      "metadata": {
        "id": "pdMRQv77LEOu"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.tools.retriever import create_retriever_tool\n",
        "\n",
        "# langchain 패키지의 tools 모듈에서 retriever 도구를 생성\n",
        "compressor = LLMChainExtractor.from_llm(llm)\n",
        "compression_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=compressor, base_retriever=multi_query_retriever\n",
        ")\n",
        "\n",
        "retriever_tool = create_retriever_tool(\n",
        "    compression_retriever,\n",
        "    name=\"pdf_search\",\n",
        "    # 도구에 대한 설명을 자세히 기입해야 합니다!!!\n",
        "    description=\"2023년 12월 AI 관련 정보를 PDF 문서에서 검색합니다. '2023년 12월 AI 산업동향' 과 관련된 질문은 이 도구를 사용해야 합니다!\",\n",
        ")"
      ],
      "metadata": {
        "id": "LzV7m0y8Nfh-"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8fu0i4kPOxo-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ECPBcrx3Num1",
        "outputId": "d81e5d9c-586a-41a0-a4d7-13093bc4615c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='`modify_tensors` 함수는 텐서를 수정하는 데 사용되는 함수입니다. 이 함수는 텐서의 값을 변경하거나 모양을 조정하는 데 사용될 수 있습니다. \\n\\n`modify_tensors` 함수를 사용하는 방법은 다음과 같습니다:\\n\\n1. 필요한 모듈을 import 합니다.\\n```python\\nimport tensorflow as tf\\n```\\n\\n2. 텐서를 생성합니다.\\n```python\\ntensor = tf.constant([[1, 2, 3], [4, 5, 6]])\\n```\\n\\n3. `modify_tensors` 함수를 사용하여 텐서를 수정합니다. 예를 들어, 텐서의 값을 변경하거나 모양을 조정할 수 있습니다.\\n```python\\nmodified_tensor = tf.add(tensor, 10) # 텐서의 모든 요소에 10을 더합니다.\\n```\\n\\n4. 수정된 텐서를 출력하거나 다른 작업에 사용할 수 있습니다.\\n```python\\nprint(modified_tensor)\\n```\\n\\n위의 예시에서는 `tf.add` 함수를 사용하여 텐서의 모든 요소에 10을 더하는 방법을 보여주었습니다. `modify_tensors` 함수를 사용하여 다양한 작업을 수행할 수 있으며, 필요에 따라 다른 함수를 사용하여 텐서를 수정할 수 있습니다.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 333, 'prompt_tokens': 20, 'total_tokens': 353}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-1fdf4a24-e209-44cb-9e9d-e2ac51b3d9a5-0', usage_metadata={'input_tokens': 20, 'output_tokens': 333, 'total_tokens': 353})"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# 프롬프트 템플릿 작성\n",
        "prompt_template = \"\"\"\n",
        "다음 질문에 대한 관련 정보를 검색하고 답변을 제공합니다:\n",
        "\n",
        "질문: {question}\n",
        "\n",
        "관련 정보: {retrieved_content}\n",
        "\n",
        "답변을 간결하고 명확하게 작성하세요.\n",
        "\"\"\"\n",
        "\n",
        "# 프롬프트를 생성합니다.\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"question\", \"retrieved_content\"],\n",
        "    template=prompt_template,\n",
        ")\n",
        "\n",
        "# LLMChain 생성\n",
        "qa_chain = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=prompt,\n",
        ")\n",
        "\n",
        "def get_answer_with_retrieval(question):\n",
        "    # 리트리버를 사용해 관련 문서를 검색합니다.\n",
        "    retrieved_docs = compression_retriever.get_relevant_documents(question)\n",
        "\n",
        "    # 검색된 내용을 텍스트로 결합합니다.\n",
        "    retrieved_content = \"\\n\".join([doc.page_content for doc in retrieved_docs])\n",
        "    return retrieved_content\n",
        "    # 검색된 정보를 포함한 프롬프트로 LLM에 전달하여 답변을 생성합니다.\n",
        "    # answer = qa_chain.run({\n",
        "    #     \"question\": question,\n",
        "    #     \"retrieved_content\": retrieved_content,\n",
        "    # })\n",
        "\n",
        "    # return answer\n",
        "\n",
        "# 예시 질문\n",
        "question = \"SentencePieceTokenTypes class 초기값\"\n",
        "answer = get_answer_with_retrieval(question)\n",
        "\n",
        "print(\"Question:\", question)\n",
        "print(\"Answer:\", answer)"
      ],
      "metadata": {
        "id": "ZtqC7_vUN9Es",
        "outputId": "3929951b-d231-4c28-a29a-1f0b5b822a0c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: SentencePieceTokenTypes class 초기값\n",
            "Answer: SentencePieceTokenTypes class 초기값\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x9WXpsYbNmIE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}