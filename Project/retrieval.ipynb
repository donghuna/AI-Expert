{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN2t73E2Y7rLPLnsWX40RkT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/donghuna/AI-Expert/blob/main/Project/retrieval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain\n",
        "!pip install tiktoken\n",
        "!pip install langchain-openai\n",
        "!pip install faiss-cpu\n",
        "!pip install langchain-community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0zvFBm5Nkfme",
        "outputId": "92188529-b75e-4d48-8bc4-5013cb38214d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.2.13-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.32)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.10.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting langchain-core<0.3.0,>=0.2.30 (from langchain)\n",
            "  Downloading langchain_core-0.2.30-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.2.2-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
            "  Downloading langsmith-0.1.99-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
            "Collecting tenacity!=8.4.0,<9.0.0,>=8.1.0 (from langchain)\n",
            "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.3.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.3.0,>=0.2.30->langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.30->langchain) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.30->langchain) (4.12.2)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.7.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.30->langchain)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Downloading langchain-0.2.13-py3-none-any.whl (997 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m997.8/997.8 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.2.30-py3-none-any.whl (384 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m384.8/384.8 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_text_splitters-0.2.2-py3-none-any.whl (25 kB)\n",
            "Downloading langsmith-0.1.99-py3-none-any.whl (140 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.4/140.4 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
            "Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Installing collected packages: tenacity, orjson, jsonpointer, jsonpatch, langsmith, langchain-core, langchain-text-splitters, langchain\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 9.0.0\n",
            "    Uninstalling tenacity-9.0.0:\n",
            "      Successfully uninstalled tenacity-9.0.0\n",
            "Successfully installed jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.2.13 langchain-core-0.2.30 langchain-text-splitters-0.2.2 langsmith-0.1.99 orjson-3.10.7 tenacity-8.5.0\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.7.4)\n",
            "Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.7.0\n",
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-0.1.21-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.29 in /usr/local/lib/python3.10/dist-packages (from langchain-openai) (0.2.30)\n",
            "Collecting openai<2.0.0,>=1.40.0 (from langchain-openai)\n",
            "  Downloading openai-1.40.6-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.10/dist-packages (from langchain-openai) (0.7.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.29->langchain-openai) (6.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.29->langchain-openai) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.75 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.29->langchain-openai) (0.1.99)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.29->langchain-openai) (24.1)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.29->langchain-openai) (2.8.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.29->langchain-openai) (8.5.0)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.29->langchain-openai) (4.12.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->langchain-openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.40.0->langchain-openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai<2.0.0,>=1.40.0->langchain-openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting jiter<1,>=0.4.0 (from openai<2.0.0,>=1.40.0->langchain-openai)\n",
            "  Downloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->langchain-openai) (4.66.5)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2.32.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.40.0->langchain-openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.40.0->langchain-openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.40.0->langchain-openai) (2024.7.4)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai<2.0.0,>=1.40.0->langchain-openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.40.0->langchain-openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.29->langchain-openai) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3.0,>=0.2.29->langchain-openai) (3.10.7)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.3.0,>=0.2.29->langchain-openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.3.0,>=0.2.29->langchain-openai) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (2.0.7)\n",
            "Downloading langchain_openai-0.1.21-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.8/49.8 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openai-1.40.6-py3-none-any.whl (361 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m361.3/361.3 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (318 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jiter, h11, httpcore, httpx, openai, langchain-openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 jiter-0.5.0 langchain-openai-0.1.21 openai-1.40.6\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.8.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: numpy<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.1)\n",
            "Downloading faiss_cpu-1.8.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m63.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.8.0.post1\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.2.12-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.0.32)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (3.10.1)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: langchain<0.3.0,>=0.2.13 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.2.13)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.30 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.2.30)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.1.99)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (1.26.4)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (8.5.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.3.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.21.3-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.13->langchain-community) (0.2.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.13->langchain-community) (2.8.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.30->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.30->langchain-community) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.30->langchain-community) (4.12.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-community) (3.10.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2024.7.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.0.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.30->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.13->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.13->langchain-community) (2.20.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Downloading langchain_community-0.2.12-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading marshmallow-3.21.3-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, typing-inspect, dataclasses-json, langchain-community\n",
            "Successfully installed dataclasses-json-0.6.7 langchain-community-0.2.12 marshmallow-3.21.3 mypy-extensions-1.0.0 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "import subprocess\n",
        "import shutil\n",
        "from langchain.text_splitter import TokenTextSplitter\n",
        "\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "fJ32Yy6-pC8E"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clone_git_repository(repo_url, clone_dir):\n",
        "    \"\"\"\n",
        "    주어진 GitHub 저장소를 지정된 디렉토리에 클론합니다.\n",
        "    \"\"\"\n",
        "    if os.path.exists(clone_dir):\n",
        "        print(f\"Directory {clone_dir} already exists. Deleting and recloning the repository.\")\n",
        "        shutil.rmtree(clone_dir)\n",
        "    subprocess.run(['git', 'clone', repo_url, clone_dir], check=True)\n",
        "    print(f\"Repository cloned to {clone_dir}\")\n",
        "\n",
        "def read_code_files(directory, extensions=['.py']):\n",
        "    \"\"\"\n",
        "    지정된 디렉토리에서 주어진 확장자를 가진 모든 코드 파일을 읽어서 반환합니다.\n",
        "    \"\"\"\n",
        "    code_texts = []\n",
        "    for root, dirs, files in os.walk(directory):\n",
        "        for file in files:\n",
        "            if any(file.endswith(ext) for ext in extensions):\n",
        "                file_path = Path(root) / file\n",
        "                with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                    code_texts.append(f.read())\n",
        "    return code_texts\n",
        "\n",
        "def split_text_with_overlap(text, chunk_size=1000, overlap=200):\n",
        "    \"\"\"\n",
        "    TokenTextSplitter를 사용하여 텍스트를 오버랩을 가진 청크로 나눕니다.\n",
        "    \"\"\"\n",
        "    text_splitter = TokenTextSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=overlap,\n",
        "        disallowed_special=()  # 모든 특수 토큰을 허용하지 않음\n",
        "    )\n",
        "    return text_splitter.split_text(text)\n",
        "\n",
        "def process_code_directory(directory, extensions=['.py', '.cpp', '.h'], chunk_size=1000, overlap=200):\n",
        "    \"\"\"\n",
        "    디렉토리 내의 모든 코드 파일을 읽고, TokenTextSplitter를 사용하여 오버랩이 있는 청크로 나눠서 반환합니다.\n",
        "    \"\"\"\n",
        "    code_texts = read_code_files(directory, extensions)\n",
        "    chunks = []\n",
        "    for text in code_texts:\n",
        "        chunks.extend(split_text_with_overlap(text, chunk_size, overlap))\n",
        "    return chunks"
      ],
      "metadata": {
        "id": "JH8B2wcMpCKV"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26FVzsaIjn-o",
        "outputId": "25d63ddc-0c0c-4c07-9752-2e14d6c091a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Repository cloned to ./llama_cpp\n"
          ]
        }
      ],
      "source": [
        "# def main():\n",
        "\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     main()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
        "from langchain.retrievers import ContextualCompressionRetriever\n",
        "from langchain.tools.retriever import create_retriever_tool\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "repo_url = 'https://github.com/ggerganov/llama.cpp.git'\n",
        "clone_dir = './llama_cpp'\n",
        "openai_organization_id = userdata.get('MY_OPENAI_ORG_ID')\n",
        "openai_key = userdata.get('OPEN-AI_KEY')\n",
        "\n",
        "# GitHub 저장소 클론\n",
        "# clone_git_repository(repo_url, clone_dir)\n",
        "\n",
        "# 클론한 디렉토리 내의 코드 파일 처리\n",
        "chunks = process_code_directory(clone_dir, extensions=['.py', '.cpp', '.h'], chunk_size=1000, overlap=200)\n",
        "\n",
        "# 임시코드\n",
        "chunks = chunks[:30]\n",
        "\n",
        "# 청크 출력\n",
        "for i, chunk in enumerate(chunks):\n",
        "    print(f\"Chunk {i+1}:\")\n",
        "    print(chunk)\n",
        "    print(\"\\n\" + \"-\"*80 + \"\\n\")\n",
        "\n",
        "# VectorStore를 생성합니다.\n",
        "embeddings = OpenAIEmbeddings(openai_api_key=openai_key, organization=openai_organization_id)\n",
        "vector = FAISS.from_texts(chunks, embeddings)\n",
        "\n",
        "# Retriever를 생성합니다.\n",
        "# base_retriever = vector.as_retriever()\n",
        "\n",
        "model_name = \"gpt-3.5-turbo\"\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPEN-AI_KEY')\n",
        "llm=ChatOpenAI(model=model_name, temperature=0.4, organization=openai_organization_id, max_tokens=500)\n",
        "\n",
        "# 프롬프트 템플릿 작성\n",
        "prompt_template = \"\"\"\n",
        "다음 질문에 대한 관련 정보를 검색하고 답변을 제공합니다:\n",
        "\n",
        "질문: {question}\n",
        "\n",
        "관련 정보: {retrieved_content}\n",
        "\n",
        "답변을 간결하고 명확하게 작성하세요.\n",
        "\"\"\"\n",
        "\n",
        "# 프롬프트를 생성합니다.\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"question\", \"retrieved_content\"],\n",
        "    template=prompt_template,\n",
        ")\n",
        "\n",
        "# LLMChain 생성\n",
        "qa_chain = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=prompt,\n",
        ")\n"
      ],
      "metadata": {
        "id": "pdMRQv77LEOu",
        "outputId": "7d5b4346-39b2-4e2c-95b2-5ff2c2ca169e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunk 1:\n",
            "#!/usr/bin/env python3\n",
            "from __future__ import annotations\n",
            "\n",
            "import logging\n",
            "import argparse\n",
            "import os\n",
            "import struct\n",
            "import sys\n",
            "from enum import IntEnum\n",
            "from pathlib import Path\n",
            "\n",
            "import numpy as np\n",
            "\n",
            "if 'NO_LOCAL_GGUF' not in os.environ:\n",
            "    sys.path.insert(1, str(Path(__file__).parent / 'gguf-py'))\n",
            "import gguf\n",
            "\n",
            "logger = logging.getLogger(\"ggml-to-gguf\")\n",
            "\n",
            "\n",
            "class GGMLFormat(IntEnum):\n",
            "    GGML = 0\n",
            "    GGMF = 1\n",
            "    GGJT = 2\n",
            "\n",
            "\n",
            "class GGMLFType(IntEnum):\n",
            "    ALL_F32              = 0\n",
            "    MOSTLY_F16           = 1\n",
            "    MOSTLY_Q4_0          = 2\n",
            "    MOSTLY_Q4_1          = 3\n",
            "    MOSTLY_Q4_1_SOME_F16 = 4\n",
            "    MOSTLY_Q8_0          = 7\n",
            "    MOSTLY_Q5_0          = 8\n",
            "    MOSTLY_Q5_1          = 9\n",
            "    MOSTLY_Q2_K          = 10\n",
            "    MOSTLY_Q3_K_S        = 11\n",
            "    MOSTLY_Q3_K_M        = 12\n",
            "    MOSTLY_Q3_K_L        = 13\n",
            "    MOSTLY_Q4_K_S        = 14\n",
            "    MOSTLY_Q4_K_M        = 15\n",
            "    MOSTLY_Q5_K_S        = 16\n",
            "    MOSTLY_Q5_K_M        = 17\n",
            "    MOSTLY_Q6_K          = 18\n",
            "\n",
            "\n",
            "class Hyperparameters:\n",
            "    def __init__(self):\n",
            "        self.n_vocab = self.n_embd = self.n_mult = self.n_head = 0\n",
            "        self.n_layer = self.n_rot = self.n_ff = 0\n",
            "        self.ftype = GGMLFType.ALL_F32\n",
            "\n",
            "    def set_n_ff(self, model):\n",
            "        ff_tensor_idx = model.tensor_map.get(b'layers.0.feed_forward.w1.weight')\n",
            "        assert ff_tensor_idx is not None, 'Missing layer 0 FF tensor'\n",
            "        ff_tensor = model.tensors[ff_tensor_idx]\n",
            "        self.n_ff = ff_tensor.dims[1]\n",
            "\n",
            "    def load(self, data, offset):\n",
            "        (\n",
            "            self.n_vocab,\n",
            "            self.n_embd,\n",
            "            self.n_mult,\n",
            "            self.n_head,\n",
            "            self.n_layer,\n",
            "            self.n_rot,\n",
            "            ftype,\n",
            "        ) = struct.unpack('<7I', data[offset:offset + (4 * 7)])\n",
            "        try:\n",
            "            self.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Chunk 2:\n",
            "   def load(self, data, offset):\n",
            "        (\n",
            "            self.n_vocab,\n",
            "            self.n_embd,\n",
            "            self.n_mult,\n",
            "            self.n_head,\n",
            "            self.n_layer,\n",
            "            self.n_rot,\n",
            "            ftype,\n",
            "        ) = struct.unpack('<7I', data[offset:offset + (4 * 7)])\n",
            "        try:\n",
            "            self.ftype = GGMLFType(ftype)\n",
            "        except ValueError:\n",
            "            raise ValueError(f'Invalid ftype {ftype}')\n",
            "        return 4 * 7\n",
            "\n",
            "    def __str__(self):\n",
            "        return f'<Hyperparameters: n_vocab={self.n_vocab}, n_embd={self.n_embd}, n_mult={self.n_mult}, n_head={self.n_head}, n_layer={self.n_layer}, n_rot={self.n_rot}, n_ff={self.n_ff}, ftype={self.ftype.name}>'\n",
            "\n",
            "\n",
            "class Vocab:\n",
            "    def __init__(self, load_scores = True):\n",
            "        self.items = []\n",
            "        self.load_scores = load_scores\n",
            "\n",
            "    def load(self, data, offset, n_vocab):\n",
            "        orig_offset = offset\n",
            "        for _ in range(n_vocab):\n",
            "            itemlen = struct.unpack('<I', data[offset:offset + 4])[0]\n",
            "            assert itemlen < 4096, 'Absurd vocab item length'\n",
            "            offset += 4\n",
            "            item_text = bytes(data[offset:offset + itemlen])\n",
            "            offset += itemlen\n",
            "            if self.load_scores:\n",
            "                item_score = struct.unpack('<f', data[offset:offset + 4])[0]\n",
            "                offset += 4\n",
            "            else:\n",
            "                item_score = 0.0\n",
            "            self.items.append((item_text, item_score))\n",
            "        return offset - orig_offset\n",
            "\n",
            "\n",
            "class Tensor:\n",
            "    def __init__(self, use_padding = True):\n",
            "        self.name = None\n",
            "        self.dims: tuple[int, ...] = ()\n",
            "        self.dtype = None\n",
            "        self.start_offset = 0\n",
            "        self.len_bytes = np.int64(0)\n",
            "        self.use_padding = use_padding\n",
            "\n",
            "    def load(self, data, offset):\n",
            "        orig_offset = offset\n",
            "        (n_dims, name_len, dtype) = struct.unpack('<3I', data[offset:offset + 12])\n",
            "        assert n_dims >= 0 and n_dims <= 4, f'Invalid tensor dimensions {n_dims}'\n",
            "        assert name_len < 4096, 'Absurd tensor name length'\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Chunk 3:\n",
            ": tuple[int, ...] = ()\n",
            "        self.dtype = None\n",
            "        self.start_offset = 0\n",
            "        self.len_bytes = np.int64(0)\n",
            "        self.use_padding = use_padding\n",
            "\n",
            "    def load(self, data, offset):\n",
            "        orig_offset = offset\n",
            "        (n_dims, name_len, dtype) = struct.unpack('<3I', data[offset:offset + 12])\n",
            "        assert n_dims >= 0 and n_dims <= 4, f'Invalid tensor dimensions {n_dims}'\n",
            "        assert name_len < 4096, 'Absurd tensor name length'\n",
            "        quant = gguf.GGML_QUANT_SIZES.get(dtype)\n",
            "        assert quant is not None, 'Unknown tensor type'\n",
            "        (blksize, tysize) = quant\n",
            "        offset += 12\n",
            "        self.dtype= dtype\n",
            "        self.dims = struct.unpack(f'<{n_dims}I', data[offset:offset + (4 * n_dims)])\n",
            "        offset += 4 * n_dims\n",
            "        self.name = bytes(data[offset:offset + name_len])\n",
            "        offset += name_len\n",
            "        pad = ((offset + 31) & ~31) - offset if self.use_padding else 0\n",
            "        offset += pad\n",
            "        n_elems = np.prod(self.dims)\n",
            "        n_bytes = np.int64(np.int64(n_elems) * np.int64(tysize)) // np.int64(blksize)\n",
            "        self.start_offset = offset\n",
            "        self.len_bytes = n_bytes\n",
            "        offset += n_bytes\n",
            "        return offset - orig_offset\n",
            "\n",
            "\n",
            "class GGMLModel:\n",
            "\n",
            "    file_format: GGMLFormat\n",
            "    format_version: int\n",
            "\n",
            "    def __init__(self):\n",
            "        self.hyperparameters = None\n",
            "        self.vocab = None\n",
            "        self.tensor_map = {}\n",
            "        self.tensors = []\n",
            "\n",
            "    def validate_header(self, data, offset):\n",
            "        magic = bytes(data[offset:offset + 4])\n",
            "        if magic == b'GGUF':\n",
            "            raise ValueError('File is already in GGUF format.')\n",
            "        if magic == b'lmgg':\n",
            "            self.file_format = GGMLFormat.GGML\n",
            "            self.format_version = 1\n",
            "            return 4\n",
            "        version = struct.unpack('<I', data[offset + 4:offset + 8])[0]\n",
            "        if magic == b'fmgg':\n",
            "            if version != 1:\n",
            "                raise ValueError(f'Cannot handle unexpected GGMF file version {version}')\n",
            "            self.file_format = GGMLFormat.GGMF\n",
            "            self.format_version = version\n",
            "            return 8\n",
            "        if magic == b'tjgg':\n",
            "            if version < 1 or version > 3:\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Chunk 4:\n",
            "      return 4\n",
            "        version = struct.unpack('<I', data[offset + 4:offset + 8])[0]\n",
            "        if magic == b'fmgg':\n",
            "            if version != 1:\n",
            "                raise ValueError(f'Cannot handle unexpected GGMF file version {version}')\n",
            "            self.file_format = GGMLFormat.GGMF\n",
            "            self.format_version = version\n",
            "            return 8\n",
            "        if magic == b'tjgg':\n",
            "            if version < 1 or version > 3:\n",
            "                raise ValueError(f'Cannot handle unexpected GGJT file version {version}')\n",
            "            self.file_format = GGMLFormat.GGJT\n",
            "            self.format_version = version\n",
            "            return 8\n",
            "        raise ValueError(f\"Unexpected file magic {magic!r}! This doesn't look like a GGML format file.\")\n",
            "\n",
            "    def validate_conversion(self, ftype):\n",
            "        err = ''\n",
            "        if (self.file_format < GGMLFormat.GGJT or self.format_version < 2):\n",
            "            if ftype not in (GGMLFType.ALL_F32, GGMLFType.MOSTLY_F16):\n",
            "                err = 'Quantizations changed in GGJTv2. Can only convert unquantized GGML files older than GGJTv2.'\n",
            "        elif (self.file_format == GGMLFormat.GGJT and self.format_version == 2):\n",
            "            if ftype in (GGMLFType.MOSTLY_Q4_0, GGMLFType.MOSTLY_Q4_1,\n",
            "                         GGMLFType.MOSTLY_Q4_1_SOME_F16, GGMLFType.MOSTLY_Q8_0):\n",
            "                err = 'Q4 and Q8 quantizations changed in GGJTv3.'\n",
            "        if len(err) > 0:\n",
            "            raise ValueError(f'{err} Sorry, your {self.file_format.name}v{self.format_version} file of type {ftype.name} is not eligible for conversion.')\n",
            "\n",
            "    def load(self, data, offset):\n",
            "        offset += self.validate_header(data, offset)\n",
            "        hp = Hyperparameters()\n",
            "        offset += hp.load(data, offset)\n",
            "        logger.info(f'* File format: {self.file_format.name}v{self.format_version} with ftype {hp.ftype.name}')\n",
            "        self.validate_conversion(hp.ftype)\n",
            "        vocab = Vocab(load_scores = self.file_format > GGMLFormat.GGML)\n",
            "        offset += vocab.load(data, offset, hp.n_vocab)\n",
            "        tensors: list[Tensor] = []\n",
            "        tensor_map = {}\n",
            "        while offset < len(data):\n",
            "            tensor = Tensor(use_padding = self.file_format > GGMLFormat.GGMF)\n",
            "         \n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Chunk 5:\n",
            " {self.file_format.name}v{self.format_version} with ftype {hp.ftype.name}')\n",
            "        self.validate_conversion(hp.ftype)\n",
            "        vocab = Vocab(load_scores = self.file_format > GGMLFormat.GGML)\n",
            "        offset += vocab.load(data, offset, hp.n_vocab)\n",
            "        tensors: list[Tensor] = []\n",
            "        tensor_map = {}\n",
            "        while offset < len(data):\n",
            "            tensor = Tensor(use_padding = self.file_format > GGMLFormat.GGMF)\n",
            "            offset += tensor.load(data, offset)\n",
            "            tensor_map[tensor.name] = len(tensors)\n",
            "            tensors.append(tensor)\n",
            "        self.hyperparameters = hp\n",
            "        self.vocab = vocab\n",
            "        self.tensors = tensors\n",
            "        self.tensor_map = tensor_map\n",
            "        hp.set_n_ff(self)\n",
            "        return offset\n",
            "\n",
            "\n",
            "class GGMLToGGUF:\n",
            "    def __init__(self, ggml_model, data, cfg, params_override = None, vocab_override = None, special_vocab = None):\n",
            "        hp = ggml_model.hyperparameters\n",
            "        self.model = ggml_model\n",
            "        self.data = data\n",
            "        self.cfg = cfg\n",
            "        self.params_override = params_override\n",
            "        self.vocab_override = vocab_override\n",
            "        self.special_vocab = special_vocab\n",
            "        if params_override is not None:\n",
            "            n_kv_head = params_override.n_head_kv\n",
            "        else:\n",
            "            if cfg.gqa == 1:\n",
            "                n_kv_head = hp.n_head\n",
            "            else:\n",
            "                gqa = float(cfg.gqa)\n",
            "                n_kv_head = None\n",
            "                for x in range(1, 256):\n",
            "                    if float(hp.n_head) / float(x) == gqa:\n",
            "                        n_kv_head = x\n",
            "                assert n_kv_head is not None, \"Couldn't determine n_kv_head from GQA param\"\n",
            "                logger.info(f'- Guessed n_kv_head = {n_kv_head} based on GQA {cfg.gqa}')\n",
            "        self.n_kv_head = n_kv_head\n",
            "        self.name_map = gguf.get_tensor_name_map(gguf.MODEL_ARCH.LLAMA, ggml_model.hyperparameters.n_layer)\n",
            "\n",
            "    def save(self):\n",
            "        logger.info('* Preparing to save GGUF file')\n",
            "    \n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Chunk 6:\n",
            "            assert n_kv_head is not None, \"Couldn't determine n_kv_head from GQA param\"\n",
            "                logger.info(f'- Guessed n_kv_head = {n_kv_head} based on GQA {cfg.gqa}')\n",
            "        self.n_kv_head = n_kv_head\n",
            "        self.name_map = gguf.get_tensor_name_map(gguf.MODEL_ARCH.LLAMA, ggml_model.hyperparameters.n_layer)\n",
            "\n",
            "    def save(self):\n",
            "        logger.info('* Preparing to save GGUF file')\n",
            "        gguf_writer = gguf.GGUFWriter(\n",
            "            self.cfg.output,\n",
            "            gguf.MODEL_ARCH_NAMES[gguf.MODEL_ARCH.LLAMA],\n",
            "            use_temp_file = False)\n",
            "        self.add_params(gguf_writer)\n",
            "        self.add_vocab(gguf_writer)\n",
            "        if self.special_vocab is not None:\n",
            "            self.special_vocab.add_to_gguf(gguf_writer)\n",
            "        self.add_tensors(gguf_writer)\n",
            "        logger.info(\"    gguf: write header\")\n",
            "        gguf_writer.write_header_to_file()\n",
            "        logger.info(\"    gguf: write metadata\")\n",
            "        gguf_writer.write_kv_data_to_file()\n",
            "        logger.info(\"    gguf: write tensors\")\n",
            "        gguf_writer.write_tensors_to_file()\n",
            "        gguf_writer.close()\n",
            "\n",
            "    def add_params(self, gguf_writer):\n",
            "        hp = self.model.hyperparameters\n",
            "        cfg = self.cfg\n",
            "        if cfg.desc is not None:\n",
            "            desc = cfg.desc\n",
            "        else:\n",
            "            desc = f'converted from legacy {self.model.file_format.name}v{self.model.format_version} {hp.ftype.name} format'\n",
            "        try:\n",
            "            # Filenames aren't necessarily valid UTF8.\n",
            "            name = cfg.name if cfg.name is not None else cfg.input.name\n",
            "        except UnicodeDecodeError:\n",
            "            name = None\n",
            "        logger.info('* Adding model parameters and KV items')\n",
            "        if name is not None:\n",
            "            gguf_writer.add_name(name)\n",
            "        gguf_writer.add_description(desc)\n",
            "        gguf_writer.add_file_type(int(hp.ftype))\n",
            "        if self.params_override is not None:\n",
            "            po = self.params_override\n",
            "            assert po.n_embd == hp.n_embd, 'Model hyperparams mismatch'\n",
            "            assert po.n_layer == hp.n_layer, 'Model hyperparams\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Chunk 7:\n",
            "  logger.info('* Adding model parameters and KV items')\n",
            "        if name is not None:\n",
            "            gguf_writer.add_name(name)\n",
            "        gguf_writer.add_description(desc)\n",
            "        gguf_writer.add_file_type(int(hp.ftype))\n",
            "        if self.params_override is not None:\n",
            "            po = self.params_override\n",
            "            assert po.n_embd == hp.n_embd, 'Model hyperparams mismatch'\n",
            "            assert po.n_layer == hp.n_layer, 'Model hyperparams mismatch'\n",
            "            assert po.n_head == hp.n_head, 'Model hyperparams mismatch'\n",
            "            gguf_writer.add_context_length      (po.n_ctx)\n",
            "            gguf_writer.add_embedding_length    (po.n_embd)\n",
            "            gguf_writer.add_block_count         (po.n_layer)\n",
            "            gguf_writer.add_feed_forward_length (po.n_ff)\n",
            "            gguf_writer.add_rope_dimension_count(po.n_embd // po.n_head)\n",
            "            gguf_writer.add_head_count          (po.n_head)\n",
            "            gguf_writer.add_head_count_kv       (po.n_head_kv)\n",
            "            gguf_writer.add_layer_norm_rms_eps  (po.f_norm_eps)\n",
            "            return\n",
            "        gguf_writer.add_context_length(cfg.context_length)\n",
            "        gguf_writer.add_embedding_length(hp.n_embd)\n",
            "        gguf_writer.add_block_count(hp.n_layer)\n",
            "        gguf_writer.add_feed_forward_length(hp.n_ff)\n",
            "        gguf_writer.add_rope_dimension_count(hp.n_embd // hp.n_head)\n",
            "        gguf_writer.add_head_count(hp.n_head)\n",
            "        gguf_writer.add_head_count_kv(self.n_kv_head)\n",
            "        gguf_writer.add_layer_norm_rms_eps(float(cfg.eps))\n",
            "\n",
            "    def add_vocab(self, gguf_writer):\n",
            "        hp = self.model.hyperparameters\n",
            "        gguf_writer.add_tokenizer_model('llama')\n",
            "        gguf_writer.add_tokenizer_pre('default')\n",
            "        tokens = []\n",
            "        scores = []\n",
            "        toktypes = []\n",
            "        if self.vocab_override is not None:\n",
            "            vo = self.vocab_override\n",
            "            logger.info('* Adding vocab item(s)')\n",
            "            for (_, (vbytes, score, ttype)) in enumerate(vo.all_tokens()):\n",
            " \n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Chunk 8:\n",
            "     hp = self.model.hyperparameters\n",
            "        gguf_writer.add_tokenizer_model('llama')\n",
            "        gguf_writer.add_tokenizer_pre('default')\n",
            "        tokens = []\n",
            "        scores = []\n",
            "        toktypes = []\n",
            "        if self.vocab_override is not None:\n",
            "            vo = self.vocab_override\n",
            "            logger.info('* Adding vocab item(s)')\n",
            "            for (_, (vbytes, score, ttype)) in enumerate(vo.all_tokens()):\n",
            "                tokens.append(vbytes)\n",
            "                scores.append(score)\n",
            "                toktypes.append(ttype)\n",
            "            assert len(tokens) == hp.n_vocab, \\\n",
            "                f'Override vocab has a different number of items than hyperparameters - override = {len(tokens)} but n_vocab={hp.n_vocab}'\n",
            "            gguf_writer.add_token_list(tokens)\n",
            "            gguf_writer.add_token_scores(scores)\n",
            "            if len(toktypes) > 0:\n",
            "                gguf_writer.add_token_types(toktypes)\n",
            "            return\n",
            "        logger.info(f'* Adding {hp.n_vocab} vocab item(s)')\n",
            "        assert len(self.model.vocab.items) >= 3, 'Cannot handle unexpectedly short model vocab'\n",
            "        for (tokid, (vbytes, vscore)) in enumerate(self.model.vocab.items):\n",
            "            tt = 1 # Normal\n",
            "            # Special handling for UNK, BOS, EOS tokens.\n",
            "            if tokid <= 2:\n",
            "                if tokid == 0:\n",
            "                    vbytes = b'<unk>'\n",
            "                    tt = 2\n",
            "                elif tokid == 1:\n",
            "                    vbytes = b'<s>'\n",
            "                    tt = 3\n",
            "                else:\n",
            "                    vbytes = b'</s>'\n",
            "                    tt = 3\n",
            "            elif len(vbytes) == 0:\n",
            "                tt = 3 # Control\n",
            "            elif tokid >= 3 and tokid <= 258 and len(vbytes) == 1:\n",
            "                vbytes = bytes(f'<0x{vbytes[0]:02X}>', encoding = 'UTF-8')\n",
            "                tt = 6 # Byte\n",
            " \n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Chunk 9:\n",
            "  else:\n",
            "                    vbytes = b'</s>'\n",
            "                    tt = 3\n",
            "            elif len(vbytes) == 0:\n",
            "                tt = 3 # Control\n",
            "            elif tokid >= 3 and tokid <= 258 and len(vbytes) == 1:\n",
            "                vbytes = bytes(f'<0x{vbytes[0]:02X}>', encoding = 'UTF-8')\n",
            "                tt = 6 # Byte\n",
            "            else:\n",
            "                vbytes = vbytes.replace(b' ', b'\\xe2\\x96\\x81')\n",
            "            toktypes.append(tt)\n",
            "            tokens.append(vbytes)\n",
            "            scores.append(vscore)\n",
            "        gguf_writer.add_token_list(tokens)\n",
            "        gguf_writer.add_token_scores(scores)\n",
            "        gguf_writer.add_token_types(toktypes)\n",
            "        gguf_writer.add_unk_token_id(0)\n",
            "        gguf_writer.add_bos_token_id(1)\n",
            "        gguf_writer.add_eos_token_id(2)\n",
            "\n",
            "    def add_tensors(self, gguf_writer):\n",
            "        tensor_map = self.name_map\n",
            "        data = self.data\n",
            "        logger.info(f'* Adding {len(self.model.tensors)} tensor(s)')\n",
            "        for tensor in self.model.tensors:\n",
            "            name = str(tensor.name, 'UTF-8')\n",
            "            mapped_name = tensor_map.get_name(name, try_suffixes = (\".weight\", \".bias\"))\n",
            "            assert mapped_name is not None, f'Bad name {name}'\n",
            "            tempdims = list(tensor.dims[:])\n",
            "            if len(tempdims) > 1:\n",
            "                temp = tempdims[1]\n",
            "                tempdims[1] = tempdims[0]\n",
            "                tempdims[0] = temp\n",
            "            gguf_writer.add_tensor(\n",
            "                mapped_name,\n",
            "                data[tensor.start_offset:tensor.start_offset + tensor.len_bytes],\n",
            "                raw_shape = tempdims,\n",
            "                raw_dtype = tensor.dtype)\n",
            "\n",
            "\n",
            "def handle_metadata(cfg, hp):\n",
            "    import examples.convert_legacy_llama as convert\n",
            "\n",
            "    assert cfg.model_metadata_dir.is_dir(), 'Metadata dir is not a directory'\n",
            "    hf_config_path   = cfg.model_metadata_dir / \"config.json\"\n",
            "    orig_config_\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Chunk 10:\n",
            "\n",
            "                mapped_name,\n",
            "                data[tensor.start_offset:tensor.start_offset + tensor.len_bytes],\n",
            "                raw_shape = tempdims,\n",
            "                raw_dtype = tensor.dtype)\n",
            "\n",
            "\n",
            "def handle_metadata(cfg, hp):\n",
            "    import examples.convert_legacy_llama as convert\n",
            "\n",
            "    assert cfg.model_metadata_dir.is_dir(), 'Metadata dir is not a directory'\n",
            "    hf_config_path   = cfg.model_metadata_dir / \"config.json\"\n",
            "    orig_config_path = cfg.model_metadata_dir / \"params.json\"\n",
            "    # We pass a fake model here. \"original\" mode will check the shapes of some\n",
            "    # tensors if information is missing in the .json file: other than that, the\n",
            "    # model data isn't used so this should be safe (at least for now).\n",
            "    fakemodel = {\n",
            "        'tok_embeddings.weight': convert.LazyTensor.__new__(convert.LazyTensor),\n",
            "        'layers.0.feed_forward.w1.weight': convert.LazyTensor.__new__(convert.LazyTensor),\n",
            "    }\n",
            "    fakemodel['tok_embeddings.weight'].shape = [hp.n_vocab]\n",
            "    fakemodel['layers.0.feed_forward.w1.weight'].shape = [hp.n_ff]\n",
            "    if hf_config_path.exists():\n",
            "        params = convert.Params.loadHFTransformerJson(fakemodel, hf_config_path)\n",
            "    elif orig_config_path.exists():\n",
            "        params = convert.Params.loadOriginalParamsJson(fakemodel, orig_config_path)\n",
            "    else:\n",
            "        raise ValueError('Unable to load metadata')\n",
            "    vocab_path = Path(cfg.vocab_dir if cfg.vocab_dir is not None else cfg.model_metadata_dir)\n",
            "    vocab_factory = convert.VocabFactory(vocab_path)\n",
            "    vocab, special_vocab = vocab_factory.load_vocab(cfg.vocabtype.split(\",\"), cfg.model_metadata_dir)\n",
            "    convert.check_vocab_size(params, vocab)\n",
            "    return params, vocab, special_vocab\n",
            "\n",
            "\n",
            "def handle_args():\n",
            "    parser = argparse.ArgumentParser(description = 'Convert GGML models to GGUF')\n",
            "    parser.add_argument('--input', '-i', type = Path, required = True,\n",
            "                        help = 'Input GGMLv3 filename')\n",
            "    parser.add_argument('--output', '-o', type = Path, required = True,\n",
            "                        help ='Output GGUF filename')\n",
            "    parser.add_argument('--name',\n",
            "                        help = 'Set model name')\n",
            "    parser.add_argument('--desc',\n",
            "                        help = 'Set model description')\n",
            "    parser.add_argument('--gqa', type = int, default = 1,\n",
            "                        help = 'grouped-query attention factor (use 8 for LLaMA2 70B)')\n",
            "    parser.add_argument('--eps', default =\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Chunk 11:\n",
            "                       help ='Output GGUF filename')\n",
            "    parser.add_argument('--name',\n",
            "                        help = 'Set model name')\n",
            "    parser.add_argument('--desc',\n",
            "                        help = 'Set model description')\n",
            "    parser.add_argument('--gqa', type = int, default = 1,\n",
            "                        help = 'grouped-query attention factor (use 8 for LLaMA2 70B)')\n",
            "    parser.add_argument('--eps', default = '5.0e-06',\n",
            "                        help = 'RMS norm eps: Use 1e-6 for LLaMA1 and OpenLLaMA, use 1e-5 for LLaMA2')\n",
            "    parser.add_argument('--context-length', '-c', type=int, default = 2048,\n",
            "                        help = 'Default max context length: LLaMA1 is typically 2048, LLaMA2 is typically 4096')\n",
            "    parser.add_argument('--model-metadata-dir', '-m', type = Path,\n",
            "                        help ='Load HuggingFace/.pth vocab and metadata from the specified directory')\n",
            "    parser.add_argument(\"--vocab-dir\", type=Path,\n",
            "                        help=\"directory containing tokenizer.model, if separate from model file - only meaningful with --model-metadata-dir\")\n",
            "    parser.add_argument(\"--vocabtype\", default=\"spm,hfft\",\n",
            "                        help=\"vocab format - only meaningful with --model-metadata-dir and/or --vocab-dir (default: spm,hfft)\")\n",
            "    parser.add_argument(\"--verbose\", action=\"store_true\", help=\"increase output verbosity\")\n",
            "    return parser.parse_args()\n",
            "\n",
            "\n",
            "def main():\n",
            "    cfg = handle_args()\n",
            "    logging.basicConfig(level=logging.DEBUG if cfg.verbose else logging.INFO)\n",
            "    logger.info(f'* Using config: {cfg}')\n",
            "    logger.warning('=== WARNING === Be aware that this conversion script is best-effort. Use a native GGUF model if possible. === WARNING ===')\n",
            "    if cfg.model_metadata_dir is None and (cfg.gqa == 1 or cfg.eps == '5.0e-06'):\n",
            "        logger.info('- Note: If converting LLaMA2, specifying \"--eps 1e-5\" is required. 70B models also need \"--gqa 8\".')\n",
            "    data = np.memmap(cfg.input, mode = 'r')\n",
            "    model = GGMLModel()\n",
            "    logger.info('* Scanning GGML input file')\n",
            "    offset = model.load(data, 0)  # noqa\n",
            "    logger.info(f'* GGML model hyperparameters: {model.hyperparameters}')\n",
            "    vocab_override = None\n",
            "    params_override = None\n",
            "    special_vocab = None\n",
            "    if cfg.model_metadata_dir is not None:\n",
            "        (params_override, vocab_override, special_vocab) = handle_metadata(cfg, model.hyperparameters)\n",
            "        logger.info('!! Note: When overriding params the --gqa, --eps and --context-length options are ignored.')\n",
            "       \n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Chunk 12:\n",
            "')\n",
            "    model = GGMLModel()\n",
            "    logger.info('* Scanning GGML input file')\n",
            "    offset = model.load(data, 0)  # noqa\n",
            "    logger.info(f'* GGML model hyperparameters: {model.hyperparameters}')\n",
            "    vocab_override = None\n",
            "    params_override = None\n",
            "    special_vocab = None\n",
            "    if cfg.model_metadata_dir is not None:\n",
            "        (params_override, vocab_override, special_vocab) = handle_metadata(cfg, model.hyperparameters)\n",
            "        logger.info('!! Note: When overriding params the --gqa, --eps and --context-length options are ignored.')\n",
            "        logger.info(f'* Overriding params: {params_override}')\n",
            "        logger.info(f'* Overriding vocab: {vocab_override}')\n",
            "        logger.info(f'* Special vocab: {special_vocab}')\n",
            "    else:\n",
            "        logger.warning('\\n=== WARNING === Special tokens may not be converted correctly. Use --model-metadata-dir if possible === WARNING ===\\n')\n",
            "        if model.file_format == GGMLFormat.GGML:\n",
            "            logger.info('! This is a very old GGML file that does not contain vocab scores. Strongly recommend using model metadata!')\n",
            "    converter = GGMLToGGUF(\n",
            "        model, data, cfg,\n",
            "        params_override = params_override,\n",
            "        vocab_override = vocab_override,\n",
            "        special_vocab = special_vocab\n",
            "    )\n",
            "    converter.save()\n",
            "    logger.info(f'* Successful completion. Output saved to: {cfg.output}')\n",
            "\n",
            "\n",
            "if __name__ == '__main__':\n",
            "    main()\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Chunk 13:\n",
            "#!/usr/bin/env python3\n",
            "# -*- coding: utf-8 -*-\n",
            "\n",
            "from __future__ import annotations\n",
            "\n",
            "import logging\n",
            "import argparse\n",
            "import contextlib\n",
            "import json\n",
            "import os\n",
            "import re\n",
            "import sys\n",
            "from enum import IntEnum\n",
            "from pathlib import Path\n",
            "from hashlib import sha256\n",
            "from typing import TYPE_CHECKING, Any, Callable, ContextManager, Iterable, Iterator, Literal, Sequence, TypeVar, cast\n",
            "\n",
            "import math\n",
            "import numpy as np\n",
            "import torch\n",
            "\n",
            "if TYPE_CHECKING:\n",
            "    from torch import Tensor\n",
            "\n",
            "if 'NO_LOCAL_GGUF' not in os.environ:\n",
            "    sys.path.insert(1, str(Path(__file__).parent / 'gguf-py'))\n",
            "import gguf\n",
            "\n",
            "logger = logging.getLogger(\"hf-to-gguf\")\n",
            "\n",
            "\n",
            "###### MODEL DEFINITIONS ######\n",
            "\n",
            "class SentencePieceTokenTypes(IntEnum):\n",
            "    NORMAL = 1\n",
            "    UNKNOWN = 2\n",
            "    CONTROL = 3\n",
            "    USER_DEFINED = 4\n",
            "    UNUSED = 5\n",
            "    BYTE = 6\n",
            "\n",
            "\n",
            "AnyModel = TypeVar(\"AnyModel\", bound=\"type[Model]\")\n",
            "\n",
            "\n",
            "class Model:\n",
            "    _model_classes: dict[str, type[Model]] = {}\n",
            "\n",
            "    dir_model: Path\n",
            "    ftype: gguf.LlamaFileType\n",
            "    fname_out: Path\n",
            "    is_big_endian: bool\n",
            "    endianess: gguf.GGUFEndian\n",
            "    use_temp_file: bool\n",
            "    lazy: bool\n",
            "    part_names: list[str]\n",
            "    is_safetensors: bool\n",
            "    hparams: dict[str, Any]\n",
            "    block_count: int\n",
            "    tensor_map: gguf.TensorNameMap\n",
            "    tensor_names: set[str] | None\n",
            "    gguf_writer: gguf.GGUFWriter\n",
            "    model_name: str | None\n",
            "    metadata_override: Path | None\n",
            "    dir_model_card: Path\n",
            "\n",
            "    # subclasses should define this!\n",
            "    model_arch: gguf.MODEL_ARCH\n",
            "\n",
            "    def __init__(self, dir_model: Path, ftype: gguf.LlamaFileType, fname_out: Path, is_big_endian: bool = False,\n",
            "                 use_temp_file: bool = False, eager: bool = False,\n",
            "                 metadata_override: Path | None = None, model_name: str | None = None,\n",
            "                 split_max_tensors: int = 0, split_max_size: int = 0, dry_run: bool = False, small_first_shard: bool = False):\n",
            "        if type(self) is Model:\n",
            "            raise TypeError(f\"{type(self).__name__!r} should not be directly instantiated\")\n",
            "\n",
            "        self.dir_model = dir_model\n",
            "        self.ftype = ftype\n",
            "        self.fname_out = fname_out\n",
            "        self.is_big_endian = is_big_endian\n",
            "        self.endianess = gguf.GGUFEndian.BIG if is_big_endian else gguf.GGUFEndian.LITTLE\n",
            "        self.use_temp_file = use_temp_file\n",
            "        self.lazy = not eager\n",
            "        self.part_names = Model.get_model_part_names(self.dir_model, \"model\", \".safetensors\")\n",
            "        self.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Chunk 14:\n",
            "     self.dir_model = dir_model\n",
            "        self.ftype = ftype\n",
            "        self.fname_out = fname_out\n",
            "        self.is_big_endian = is_big_endian\n",
            "        self.endianess = gguf.GGUFEndian.BIG if is_big_endian else gguf.GGUFEndian.LITTLE\n",
            "        self.use_temp_file = use_temp_file\n",
            "        self.lazy = not eager\n",
            "        self.part_names = Model.get_model_part_names(self.dir_model, \"model\", \".safetensors\")\n",
            "        self.is_safetensors = len(self.part_names) > 0\n",
            "        if not self.is_safetensors:\n",
            "            self.part_names = Model.get_model_part_names(self.dir_model, \"pytorch_model\", \".bin\")\n",
            "        self.hparams = Model.load_hparams(self.dir_model)\n",
            "        self.block_count = self.find_hparam([\"n_layers\", \"num_hidden_layers\", \"n_layer\", \"num_layers\"])\n",
            "        self.tensor_map = gguf.get_tensor_name_map(self.model_arch, self.block_count)\n",
            "        self.tensor_names = None\n",
            "        self.metadata_override = metadata_override\n",
            "        self.model_name = model_name\n",
            "        self.dir_model_card = dir_model  # overridden in convert_lora_to_gguf.py\n",
            "\n",
            "        # Apply heuristics to figure out typical tensor encoding based on first layer tensor encoding type\n",
            "        if self.ftype == gguf.LlamaFileType.GUESSED:\n",
            "            # NOTE: can't use field \"torch_dtype\" in config.json, because some finetunes lie.\n",
            "            _, first_tensor = next(self.get_tensors())\n",
            "            if first_tensor.dtype == torch.float16:\n",
            "                logger.info(f\"choosing --outtype f16 from first tensor type ({first_tensor.dtype})\")\n",
            "                self.ftype = gguf.LlamaFileType.MOSTLY_F16\n",
            "            else:\n",
            "                logger.info(f\"choosing --outtype bf16 from first tensor type ({first_tensor.dtype})\")\n",
            "                self.ftype = gguf.LlamaFileType.MOSTLY_BF16\n",
            "\n",
            "        # Configure GGUF Writer\n",
            "        self.gguf_writer = gguf.GGUFWriter(path=None, arch=gguf.MODEL_ARCH_NAMES[self.model_arch], endianess=self.endianess, use_temp_file=self.use_temp_file,\n",
            "                                           split_max_tensors=split_max_tensors, split_max_size=split_max_size, dry_run=dry_run, small_first_shard=small_first_shard)\n",
            "\n",
            "    @classmethod\n",
            "    def __init_subclass\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Chunk 15:\n",
            "MOSTLY_BF16\n",
            "\n",
            "        # Configure GGUF Writer\n",
            "        self.gguf_writer = gguf.GGUFWriter(path=None, arch=gguf.MODEL_ARCH_NAMES[self.model_arch], endianess=self.endianess, use_temp_file=self.use_temp_file,\n",
            "                                           split_max_tensors=split_max_tensors, split_max_size=split_max_size, dry_run=dry_run, small_first_shard=small_first_shard)\n",
            "\n",
            "    @classmethod\n",
            "    def __init_subclass__(cls):\n",
            "        # can't use an abstract property, because overriding it without type errors\n",
            "        # would require using decorated functions instead of simply defining the property\n",
            "        if \"model_arch\" not in cls.__dict__:\n",
            "            raise TypeError(f\"Missing property 'model_arch' for {cls.__name__!r}\")\n",
            "\n",
            "    def find_hparam(self, keys: Iterable[str], optional: bool = False) -> Any:\n",
            "        key = next((k for k in keys if k in self.hparams), None)\n",
            "        if key is not None:\n",
            "            return self.hparams[key]\n",
            "        if optional:\n",
            "            return None\n",
            "        raise KeyError(f\"could not find any of: {keys}\")\n",
            "\n",
            "    def set_vocab(self):\n",
            "        self._set_vocab_gpt2()\n",
            "\n",
            "    def get_tensors(self) -> Iterator[tuple[str, Tensor]]:\n",
            "        tensor_names_from_parts: set[str] = set()\n",
            "\n",
            "        if len(self.part_names) > 1:\n",
            "            self.tensor_names = set()\n",
            "            index_name = \"model.safetensors\" if self.is_safetensors else \"pytorch_model.bin\"\n",
            "            index_name += \".index.json\"\n",
            "            logger.info(f\"gguf: loading model weight map from '{index_name}'\")\n",
            "            with open(self.dir_model / index_name, \"r\", encoding=\"utf-8\") as f:\n",
            "                index: dict[str, Any] = json.load(f)\n",
            "                weight_map = index.get(\"weight_map\")\n",
            "                if weight_map is None or not isinstance(weight_map, dict):\n",
            "                    raise ValueError(f\"Can't load 'weight_map' from {index_name!r}\")\n",
            "                self.tensor_names.update(weight_map.keys())\n",
            "        else:\n",
            "            self.tensor_names = tensor_names_from_parts\n",
            "\n",
            "        for part_name in self.part_names:\n",
            "            logger.info(f\"gguf: loading model part '{part_name}'\")\n",
            "            ctx: ContextManager[Any]\n",
            "            if self.is_safetensors:\n",
            " \n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Chunk 16:\n",
            "               raise ValueError(f\"Can't load 'weight_map' from {index_name!r}\")\n",
            "                self.tensor_names.update(weight_map.keys())\n",
            "        else:\n",
            "            self.tensor_names = tensor_names_from_parts\n",
            "\n",
            "        for part_name in self.part_names:\n",
            "            logger.info(f\"gguf: loading model part '{part_name}'\")\n",
            "            ctx: ContextManager[Any]\n",
            "            if self.is_safetensors:\n",
            "                from safetensors import safe_open\n",
            "                ctx = cast(ContextManager[Any], safe_open(self.dir_model / part_name, framework=\"pt\", device=\"cpu\"))\n",
            "            else:\n",
            "                ctx = contextlib.nullcontext(torch.load(str(self.dir_model / part_name), map_location=\"cpu\", mmap=True, weights_only=True))\n",
            "\n",
            "            with ctx as model_part:\n",
            "                tensor_names_from_parts.update(model_part.keys())\n",
            "\n",
            "                for name in model_part.keys():\n",
            "                    if self.is_safetensors:\n",
            "                        if self.lazy:\n",
            "                            data = model_part.get_slice(name)\n",
            "                            data = LazyTorchTensor.from_safetensors_slice(data)\n",
            "                        else:\n",
            "                            data = model_part.get_tensor(name)\n",
            "                    else:\n",
            "                        data = model_part[name]\n",
            "                        if self.lazy:\n",
            "                            data = LazyTorchTensor.from_eager(data)\n",
            "                    yield name, data\n",
            "\n",
            "        # only verify tensor name presence; it doesn't matter if they are not in the right files\n",
            "        if len(sym_diff := tensor_names_from_parts.symmetric_difference(self.tensor_names)) > 0:\n",
            "            raise ValueError(f\"Mismatch between weight map and model parts for tensor names: {sym_diff}\")\n",
            "\n",
            "    def format_tensor_name(self, key: gguf.MODEL_TENSOR, bid: int | None = None, suffix: str = \".weight\") -> str:\n",
            "        if key not in gguf.MODEL_TENSORS[self.model_arch]:\n",
            "            raise ValueError(f\"Missing {key!r} for MODEL_TENS\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Chunk 17:\n",
            " tensor name presence; it doesn't matter if they are not in the right files\n",
            "        if len(sym_diff := tensor_names_from_parts.symmetric_difference(self.tensor_names)) > 0:\n",
            "            raise ValueError(f\"Mismatch between weight map and model parts for tensor names: {sym_diff}\")\n",
            "\n",
            "    def format_tensor_name(self, key: gguf.MODEL_TENSOR, bid: int | None = None, suffix: str = \".weight\") -> str:\n",
            "        if key not in gguf.MODEL_TENSORS[self.model_arch]:\n",
            "            raise ValueError(f\"Missing {key!r} for MODEL_TENSORS of {self.model_arch!r}\")\n",
            "        name: str = gguf.TENSOR_NAMES[key]\n",
            "        if \"{bid}\" in name:\n",
            "            assert bid is not None\n",
            "            name = name.format(bid=bid)\n",
            "        return name + suffix\n",
            "\n",
            "    def match_model_tensor_name(self, name: str, key: gguf.MODEL_TENSOR, bid: int | None, suffix: str = \".weight\") -> bool:\n",
            "        if key not in gguf.MODEL_TENSORS[self.model_arch]:\n",
            "            return False\n",
            "        key_name: str = gguf.TENSOR_NAMES[key]\n",
            "        if \"{bid}\" in key_name:\n",
            "            if bid is None:\n",
            "                return False\n",
            "            key_name = key_name.format(bid=bid)\n",
            "        else:\n",
            "            if bid is not None:\n",
            "                return False\n",
            "        return name == (key_name + suffix)\n",
            "\n",
            "    def map_tensor_name(self, name: str, try_suffixes: Sequence[str] = (\".weight\", \".bias\")) -> str:\n",
            "        new_name = self.tensor_map.get_name(key=name, try_suffixes=try_suffixes)\n",
            "        if new_name is None:\n",
            "            raise ValueError(f\"Can not map tensor {name!r}\")\n",
            "        return new_name\n",
            "\n",
            "    def set_gguf_parameters(self):\n",
            "        self.gguf_writer.add_block_count(self.block_count)\n",
            "\n",
            "        if (n_ctx := self.find_hparam([\"max_position_embeddings\", \"n_ctx\"], optional=True)) is not None:\n",
            "            self.gguf_writer.add_context_length(n_ctx)\n",
            "            logger.info(f\"gguf: context length = {n_ctx}\")\n",
            "\n",
            "        n_embd = self.find_hparam([\"hidden_size\", \"n_embd\"])\n",
            "        self.gguf_writer.add_embedding_length(n_embd)\n",
            "        logger.info(f\"gguf: embedding length = {n_embd}\")\n",
            "\n",
            "        if (n_ff := self.find_hparam([\"intermediate_size\", \"n_inner\"], optional=True)) is not None:\n",
            "            self.gguf_writer.add_feed_forward_length(n_ff\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Chunk 18:\n",
            "_writer.add_context_length(n_ctx)\n",
            "            logger.info(f\"gguf: context length = {n_ctx}\")\n",
            "\n",
            "        n_embd = self.find_hparam([\"hidden_size\", \"n_embd\"])\n",
            "        self.gguf_writer.add_embedding_length(n_embd)\n",
            "        logger.info(f\"gguf: embedding length = {n_embd}\")\n",
            "\n",
            "        if (n_ff := self.find_hparam([\"intermediate_size\", \"n_inner\"], optional=True)) is not None:\n",
            "            self.gguf_writer.add_feed_forward_length(n_ff)\n",
            "            logger.info(f\"gguf: feed forward length = {n_ff}\")\n",
            "\n",
            "        n_head = self.find_hparam([\"num_attention_heads\", \"n_head\"])\n",
            "        self.gguf_writer.add_head_count(n_head)\n",
            "        logger.info(f\"gguf: head count = {n_head}\")\n",
            "\n",
            "        if (n_head_kv := self.hparams.get(\"num_key_value_heads\")) is not None:\n",
            "            self.gguf_writer.add_head_count_kv(n_head_kv)\n",
            "            logger.info(f\"gguf: key-value head count = {n_head_kv}\")\n",
            "\n",
            "        if (rope_theta := self.hparams.get(\"rope_theta\")) is not None:\n",
            "            self.gguf_writer.add_rope_freq_base(rope_theta)\n",
            "            logger.info(f\"gguf: rope theta = {rope_theta}\")\n",
            "        if (f_rms_eps := self.hparams.get(\"rms_norm_eps\")) is not None:\n",
            "            self.gguf_writer.add_layer_norm_rms_eps(f_rms_eps)\n",
            "            logger.info(f\"gguf: rms norm epsilon = {f_rms_eps}\")\n",
            "        if (f_norm_eps := self.find_hparam([\"layer_norm_eps\", \"layer_norm_epsilon\", \"norm_epsilon\"], optional=True)) is not None:\n",
            "            self.gguf_writer.add_layer_norm_eps(f_norm_eps)\n",
            "            logger.info(f\"gguf: layer norm epsilon = {f_norm_eps}\")\n",
            "        if (n_experts := self.hparams.get(\"num_local_experts\")) is not None:\n",
            "            self.gguf_writer.add_expert_count(n_experts)\n",
            "            logger.info(f\"gguf: expert count = {n_experts}\")\n",
            "        if (n_experts_used := self.hparams.get(\"num_experts_per_tok\")) is not None:\n",
            "            self.gguf_writer.add_expert_used_count(n_experts_used)\n",
            "            logger.info(f\"gguf: experts used count = {n_experts_used}\")\n",
            "\n",
            "        if (head_dim := self.hparams.get(\"head_dim\")) is not None:\n",
            "            self.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Chunk 19:\n",
            " self.gguf_writer.add_expert_count(n_experts)\n",
            "            logger.info(f\"gguf: expert count = {n_experts}\")\n",
            "        if (n_experts_used := self.hparams.get(\"num_experts_per_tok\")) is not None:\n",
            "            self.gguf_writer.add_expert_used_count(n_experts_used)\n",
            "            logger.info(f\"gguf: experts used count = {n_experts_used}\")\n",
            "\n",
            "        if (head_dim := self.hparams.get(\"head_dim\")) is not None:\n",
            "            self.gguf_writer.add_key_length(head_dim)\n",
            "            self.gguf_writer.add_value_length(head_dim)\n",
            "\n",
            "        self.gguf_writer.add_file_type(self.ftype)\n",
            "        logger.info(f\"gguf: file type = {self.ftype}\")\n",
            "\n",
            "    def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -> Iterable[tuple[str, Tensor]]:\n",
            "        del bid  # unused\n",
            "\n",
            "        return [(self.map_tensor_name(name), data_torch)]\n",
            "\n",
            "    def tensor_force_quant(self, name: str, new_name: str, bid: int | None, n_dims: int) -> gguf.GGMLQuantizationType | bool:\n",
            "        del name, new_name, bid, n_dims  # unused\n",
            "\n",
            "        return False\n",
            "\n",
            "    def prepare_tensors(self):\n",
            "        max_name_len = max(len(s) for _, s in self.tensor_map.mapping.values()) + len(\".weight,\")\n",
            "\n",
            "        for name, data_torch in self.get_tensors():\n",
            "            # we don't need these\n",
            "            if name.endswith((\".attention.masked_bias\", \".attention.bias\", \".rotary_emb.inv_freq\")):\n",
            "                continue\n",
            "\n",
            "            old_dtype = data_torch.dtype\n",
            "\n",
            "            # convert any unsupported data types to float32\n",
            "            if data_torch.dtype not in (torch.float16, torch.float32):\n",
            "                data_torch = data_torch.to(torch.float32)\n",
            "\n",
            "            # use the first number-like part of the tensor name as the block id\n",
            "            bid = None\n",
            "            for part in name.split(\".\"):\n",
            "                if part.isdecimal():\n",
            "                    bid = int(part)\n",
            "                    break\n",
            "\n",
            "            for new_name, data in ((n, d.squeeze().numpy()) for n, d in self.modify_tensors(data_torch, name, bid)):\n",
            "                data: np.ndarray  # type hint\n",
            "                n_dims = len(data.shape)\n",
            "               \n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Chunk 20:\n",
            " name.split(\".\"):\n",
            "                if part.isdecimal():\n",
            "                    bid = int(part)\n",
            "                    break\n",
            "\n",
            "            for new_name, data in ((n, d.squeeze().numpy()) for n, d in self.modify_tensors(data_torch, name, bid)):\n",
            "                data: np.ndarray  # type hint\n",
            "                n_dims = len(data.shape)\n",
            "                data_qtype: gguf.GGMLQuantizationType | bool = self.tensor_force_quant(name, new_name, bid, n_dims)\n",
            "\n",
            "                # Most of the codebase that takes in 1D tensors or norms only handles F32 tensors\n",
            "                if n_dims <= 1 or new_name.endswith(\"_norm.weight\"):\n",
            "                    data_qtype = gguf.GGMLQuantizationType.F32\n",
            "\n",
            "                # Conditions should closely match those in llama_model_quantize_internal in llama.cpp\n",
            "                # Some tensor types are always in float32\n",
            "                if data_qtype is False and (\n",
            "                    any(\n",
            "                        self.match_model_tensor_name(new_name, key, bid)\n",
            "                        for key in (\n",
            "                            gguf.MODEL_TENSOR.FFN_GATE_INP,\n",
            "                            gguf.MODEL_TENSOR.POS_EMBD,\n",
            "                            gguf.MODEL_TENSOR.TOKEN_TYPES,\n",
            "                        )\n",
            "                    )\n",
            "                    or not name.endswith(\".weight\")\n",
            "                ):\n",
            "                    data_qtype = gguf.GGMLQuantizationType.F32\n",
            "\n",
            "                # No override (data_qtype is False), or wants to be quantized (data_qtype is True)\n",
            "                if isinstance(data_qtype, bool):\n",
            "                    if self.ftype == gguf.LlamaFileType.ALL_F32:\n",
            "                        data_qtype = gguf.GGMLQuantizationType.F32\n",
            "                    elif self.ftype == gguf.LlamaFileType.MOSTLY_F16:\n",
            "    \n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Chunk 21:\n",
            "32\n",
            "\n",
            "                # No override (data_qtype is False), or wants to be quantized (data_qtype is True)\n",
            "                if isinstance(data_qtype, bool):\n",
            "                    if self.ftype == gguf.LlamaFileType.ALL_F32:\n",
            "                        data_qtype = gguf.GGMLQuantizationType.F32\n",
            "                    elif self.ftype == gguf.LlamaFileType.MOSTLY_F16:\n",
            "                        data_qtype = gguf.GGMLQuantizationType.F16\n",
            "                    elif self.ftype == gguf.LlamaFileType.MOSTLY_BF16:\n",
            "                        data_qtype = gguf.GGMLQuantizationType.BF16\n",
            "                    elif self.ftype == gguf.LlamaFileType.MOSTLY_Q8_0:\n",
            "                        data_qtype = gguf.GGMLQuantizationType.Q8_0\n",
            "                    else:\n",
            "                        raise ValueError(f\"Unknown file type: {self.ftype.name}\")\n",
            "\n",
            "                try:\n",
            "                    data = gguf.quants.quantize(data, data_qtype)\n",
            "                except gguf.QuantError as e:\n",
            "                    logger.warning(\"%s, %s\", e, \"falling back to F16\")\n",
            "                    data_qtype = gguf.GGMLQuantizationType.F16\n",
            "                    data = gguf.quants.quantize(data, data_qtype)\n",
            "\n",
            "                shape = gguf.quant_shape_from_byte_shape(data.shape, data_qtype) if data.dtype == np.uint8 else data.shape\n",
            "\n",
            "                # reverse shape to make it similar to the internal ggml dimension order\n",
            "                shape_str = f\"{{{', '.join(str(n) for n in reversed(shape))}}}\"\n",
            "\n",
            "                # n_dims is implicit in the shape\n",
            "                logger.info(f\"{f'%-{max_name_len}s' % f'{new_name},'} {old_dtype} --> {data_qtype.name}, shape = {shape_str}\")\n",
            "\n",
            "                self.gguf_writer.add_tensor(new_name, data, raw_dtype=data_qtype)\n",
            "\n",
            "    def set_type(self):\n",
            "        self.gguf_writer.add_type(gguf.GGUFType.MODEL)\n",
            "\n",
            "    def prepare_metadata(self, vocab_only: bool):\n",
            "\n",
            "  \n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Chunk 22:\n",
            "}\"\n",
            "\n",
            "                # n_dims is implicit in the shape\n",
            "                logger.info(f\"{f'%-{max_name_len}s' % f'{new_name},'} {old_dtype} --> {data_qtype.name}, shape = {shape_str}\")\n",
            "\n",
            "                self.gguf_writer.add_tensor(new_name, data, raw_dtype=data_qtype)\n",
            "\n",
            "    def set_type(self):\n",
            "        self.gguf_writer.add_type(gguf.GGUFType.MODEL)\n",
            "\n",
            "    def prepare_metadata(self, vocab_only: bool):\n",
            "\n",
            "        total_params, shared_params, expert_params, expert_count = self.gguf_writer.get_total_parameter_count()\n",
            "\n",
            "        self.metadata = gguf.Metadata.load(self.metadata_override, self.dir_model_card, self.model_name, total_params)\n",
            "\n",
            "        # Fallback to model directory name if metadata name is still missing\n",
            "        if self.metadata.name is None:\n",
            "            self.metadata.name = self.dir_model.name\n",
            "\n",
            "        # Generate parameter weight class (useful for leader boards) if not yet determined\n",
            "        if self.metadata.size_label is None and total_params > 0:\n",
            "            self.metadata.size_label = gguf.size_label(total_params, shared_params, expert_params, expert_count)\n",
            "\n",
            "        # Extract the encoding scheme from the file type name. e.g. 'gguf.LlamaFileType.MOSTLY_Q8_0' --> 'Q8_0'\n",
            "        output_type: str = self.ftype.name.partition(\"_\")[2]\n",
            "\n",
            "        # Filename Output\n",
            "        if self.fname_out.is_dir():\n",
            "            # Generate default filename based on model specification and available metadata\n",
            "            if not vocab_only:\n",
            "                fname_default: str = gguf.naming_convention(self.metadata.name, self.metadata.basename, self.metadata.finetune, self.metadata.version, self.metadata.size_label, output_type, model_type=\"LoRA\" if total_params < 0 else None)\n",
            "            else:\n",
            "                fname_default: str = gguf.naming_convention(self.metadata.name, self.metadata.basename, self.metadata.finetune, self.metadata.version, size_label=None, output_type=None, model_type=\"vocab\")\n",
            "\n",
            "            # Use the default filename\n",
            "            self.fname_out = self.fname_out / f\"{fname_default}.gguf\"\n",
            "        else:\n",
            "            # Output path is a custom defined templated filename\n",
            "            # Note: `not is_dir()` is used because `.is_file()` will not detect\n",
            "            #       file template strings as it doesn't actually exist as a file\n",
            "\n",
            "            # Process templated file name with the output ftype, useful with the \"auto\" ftype\n",
            "            self.fname_out = self.fname_out.parent / gguf.fill_templated_filename(\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Chunk 23:\n",
            "    self.fname_out = self.fname_out / f\"{fname_default}.gguf\"\n",
            "        else:\n",
            "            # Output path is a custom defined templated filename\n",
            "            # Note: `not is_dir()` is used because `.is_file()` will not detect\n",
            "            #       file template strings as it doesn't actually exist as a file\n",
            "\n",
            "            # Process templated file name with the output ftype, useful with the \"auto\" ftype\n",
            "            self.fname_out = self.fname_out.parent / gguf.fill_templated_filename(self.fname_out.name, output_type)\n",
            "\n",
            "        self.set_type()\n",
            "\n",
            "        logger.info(\"Set meta model\")\n",
            "        self.metadata.set_gguf_meta_model(self.gguf_writer)\n",
            "\n",
            "        logger.info(\"Set model parameters\")\n",
            "        self.set_gguf_parameters()\n",
            "\n",
            "        logger.info(\"Set model tokenizer\")\n",
            "        self.set_vocab()\n",
            "\n",
            "        logger.info(\"Set model quantization version\")\n",
            "        self.gguf_writer.add_quantization_version(gguf.GGML_QUANT_VERSION)\n",
            "\n",
            "    def write(self):\n",
            "        self.prepare_tensors()\n",
            "        self.prepare_metadata(vocab_only=False)\n",
            "        self.gguf_writer.write_header_to_file(path=self.fname_out)\n",
            "        self.gguf_writer.write_kv_data_to_file()\n",
            "        self.gguf_writer.write_tensors_to_file(progress=True)\n",
            "        self.gguf_writer.close()\n",
            "\n",
            "    def write_vocab(self):\n",
            "        if len(self.gguf_writer.tensors) != 1:\n",
            "            raise ValueError('Splitting the vocabulary is not supported')\n",
            "\n",
            "        self.prepare_metadata(vocab_only=True)\n",
            "        self.gguf_writer.write_header_to_file(path=self.fname_out)\n",
            "        self.gguf_writer.write_kv_data_to_file()\n",
            "        self.gguf_writer.close()\n",
            "\n",
            "    @staticmethod\n",
            "    def get_model_part_names(dir_model: Path, prefix: str, suffix: str) -> list[str]:\n",
            "        part_names: list[str] = []\n",
            "        for filename in os.listdir(dir_model):\n",
            "            if filename.startswith(prefix) and filename.endswith(suffix):\n",
            "                part_names.append(filename)\n",
            "\n",
            "        part_names.sort()\n",
            "\n",
            "        return part_names\n",
            "\n",
            "    @staticmethod\n",
            "    def load_hparams(dir_model: Path):\n",
            "        with open(dir_model / \"config.json\", \"r\", encoding=\"utf-8\") as f:\n",
            "            return json.load(f)\n",
            "\n",
            "    @classmethod\n",
            "    def register(cls, *names: str) -> Callable[[AnyModel], AnyModel]:\n",
            "        assert names\n",
            "\n",
            "        def func(modelcls: AnyModel) -> AnyModel:\n",
            "     \n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Chunk 24:\n",
            "swith(suffix):\n",
            "                part_names.append(filename)\n",
            "\n",
            "        part_names.sort()\n",
            "\n",
            "        return part_names\n",
            "\n",
            "    @staticmethod\n",
            "    def load_hparams(dir_model: Path):\n",
            "        with open(dir_model / \"config.json\", \"r\", encoding=\"utf-8\") as f:\n",
            "            return json.load(f)\n",
            "\n",
            "    @classmethod\n",
            "    def register(cls, *names: str) -> Callable[[AnyModel], AnyModel]:\n",
            "        assert names\n",
            "\n",
            "        def func(modelcls: AnyModel) -> AnyModel:\n",
            "            for name in names:\n",
            "                cls._model_classes[name] = modelcls\n",
            "            return modelcls\n",
            "        return func\n",
            "\n",
            "    @classmethod\n",
            "    def from_model_architecture(cls, arch: str) -> type[Model]:\n",
            "        try:\n",
            "            return cls._model_classes[arch]\n",
            "        except KeyError:\n",
            "            raise NotImplementedError(f'Architecture {arch!r} not supported!') from None\n",
            "\n",
            "    def does_token_look_special(self, token: str | bytes) -> bool:\n",
            "        if isinstance(token, (bytes, bytearray)):\n",
            "            token_text = token.decode(encoding=\"utf-8\")\n",
            "        elif isinstance(token, memoryview):\n",
            "            token_text = token.tobytes().decode(encoding=\"utf-8\")\n",
            "        else:\n",
            "            token_text = token\n",
            "\n",
            "        # Some models mark some added tokens which ought to be control tokens as not special.\n",
            "        # (e.g. command-r, command-r-plus, deepseek-coder, gemma{,-2})\n",
            "        seems_special = token_text in (\n",
            "            \"<pad>\",  # deepseek-coder\n",
            "            \"<mask>\", \"<2mass>\", \"[@BOS@]\",  # gemma{,-2}\n",
            "        )\n",
            "\n",
            "        seems_special = seems_special or (token_text.startswith(\"<|\") and token_text.endswith(\"|>\"))\n",
            "        seems_special = seems_special or (token_text.startswith(\"<｜\") and token_text.endswith(\"｜>\"))  # deepseek-coder\n",
            "\n",
            "        # TODO: should these be marked as UNUSED instead? (maybe not)\n",
            "        seems_special = seems_special or (token_text.startswith(\"<unused\") and token_text.endswith(\">\"))  # gemma{,-2}\n",
            "\n",
            "        return seems_special\n",
            "\n",
            "    # used for GPT-2 BPE and WordPiece vocabs\n",
            "    def get_vocab_base(self) -> tuple[list[str], list[int], str]:\n",
            "        tokens: list[str] = []\n",
            "        toktypes: list[int] = []\n",
            "\n",
            "        from transformers import AutoTokenizer\n",
            "        tokenizer = AutoTokenizer.from_pretrained(self.dir_model)\n",
            "        vocab_size = self.hparams.get(\"vocab_size\", len(\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Chunk 25:\n",
            " seems_special = seems_special or (token_text.startswith(\"<unused\") and token_text.endswith(\">\"))  # gemma{,-2}\n",
            "\n",
            "        return seems_special\n",
            "\n",
            "    # used for GPT-2 BPE and WordPiece vocabs\n",
            "    def get_vocab_base(self) -> tuple[list[str], list[int], str]:\n",
            "        tokens: list[str] = []\n",
            "        toktypes: list[int] = []\n",
            "\n",
            "        from transformers import AutoTokenizer\n",
            "        tokenizer = AutoTokenizer.from_pretrained(self.dir_model)\n",
            "        vocab_size = self.hparams.get(\"vocab_size\", len(tokenizer.vocab))\n",
            "        assert max(tokenizer.vocab.values()) < vocab_size\n",
            "\n",
            "        tokpre = self.get_vocab_base_pre(tokenizer)\n",
            "\n",
            "        reverse_vocab = {id_: encoded_tok for encoded_tok, id_ in tokenizer.vocab.items()}\n",
            "        added_vocab = tokenizer.get_added_vocab()\n",
            "\n",
            "        for i in range(vocab_size):\n",
            "            if i not in reverse_vocab:\n",
            "                tokens.append(f\"[PAD{i}]\")\n",
            "                toktypes.append(gguf.TokenType.UNUSED)\n",
            "            else:\n",
            "                token: str = reverse_vocab[i]\n",
            "                if token in added_vocab:\n",
            "                    if tokenizer.added_tokens_decoder[i].special or self.does_token_look_special(token):\n",
            "                        toktypes.append(gguf.TokenType.CONTROL)\n",
            "                    else:\n",
            "                        token = token.replace(b\"\\xe2\\x96\\x81\".decode(\"utf-8\"), \" \")  # pre-normalize user-defined spaces\n",
            "                        toktypes.append(gguf.TokenType.USER_DEFINED)\n",
            "                else:\n",
            "                    toktypes.append(gguf.TokenType.NORMAL)\n",
            "                tokens.append(token)\n",
            "\n",
            "        return tokens, toktypes, tokpre\n",
            "\n",
            "    # NOTE: this function is generated by convert_hf_to_gguf_update.py\n",
            "    #       do not modify it manually!\n",
            "    # ref:  https://github.com/ggerganov/llama.cpp/pull/6920\n",
            "    # Marker: Start get_vocab_base_pre\n",
            "    def get_vocab_base_pre(self, tokenizer) -> str:\n",
            "        # encoding this string and hashing the resulting tokens would (hopefully) give us a unique identifier that\n",
            "        # is specific for the BPE pre-tokenizer used by the model\n",
            "        # we will use this unique identifier to write a \"tokenizer.ggml.pre\" entry in the GGUF file which we can\n",
            "        # use in llama.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Chunk 26:\n",
            " is generated by convert_hf_to_gguf_update.py\n",
            "    #       do not modify it manually!\n",
            "    # ref:  https://github.com/ggerganov/llama.cpp/pull/6920\n",
            "    # Marker: Start get_vocab_base_pre\n",
            "    def get_vocab_base_pre(self, tokenizer) -> str:\n",
            "        # encoding this string and hashing the resulting tokens would (hopefully) give us a unique identifier that\n",
            "        # is specific for the BPE pre-tokenizer used by the model\n",
            "        # we will use this unique identifier to write a \"tokenizer.ggml.pre\" entry in the GGUF file which we can\n",
            "        # use in llama.cpp to implement the same pre-tokenizer\n",
            "\n",
            "        chktxt = '\\n \\n\\n \\n\\n\\n \\t \\t\\t \\t\\n  \\n   \\n    \\n     \\n🚀 (normal) 😶\\u200d🌫️ (multiple emojis concatenated) ✅ 🦙🦙 3 33 333 3333 33333 333333 3333333 33333333 3.3 3..3 3...3 កាន់តែពិសេសអាច😁 ?我想在apple工作1314151天～ ------======= нещо на Български \\'\\'\\'\\'\\'\\'```````\"\"\"\"......!!!!!!?????? I\\'ve been \\'told he\\'s there, \\'RE you sure? \\'M not sure I\\'ll make it, \\'D you like some tea? We\\'Ve a\\'lL'\n",
            "\n",
            "        chktok = tokenizer.encode(chktxt)\n",
            "        chkhsh = sha256(str(chktok).encode()).hexdigest()\n",
            "\n",
            "        logger.debug(f\"chktok: {chktok}\")\n",
            "        logger.debug(f\"chkhsh: {chkhsh}\")\n",
            "\n",
            "        res = None\n",
            "\n",
            "        # NOTE: if you get an error here, you need to update the convert_hf_to_gguf_update.py script\n",
            "        #       or pull the latest version of the model from Huggingface\n",
            "        #       don't edit the hashes manually!\n",
            "        if chkhsh == \"0ef9807a4087ebef797fc749390439009c3b9eda9ad1a097abbe738f486c01e5\":\n",
            "            # ref: https://huggingface.co/meta-llama/Meta-Llama-3-8B\n",
            "            res = \"llama-bpe\"\n",
            "        if chkhsh == \"049ecf7629871e3041641907f3de7c733e4dbfdc736f57d882ba0b0845599754\":\n",
            "            # ref: https://huggingface.co/deepseek-ai/deepseek-llm-7b-base\n",
            "            res = \"deepseek-llm\"\n",
            "        if chkhsh == \"347715f544604f9118bb75ed199f68779f423cabb20db6de6f31b908d04d7821\":\n",
            "            # ref: https://huggingface.co/deepseek-ai/deepseek-coder-6.7b-base\n",
            "            res = \"deepseek-coder\"\n",
            "      \n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Chunk 27:\n",
            "7c733e4dbfdc736f57d882ba0b0845599754\":\n",
            "            # ref: https://huggingface.co/deepseek-ai/deepseek-llm-7b-base\n",
            "            res = \"deepseek-llm\"\n",
            "        if chkhsh == \"347715f544604f9118bb75ed199f68779f423cabb20db6de6f31b908d04d7821\":\n",
            "            # ref: https://huggingface.co/deepseek-ai/deepseek-coder-6.7b-base\n",
            "            res = \"deepseek-coder\"\n",
            "        if chkhsh == \"8aeee3860c56296a157a1fe2fad249ec40aa59b1bb5709f4ade11c4e6fe652ed\":\n",
            "            # ref: https://huggingface.co/tiiuae/falcon-7b\n",
            "            res = \"falcon\"\n",
            "        if chkhsh == \"0876d13b50744004aa9aeae05e7b0647eac9d801b5ba4668afc01e709c15e19f\":\n",
            "            # ref: https://huggingface.co/BAAI/bge-small-en-v1.5\n",
            "            res = \"bert-bge\"\n",
            "        if chkhsh == \"b6dc8df998e1cfbdc4eac8243701a65afe638679230920b50d6f17d81c098166\":\n",
            "            # ref: https://huggingface.co/mosaicml/mpt-7b\n",
            "            res = \"mpt\"\n",
            "        if chkhsh == \"35d91631860c815f952d711435f48d356ebac988362536bed955d43bfa436e34\":\n",
            "            # ref: https://huggingface.co/bigcode/starcoder2-3b\n",
            "            res = \"starcoder\"\n",
            "        if chkhsh == \"3ce83efda5659b07b1ad37ca97ca5797ea4285d9b9ab0dc679e4a720c9da7454\":\n",
            "            # ref: https://huggingface.co/openai-community/gpt2\n",
            "            res = \"gpt-2\"\n",
            "        if chkhsh == \"32d85c31273f8019248f2559fed492d929ea28b17e51d81d3bb36fff23ca72b3\":\n",
            "            # ref: https://huggingface.co/stabilityai/stablelm-2-zephyr-1_6b\n",
            "            res = \"stablelm2\"\n",
            "        if chkhsh == \"6221ad2852e85ce96f791f476e0b390cf9b474c9e3d1362f53a24a06dc8220ff\":\n",
            "            # ref: https://huggingface.co/smallcloudai/Refact-1_6-base\n",
            "            res = \"refact\"\n",
            "        if chkhsh == \"9c2227e4dd922002fb81bde4fc02b0483ca4f12911410dee2255e4987644e3f8\":\n",
            " \n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Chunk 28:\n",
            "abilityai/stablelm-2-zephyr-1_6b\n",
            "            res = \"stablelm2\"\n",
            "        if chkhsh == \"6221ad2852e85ce96f791f476e0b390cf9b474c9e3d1362f53a24a06dc8220ff\":\n",
            "            # ref: https://huggingface.co/smallcloudai/Refact-1_6-base\n",
            "            res = \"refact\"\n",
            "        if chkhsh == \"9c2227e4dd922002fb81bde4fc02b0483ca4f12911410dee2255e4987644e3f8\":\n",
            "            # ref: https://huggingface.co/CohereForAI/c4ai-command-r-v01\n",
            "            res = \"command-r\"\n",
            "        if chkhsh == \"e636dc30a262dcc0d8c323492e32ae2b70728f4df7dfe9737d9f920a282b8aea\":\n",
            "            # ref: https://huggingface.co/Qwen/Qwen1.5-7B\n",
            "            res = \"qwen2\"\n",
            "        if chkhsh == \"b6dc8df998e1cfbdc4eac8243701a65afe638679230920b50d6f17d81c098166\":\n",
            "            # ref: https://huggingface.co/allenai/OLMo-1.7-7B-hf\n",
            "            res = \"olmo\"\n",
            "        if chkhsh == \"a8594e3edff7c29c003940395316294b2c623e09894deebbc65f33f1515df79e\":\n",
            "            # ref: https://huggingface.co/databricks/dbrx-base\n",
            "            res = \"dbrx\"\n",
            "        if chkhsh == \"0876d13b50744004aa9aeae05e7b0647eac9d801b5ba4668afc01e709c15e19f\":\n",
            "            # ref: https://huggingface.co/jinaai/jina-embeddings-v2-base-en\n",
            "            res = \"jina-v2-en\"\n",
            "        if chkhsh == \"171aeeedd6fb548d418a7461d053f11b6f1f1fc9b387bd66640d28a4b9f5c643\":\n",
            "            # ref: https://huggingface.co/jinaai/jina-embeddings-v2-base-es\n",
            "            res = \"jina-v2-es\"\n",
            "        if chkhsh == \"27949a2493fc4a9f53f5b9b029c82689cfbe5d3a1929bb25e043089e28466de6\":\n",
            "            # ref: https://huggingface.co/jinaai/jina-embeddings-v2-base-de\n",
            "            res = \"jina-v2-de\"\n",
            "        if chkhsh == \"c136ed14d01c2745d4f60a9596ae66800e2b61fa45643e72436041855ad4089d\":\n",
            "           \n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Chunk 29:\n",
            "        res = \"jina-v2-es\"\n",
            "        if chkhsh == \"27949a2493fc4a9f53f5b9b029c82689cfbe5d3a1929bb25e043089e28466de6\":\n",
            "            # ref: https://huggingface.co/jinaai/jina-embeddings-v2-base-de\n",
            "            res = \"jina-v2-de\"\n",
            "        if chkhsh == \"c136ed14d01c2745d4f60a9596ae66800e2b61fa45643e72436041855ad4089d\":\n",
            "            # ref: https://huggingface.co/abacusai/Smaug-Llama-3-70B-Instruct\n",
            "            res = \"smaug-bpe\"\n",
            "        if chkhsh == \"c7ea5862a53e4272c035c8238367063e2b270d51faa48c0f09e9d5b54746c360\":\n",
            "            # ref: https://huggingface.co/LumiOpen/Poro-34B-chat\n",
            "            res = \"poro-chat\"\n",
            "        if chkhsh == \"7967bfa498ade6b757b064f31e964dddbb80f8f9a4d68d4ba7998fcf281c531a\":\n",
            "            # ref: https://huggingface.co/jinaai/jina-embeddings-v2-base-code\n",
            "            res = \"jina-v2-code\"\n",
            "        if chkhsh == \"b6e8e1518dc4305be2fe39c313ed643381c4da5db34a98f6a04c093f8afbe99b\":\n",
            "            # ref: https://huggingface.co/THUDM/glm-4-9b-chat\n",
            "            res = \"chatglm-bpe\"\n",
            "        if chkhsh == \"7fc505bd3104ca1083b150b17d088b59534ede9bde81f0dd2090967d7fe52cee\":\n",
            "            # ref: https://huggingface.co/LumiOpen/Viking-7B\n",
            "            res = \"viking\"\n",
            "        if chkhsh == \"b53802fb28e26d645c3a310b34bfe07da813026ec7c7716883404d5e0f8b1901\":\n",
            "            # ref: https://huggingface.co/core42/jais-13b\n",
            "            res = \"jais\"\n",
            "        if chkhsh == \"7b3e7548e4308f52a76e8229e4e6cc831195d0d1df43aed21ac6c93da05fec5f\":\n",
            "            # ref: https://huggingface.co/WisdomShell/CodeShell-7B\n",
            "            res = \"codeshell\"\n",
            "        if chkhsh == \"63b97e4253352e6f357cc59ea5b583e3a680eaeaf2632188c2b952de2588485e\":\n",
            "            # ref: https://huggingface.co/mistralai/Mistral-Nemo-Base-2407\n",
            " \n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Chunk 30:\n",
            "\"\n",
            "        if chkhsh == \"7b3e7548e4308f52a76e8229e4e6cc831195d0d1df43aed21ac6c93da05fec5f\":\n",
            "            # ref: https://huggingface.co/WisdomShell/CodeShell-7B\n",
            "            res = \"codeshell\"\n",
            "        if chkhsh == \"63b97e4253352e6f357cc59ea5b583e3a680eaeaf2632188c2b952de2588485e\":\n",
            "            # ref: https://huggingface.co/mistralai/Mistral-Nemo-Base-2407\n",
            "            res = \"tekken\"\n",
            "        if chkhsh == \"855059429035d75a914d1eda9f10a876752e281a054a7a3d421ef0533e5b6249\":\n",
            "            # ref: https://huggingface.co/HuggingFaceTB/SmolLM-135M\n",
            "            res = \"smollm\"\n",
            "\n",
            "        if res is None:\n",
            "            logger.warning(\"\\n\")\n",
            "            logger.warning(\"**************************************************************************************\")\n",
            "            logger.warning(\"** WARNING: The BPE pre-tokenizer was not recognized!\")\n",
            "            logger.warning(\"**          There are 2 possible reasons for this:\")\n",
            "            logger.warning(\"**          - the model has not been added to convert_hf_to_gguf_update.py yet\")\n",
            "            logger.warning(\"**          - the pre-tokenization config has changed upstream\")\n",
            "            logger.warning(\"**          Check your model files and convert_hf_to_gguf_update.py and update them accordingly.\")\n",
            "            logger.warning(\"** ref:     https://github.com/ggerganov/llama.cpp/pull/6920\")\n",
            "            logger.warning(\"**\")\n",
            "            logger.warning(f\"** chkhsh:  {chkhsh}\")\n",
            "            logger.warning(\"**************************************************************************************\")\n",
            "            logger.warning(\"\\n\")\n",
            "            raise NotImplementedError(\"BPE pre-tokenizer was not recognized - update get_vocab_base_pre()\")\n",
            "\n",
            "        logger.debug(f\"tokenizer.ggml.pre: {repr(res)}\")\n",
            "        logger.debug(f\"chkhsh: {chkhsh}\")\n",
            "\n",
            "        return res\n",
            "        # Marker: End get_vocab_base_pre\n",
            "\n",
            "    def _set_vocab_gpt2(self) -> None:\n",
            "        tokens, toktypes, tokpre = self.get_vocab_base()\n",
            "        self.gguf_writer.add_tokenizer_model(\"gpt2\")\n",
            "        self.gguf_writer.add_tokenizer_pre(tokpre)\n",
            "        self.gguf_writer.add_token_list(tokens)\n",
            "        self.gguf_writer.add_token_types(toktypes)\n",
            "\n",
            "        special_vocab = gguf\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def get_answer_with_retrieval(question):\n",
        "    # 검색 쿼리에 대한 임베딩 계산\n",
        "    query_embedding = embeddings.embed_query(question)\n",
        "\n",
        "    # query_embedding을 NumPy 배열로 변환\n",
        "    query_embedding = np.array(query_embedding).reshape(1, -1)\n",
        "\n",
        "    # FAISS에서 유사도 검색 (유사도 점수와 함께)\n",
        "    scores, indices = vector.index.search(query_embedding.reshape(1, -1), k=3)\n",
        "\n",
        "    # 검색된 문서(청크)와 유사도 점수를 함께 가져옵니다.\n",
        "    retrieved_docs_with_scores = [(chunks[i], scores[0][j]) for j, i in enumerate(indices[0])]\n",
        "\n",
        "    # 각 청크와 유사도 점수를 출력합니다.\n",
        "    for content, score in retrieved_docs_with_scores:\n",
        "        print(f\"Chunk: {content[:500]}...\")  # 청크 내용 일부 출력\n",
        "        print(f\"Similarity Score: {score}\")\n",
        "        print(\"-\" * 80)\n",
        "\n",
        "    # 검색된 내용을 텍스트로 결합합니다.\n",
        "    retrieved_content = \"\\n\".join([content for content, score in retrieved_docs_with_scores])\n",
        "\n",
        "    # 검색된 정보를 포함한 프롬프트로 LLM에 전달하여 답변을 생성합니다.\n",
        "    answer = qa_chain.run({\n",
        "        \"question\": question,\n",
        "        \"retrieved_content\": retrieved_content,\n",
        "    })\n",
        "\n",
        "    return answer\n",
        "\n",
        "question = \"GGMLFType에서 MOSTLY_F16 값이 뭘까\"\n",
        "answer = get_answer_with_retrieval(question)\n",
        "\n",
        "print(\"Question:\", question)\n",
        "print(\"Answer:\", answer)"
      ],
      "metadata": {
        "id": "DtrCK3oUjoRY",
        "outputId": "f7bee8fe-3a73-4393-895b-26590f7e847b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunk: #!/usr/bin/env python3\n",
            "from __future__ import annotations\n",
            "\n",
            "import logging\n",
            "import argparse\n",
            "import os\n",
            "import struct\n",
            "import sys\n",
            "from enum import IntEnum\n",
            "from pathlib import Path\n",
            "\n",
            "import numpy as np\n",
            "\n",
            "if 'NO_LOCAL_GGUF' not in os.environ:\n",
            "    sys.path.insert(1, str(Path(__file__).parent / 'gguf-py'))\n",
            "import gguf\n",
            "\n",
            "logger = logging.getLogger(\"ggml-to-gguf\")\n",
            "\n",
            "\n",
            "class GGMLFormat(IntEnum):\n",
            "    GGML = 0\n",
            "    GGMF = 1\n",
            "    GGJT = 2\n",
            "\n",
            "\n",
            "class GGMLFType(IntEnum):\n",
            "    ALL_F32              = 0\n",
            "    MOSTLY_F16           = 1\n",
            "    MOSTLY_Q4_0          = 2\n",
            "    MOSTLY_Q4_1          = 3\n",
            "    MOSTLY_Q4_1_SOME_F16 = 4\n",
            "    MOSTLY_Q8_0          = 7\n",
            "    MOSTLY_Q5_0          = 8\n",
            "    MOSTLY_Q5_1          = 9\n",
            "    MOSTLY_Q2_K          = 10\n",
            "    MOSTLY_Q3_K_S        = 11\n",
            "    MOSTLY_Q3_K_M        = 12\n",
            "    MOSTLY_Q3_K_L        = 13\n",
            "    MOSTLY_Q4_K_S        = 14\n",
            "    MOSTLY_Q4_K_M        = 15\n",
            "    MOSTLY_Q5_K_S        = 16\n",
            "    MOSTLY_Q5_K_M        = 17\n",
            "    MOSTLY_Q6_K          = 18\n",
            "\n",
            "\n",
            "class Hyperparameters:\n",
            "    def __init__(self):\n",
            "...\n",
            "Similarity Score: 0.5767102241516113\n",
            "--------------------------------------------------------------------------------\n",
            "Chunk: 32\n",
            "\n",
            "                # No override (data_qtype is False), or wants to be quantized (data_qtype is True)\n",
            "                if isinstance(data_qtype, bool):\n",
            "                    if self.ftype == gguf.LlamaFileType.ALL_F32:\n",
            "                        data_qtype = gguf.GGMLQuantizationType.F32\n",
            "                    elif self.ftype == gguf.LlamaFileType.MOSTLY_F16:\n",
            "                        data_qtype = gguf.GGMLQuantizationType.F16\n",
            "                    elif self.ftype == gguf.LlamaFileType.MOSTLY_BF16:\n",
            "                        data_qtype = gguf.GGMLQuantizationType.BF16\n",
            "                    elif self.ftype == gguf.LlamaFileType.MOSTLY_Q8_0:\n",
            "                        data_qtype = gguf.GGMLQuantizationType.Q8_0\n",
            "                    else:\n",
            "                        raise ValueError(f\"Unknown file type: {self.ftype.name}\")\n",
            "\n",
            "                try:\n",
            "                    data = gguf.quants.quantize(data, data_qtype)\n",
            "                except gguf.QuantError as e:\n",
            "                    logger.warning(\"%s, %s\", e, \"falling bac...\n",
            "Similarity Score: 0.6540786027908325\n",
            "--------------------------------------------------------------------------------\n",
            "Chunk: #!/usr/bin/env python3\n",
            "# -*- coding: utf-8 -*-\n",
            "\n",
            "from __future__ import annotations\n",
            "\n",
            "import logging\n",
            "import argparse\n",
            "import contextlib\n",
            "import json\n",
            "import os\n",
            "import re\n",
            "import sys\n",
            "from enum import IntEnum\n",
            "from pathlib import Path\n",
            "from hashlib import sha256\n",
            "from typing import TYPE_CHECKING, Any, Callable, ContextManager, Iterable, Iterator, Literal, Sequence, TypeVar, cast\n",
            "\n",
            "import math\n",
            "import numpy as np\n",
            "import torch\n",
            "\n",
            "if TYPE_CHECKING:\n",
            "    from torch import Tensor\n",
            "\n",
            "if 'NO_LOCAL_GGUF' not in os.environ:\n",
            "    sys.path.insert(1, str(Path(__file__).parent / 'gguf-py'))\n",
            "import gguf\n",
            "\n",
            "logger = logging.getLogger(\"hf-to-gguf\")\n",
            "\n",
            "\n",
            "###### MODEL DEFINITIONS ######\n",
            "\n",
            "class SentencePieceTokenTypes(IntEnum):\n",
            "    NORMAL = 1\n",
            "    UNKNOWN = 2\n",
            "    CONTROL = 3\n",
            "    USER_DEFINED = 4\n",
            "    UNUSED = 5\n",
            "    BYTE = 6\n",
            "\n",
            "\n",
            "AnyModel = TypeVar(\"AnyModel\", bound=\"type[Model]\")\n",
            "\n",
            "\n",
            "class Model:\n",
            "    _model_classes: dict[str, type[Model]] = {}\n",
            "\n",
            "    dir_model: Path\n",
            "    ftype: gguf.LlamaFileType\n",
            "    fname_out: Path\n",
            "    is_big_endian: bo...\n",
            "Similarity Score: 0.6553962826728821\n",
            "--------------------------------------------------------------------------------\n",
            "Question: GGMLFType에서 MOSTLY_F16 값이 뭘까\n",
            "Answer: MOSTLY_F16 값은 GGMLFType 열거형에서 1에 해당합니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x9WXpsYbNmIE"
      },
      "execution_count": 9,
      "outputs": []
    }
  ]
}