{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/donghuna/AI-Expert/blob/main/System/%EC%9C%A0%EC%8A%B9%EC%A3%BC/BYOL_Practice_Answer_2024.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bC3TkQ3gJbBP"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchvision.datasets as datasets\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import os\n",
        "\n",
        "import copy\n",
        "\n",
        "import time"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvdDhbPFJbBR"
      },
      "source": [
        "### Step 1. Design BYOL Model & Loss\n",
        "\n",
        "#### Implementation 1-1. Create modules\n",
        "\n",
        "#### Implementation 1-2. Design BYOL loss function\n",
        "\n",
        "#### Implementation 1-3. Design forward function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ftel14QJbBR"
      },
      "source": [
        "import math\n",
        "from torchvision.models.resnet import conv3x3\n",
        "\n",
        "\n",
        "# ResNet 구현부분\n",
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, inplanes, planes, norm_layer, stride=1, downsample=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "        self.bn1 = norm_layer(inplanes)\n",
        "        self.relu1 = nn.ReLU(inplace=True)\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "\n",
        "        self.bn2 = norm_layer(planes)\n",
        "        self.relu2 = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        residual = self.bn1(residual)\n",
        "        residual = self.relu1(residual)\n",
        "        residual = self.conv1(residual)\n",
        "\n",
        "        residual = self.bn2(residual)\n",
        "        residual = self.relu2(residual)\n",
        "        residual = self.conv2(residual)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            x = self.downsample(x)\n",
        "        return x + residual\n",
        "\n",
        "\n",
        "class Downsample(nn.Module):\n",
        "    def __init__(self, nIn, nOut, stride):\n",
        "        super(Downsample, self).__init__()\n",
        "        self.avg = nn.AvgPool2d(stride)\n",
        "        assert nOut % nIn == 0\n",
        "        self.expand_ratio = nOut // nIn\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.avg(x)\n",
        "        return torch.cat([x] + [x.mul(0)] * (self.expand_ratio - 1), 1)\n",
        "\n",
        "\n",
        "class ResNetCifar(nn.Module):\n",
        "    def __init__(self, depth=26, width=1, channels=3, norm_layer=nn.BatchNorm2d):\n",
        "        assert (depth - 2) % 6 == 0         # depth is 6N+2\n",
        "        self.N = (depth - 2) // 6\n",
        "        super(ResNetCifar, self).__init__()\n",
        "\n",
        "        # Following the Wide ResNet convention, we fix the very first convolution\n",
        "        self.conv1 = nn.Conv2d(channels, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.inplanes = 16\n",
        "        self.layer1 = self._make_layer(norm_layer, 16 * width)\n",
        "        self.layer2 = self._make_layer(norm_layer, 32 * width, stride=2)\n",
        "        self.layer3 = self._make_layer(norm_layer, 64 * width, stride=2)\n",
        "        self.bn = norm_layer(64 * width)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.avgpool = nn.AvgPool2d(8)\n",
        "\n",
        "        # Initialization\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "\n",
        "    def _make_layer(self, norm_layer, planes, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes:\n",
        "            downsample = Downsample(self.inplanes, planes, stride)\n",
        "        layers = [BasicBlock(self.inplanes, planes, norm_layer, stride, downsample)]\n",
        "        self.inplanes = planes\n",
        "        for i in range(self.N - 1):\n",
        "            layers.append(BasicBlock(self.inplanes, planes, norm_layer))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return x\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, idim, hdim, odim, width=1.):\n",
        "        super().__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(idim * width, hdim * width)\n",
        "        self.bn1 = nn.BatchNorm1d(hdim * width)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hdim * width, odim * width)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class BYOLNet(nn.Module):\n",
        "    def __init__(self,\n",
        "                 width=1,\n",
        "                 feat_dim=64,\n",
        "                 hidden_dim=128,\n",
        "                 byol_feat_dim=32,\n",
        "                 num_classes=10):\n",
        "        super().__init__()\n",
        "\n",
        "        ### IMPLEMENTATION 1-1 ###\n",
        "        # f: convolutional feature encoder, use ResNetCifar class above\n",
        "        # g: projection MLP, use MLP class above\n",
        "        # q: prediction MLP, use MLP class above\n",
        "        # h: linear classifier, use nn.Linear class\n",
        "        self.f = ResNetCifar(width=width)\n",
        "        self.g = MLP(idim=feat_dim, hdim=hidden_dim, odim=byol_feat_dim, width=width)\n",
        "        self.q = MLP(idim=byol_feat_dim, hdim=hidden_dim, odim=byol_feat_dim, width=width)\n",
        "        self.h = nn.Linear(feat_dim, num_classes)\n",
        "        ### IMPLEMENTATION ENDS HERE ###\n",
        "\n",
        "        self.online_net = nn.Sequential(self.f, self.g, self.q)\n",
        "\n",
        "        self.f_target = copy.deepcopy(self.f)\n",
        "        self.g_target = copy.deepcopy(self.g)\n",
        "\n",
        "        self.target_net = nn.Sequential(self.f_target, self.g_target)\n",
        "\n",
        "        for p in self.target_net.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "    def byol_loss(self, pred, proj):\n",
        "        ### IMPLEMENTATION 1-2 ###\n",
        "        # pred: prediction vectors from online net\n",
        "        # proj: projection vectors from target net (must not have grad)\n",
        "        # return the loss values of shape (batch, 1)\n",
        "        # Note that the last dimension should contain the value of dot product\n",
        "        pred = F.normalize(pred, dim=-1)\n",
        "        proj = F.normalize(proj, dim=-1)\n",
        "        return -2. * (pred * proj).sum(dim=-1)\n",
        "        ### IMPLEMENTATION ENDS HERE ###\n",
        "\n",
        "    def byol_forward(self, x1, x2):\n",
        "        ### IMPLEMENTATION 1-3 ###\n",
        "        # x1: tensor containing input image (view #1)\n",
        "        # x2: tensor containing input image (view #2)\n",
        "        # Compute 2 loss values using self.byol_loss method twice\n",
        "        pred1 = self.online_net(x1)\n",
        "        pred2 = self.online_net(x2)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            proj1 = self.target_net(x2)\n",
        "            proj2 = self.target_net(x1)\n",
        "\n",
        "        loss1 = self.byol_loss(pred1, proj1)\n",
        "        loss2 = self.byol_loss(pred2, proj2)\n",
        "\n",
        "        ### IMPLEMENTATION ENDS HERE ###\n",
        "        loss = (loss1 + loss2).mean()\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def update_target_net(self, decay):\n",
        "        for p_online, p_target in zip(self.online_net.parameters(), self.target_net.parameters()):\n",
        "            p_target.data = p_target.data * decay + p_online.data * (1 - decay)\n",
        "\n",
        "    def finetune_forward(self, x):\n",
        "        return self.h(self.f(x))"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-ZCWWJ5JbBT"
      },
      "source": [
        "### Step 2. Prepare datasets & data augmentations\n",
        "\n",
        "For contrastaive learning, a set of random augmentation functions is defined.\n",
        "\n",
        "Then, the set is applied twice to each image, which is implemented as in provided DoubleCompose module.\n",
        "\n",
        "https://pytorch.org/docs/stable/torchvision/transforms.html\n",
        "\n",
        "Refer to the torchvision.transforms documentation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5kHhEH-JbBU"
      },
      "source": [
        "class DoubleCompose(object):\n",
        "    def __init__(self, trf1, trf2):\n",
        "        self.trf1 = trf1\n",
        "        self.trf2 = trf2\n",
        "\n",
        "    def __call__(self, img):\n",
        "        img1 = img.copy()\n",
        "        img2 = img.copy()\n",
        "        for t1 in self.trf1:\n",
        "            img1 = t1(img1)\n",
        "        for t2 in self.trf2:\n",
        "            img2 = t2(img2)\n",
        "        return img1, img2\n",
        "\n",
        "import cv2\n",
        "cv2.setNumThreads(0)\n",
        "from PIL import Image\n",
        "\n",
        "class GaussianBlur(object):\n",
        "    # Implements Gaussian blur as described in the SimCLR paper\n",
        "    def __init__(self, kernel_size, min=0.1, max=2.0, p=1.0):\n",
        "        self.min = min\n",
        "        self.max = max\n",
        "        # kernel size is set to be 10% of the image height/width\n",
        "        self.kernel_size = kernel_size\n",
        "\n",
        "        if self.kernel_size % 2 == 0:\n",
        "            self.kernel_size += 1\n",
        "\n",
        "        self.p = p\n",
        "\n",
        "    def __call__(self, sample,):\n",
        "        sample = np.array(sample)\n",
        "\n",
        "        prob = np.random.random_sample()\n",
        "\n",
        "        if prob < self.p:\n",
        "            sigma = (self.max - self.min) * np.random.random_sample() + self.min\n",
        "            sample = cv2.GaussianBlur(sample, (self.kernel_size, self.kernel_size), sigma)\n",
        "\n",
        "        return Image.fromarray(sample.astype(np.uint8))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUWa_UJnJbBU"
      },
      "source": [
        "import torchvision.transforms as transforms\n",
        "\n",
        "img_size = (32, 32)\n",
        "\n",
        "color_jitter = transforms.ColorJitter(0.4, 0.4, 0.2, 0.1)\n",
        "\n",
        "transform1 = [\n",
        "    transforms.RandomResizedCrop(size=img_size),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomApply([color_jitter], p=0.8),\n",
        "    transforms.RandomGrayscale(p=0.2),\n",
        "    GaussianBlur(3, p=1.0),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "]\n",
        "\n",
        "transform2 = [\n",
        "    transforms.RandomResizedCrop(size=img_size),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomApply([color_jitter], p=0.8),\n",
        "    transforms.RandomGrayscale(p=0.2),\n",
        "    GaussianBlur(3, p=0.1),\n",
        "    transforms.RandomSolarize(5, p=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "]\n",
        "\n",
        "train_transform = DoubleCompose(transform1, transform2)\n",
        "\n",
        "finetune_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5txTPQjntcL"
      },
      "source": [
        "Note that we should make three dataloaders:\n",
        "\n",
        "1. Pre-training loader with two heterogeneous data augmentations\n",
        "2. Fine-tuning loader with one basic or no data augmentation\n",
        "3. Test loader with no data augmentation\n",
        "\n",
        "And also note that we should always contain **transforms.ToTensor()** to make sure that the input values are normalized into the range [0, 1]. Dataset-specific normalization (whitening) is recommended, but not mandatory (it is good to testify its effectiveness as an ablation)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OTIHLlI1JbBV",
        "outputId": "9f2cb1da-f4f2-426a-e6bd-46b72afdaea2"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataset = datasets.CIFAR10(root='.',\n",
        "                                 train=True,\n",
        "                                 download=True,\n",
        "                                 transform=train_transform\n",
        "                                )\n",
        "\n",
        "finetune_dataset = datasets.CIFAR10(root='.',\n",
        "                                 train=True,\n",
        "                                 download=True,\n",
        "                                 transform=finetune_transform\n",
        "                                )\n",
        "\n",
        "test_dataset = datasets.CIFAR10(root='.',\n",
        "                                 train=False,\n",
        "                                 download=True,\n",
        "                                 transform=test_transform\n",
        "                                )\n",
        "\n",
        "train_loader = DataLoader(train_dataset,\n",
        "                          batch_size=256,\n",
        "                          num_workers=4,\n",
        "                          shuffle=True,\n",
        "                          drop_last=True\n",
        "                         )\n",
        "\n",
        "finetune_loader = DataLoader(finetune_dataset,\n",
        "                          batch_size=256,\n",
        "                          num_workers=4,\n",
        "                          shuffle=True,\n",
        "                          drop_last=True\n",
        "                         )\n",
        "\n",
        "test_loader = DataLoader(finetune_dataset,\n",
        "                          batch_size=256,\n",
        "                          num_workers=4,\n",
        "                          shuffle=False,\n",
        "                          drop_last=False\n",
        "                         )"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:18<00:00, 9066057.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./cifar-10-python.tar.gz to .\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BU5_4d7BJbBW"
      },
      "source": [
        "### Step 3. Run pre-training step"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCaPUWVLprMr"
      },
      "source": [
        "import math\n",
        "def get_decay_value(base_decay, global_iter, total_iter):\n",
        "    return 1 - (1 - base_decay) * (math.cos(math.pi * global_iter / total_iter) + 1)/2"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GATvSb8JbBW"
      },
      "source": [
        "def train(net, loader):\n",
        "\n",
        "    optimizer = torch.optim.Adam(net.online_net.parameters(), 3e-4)\n",
        "\n",
        "    train_start = time.time()\n",
        "\n",
        "    global_iter = 0\n",
        "    total_iter = 100 * len(loader)\n",
        "\n",
        "    for epoch in range(1, 100 + 1):\n",
        "\n",
        "        train_loss = 0\n",
        "        net.train()\n",
        "\n",
        "        epoch_start = time.time()\n",
        "        for idx, (data, target) in enumerate(loader):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            xi, xj = data[0].cuda(), data[1].cuda()\n",
        "\n",
        "            loss = net.byol_forward(xi, xj)\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            net.update_target_net(get_decay_value(0.996, global_iter, total_iter))\n",
        "            global_iter += 1\n",
        "\n",
        "        train_loss /= (idx + 1)\n",
        "\n",
        "        epoch_time = time.time() - epoch_start\n",
        "        print(\"Epoch\\t\", epoch,\n",
        "              \"\\tLoss\\t\", train_loss,\n",
        "              \"\\tTime\\t\", epoch_time,\n",
        "             )\n",
        "\n",
        "    elapsed_train_time = time.time() - train_start\n",
        "    print('Finished training. Train time was:', elapsed_train_time)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DeVkjz5MJbBW"
      },
      "source": [
        "GPU_NUM = '0'\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = GPU_NUM\n",
        "\n",
        "net = BYOLNet()\n",
        "\n",
        "net = net.cuda()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4geMfDbuJbBW",
        "outputId": "a49f4676-0ec1-4f8c-b34a-8ca3152c175f"
      },
      "source": [
        "train(net, train_loader)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch\t 1 \tLoss\t -2.4424696032817548 \tTime\t 30.787409782409668\n",
            "Epoch\t 2 \tLoss\t -2.8478147567846834 \tTime\t 29.37113380432129\n",
            "Epoch\t 3 \tLoss\t -2.9064274188799737 \tTime\t 28.685027360916138\n",
            "Epoch\t 4 \tLoss\t -2.904709793971135 \tTime\t 29.525988340377808\n",
            "Epoch\t 5 \tLoss\t -2.899431627224653 \tTime\t 28.842569828033447\n",
            "Epoch\t 6 \tLoss\t -2.8980483348552997 \tTime\t 29.084099769592285\n",
            "Epoch\t 7 \tLoss\t -2.901091133019863 \tTime\t 29.31484818458557\n",
            "Epoch\t 8 \tLoss\t -2.8955409881396172 \tTime\t 29.567991495132446\n",
            "Epoch\t 9 \tLoss\t -2.9044429497841078 \tTime\t 29.337963819503784\n",
            "Epoch\t 10 \tLoss\t -2.9256034753261466 \tTime\t 29.42161464691162\n",
            "Epoch\t 11 \tLoss\t -2.946738653916579 \tTime\t 29.062496662139893\n",
            "Epoch\t 12 \tLoss\t -2.952676546879304 \tTime\t 29.297667980194092\n",
            "Epoch\t 13 \tLoss\t -2.974532528412648 \tTime\t 28.971834182739258\n",
            "Epoch\t 14 \tLoss\t -2.9799391159644495 \tTime\t 29.17529010772705\n",
            "Epoch\t 15 \tLoss\t -2.983479491258279 \tTime\t 28.95585584640503\n",
            "Epoch\t 16 \tLoss\t -2.9893240843063746 \tTime\t 29.5987765789032\n",
            "Epoch\t 17 \tLoss\t -3.005087529695951 \tTime\t 29.492771863937378\n",
            "Epoch\t 18 \tLoss\t -3.0150295942257612 \tTime\t 29.381589889526367\n",
            "Epoch\t 19 \tLoss\t -3.0117676502619033 \tTime\t 29.865895748138428\n",
            "Epoch\t 20 \tLoss\t -3.0072638719509808 \tTime\t 29.61159348487854\n",
            "Epoch\t 21 \tLoss\t -3.006859099559295 \tTime\t 29.47665524482727\n",
            "Epoch\t 22 \tLoss\t -3.0033810725578896 \tTime\t 29.21584177017212\n",
            "Epoch\t 23 \tLoss\t -3.0161831427843144 \tTime\t 29.203837871551514\n",
            "Epoch\t 24 \tLoss\t -3.0174983049050357 \tTime\t 29.032888174057007\n",
            "Epoch\t 25 \tLoss\t -3.017142294614743 \tTime\t 29.31704020500183\n",
            "Epoch\t 26 \tLoss\t -3.025528761056753 \tTime\t 29.226581811904907\n",
            "Epoch\t 27 \tLoss\t -3.0446146671588603 \tTime\t 29.34615421295166\n",
            "Epoch\t 28 \tLoss\t -3.0417619705200196 \tTime\t 29.252073526382446\n",
            "Epoch\t 29 \tLoss\t -3.0531161381648135 \tTime\t 28.75487470626831\n",
            "Epoch\t 30 \tLoss\t -3.0648832174447866 \tTime\t 29.325634002685547\n",
            "Epoch\t 31 \tLoss\t -3.063778131436079 \tTime\t 29.21469759941101\n",
            "Epoch\t 32 \tLoss\t -3.0661055846092027 \tTime\t 29.241656064987183\n",
            "Epoch\t 33 \tLoss\t -3.079262795815101 \tTime\t 29.02796721458435\n",
            "Epoch\t 34 \tLoss\t -3.0873555770287147 \tTime\t 29.060604333877563\n",
            "Epoch\t 35 \tLoss\t -3.085662915156438 \tTime\t 29.61641788482666\n",
            "Epoch\t 36 \tLoss\t -3.0916928890423896 \tTime\t 28.857801914215088\n",
            "Epoch\t 37 \tLoss\t -3.0968278273558005 \tTime\t 28.966022491455078\n",
            "Epoch\t 38 \tLoss\t -3.103350295775976 \tTime\t 28.83983874320984\n",
            "Epoch\t 39 \tLoss\t -3.1092349064655793 \tTime\t 28.85136842727661\n",
            "Epoch\t 40 \tLoss\t -3.1208998264410557 \tTime\t 29.016815185546875\n",
            "Epoch\t 41 \tLoss\t -3.1234543384649816 \tTime\t 29.07586407661438\n",
            "Epoch\t 42 \tLoss\t -3.1283147873022616 \tTime\t 28.924338579177856\n",
            "Epoch\t 43 \tLoss\t -3.1428913446573112 \tTime\t 29.368977785110474\n",
            "Epoch\t 44 \tLoss\t -3.1410614392696283 \tTime\t 28.80825400352478\n",
            "Epoch\t 45 \tLoss\t -3.1433738158299374 \tTime\t 29.113545656204224\n",
            "Epoch\t 46 \tLoss\t -3.1478589094602145 \tTime\t 28.976959943771362\n",
            "Epoch\t 47 \tLoss\t -3.146406056330754 \tTime\t 28.87235975265503\n",
            "Epoch\t 48 \tLoss\t -3.1490138714130107 \tTime\t 28.778828382492065\n",
            "Epoch\t 49 \tLoss\t -3.1531741557977138 \tTime\t 29.36391282081604\n",
            "Epoch\t 50 \tLoss\t -3.155460274525178 \tTime\t 28.85983443260193\n",
            "Epoch\t 51 \tLoss\t -3.1619315122946716 \tTime\t 28.809446096420288\n",
            "Epoch\t 52 \tLoss\t -3.168915167832986 \tTime\t 29.015648365020752\n",
            "Epoch\t 53 \tLoss\t -3.170955244700114 \tTime\t 29.231488466262817\n",
            "Epoch\t 54 \tLoss\t -3.173839270762908 \tTime\t 29.274321794509888\n",
            "Epoch\t 55 \tLoss\t -3.181026292458559 \tTime\t 29.257960081100464\n",
            "Epoch\t 56 \tLoss\t -3.180057153946314 \tTime\t 28.888202667236328\n",
            "Epoch\t 57 \tLoss\t -3.1794312367072473 \tTime\t 29.616448402404785\n",
            "Epoch\t 58 \tLoss\t -3.1840826438023493 \tTime\t 28.89540386199951\n",
            "Epoch\t 59 \tLoss\t -3.186266204638359 \tTime\t 28.890623092651367\n",
            "Epoch\t 60 \tLoss\t -3.1833043636419833 \tTime\t 29.087806224822998\n",
            "Epoch\t 61 \tLoss\t -3.192523231261816 \tTime\t 28.842787265777588\n",
            "Epoch\t 62 \tLoss\t -3.1952746158991103 \tTime\t 29.88346266746521\n",
            "Epoch\t 63 \tLoss\t -3.2005026621696278 \tTime\t 29.443854093551636\n",
            "Epoch\t 64 \tLoss\t -3.195941464106242 \tTime\t 29.493630170822144\n",
            "Epoch\t 65 \tLoss\t -3.201621460303282 \tTime\t 29.237684965133667\n",
            "Epoch\t 66 \tLoss\t -3.1979700724283853 \tTime\t 29.11578392982483\n",
            "Epoch\t 67 \tLoss\t -3.207871251228528 \tTime\t 29.221340894699097\n",
            "Epoch\t 68 \tLoss\t -3.206588592284765 \tTime\t 29.563268423080444\n",
            "Epoch\t 69 \tLoss\t -3.206639025761531 \tTime\t 29.080476760864258\n",
            "Epoch\t 70 \tLoss\t -3.209820710695707 \tTime\t 29.158894777297974\n",
            "Epoch\t 71 \tLoss\t -3.209775776740832 \tTime\t 29.05147123336792\n",
            "Epoch\t 72 \tLoss\t -3.2121579060187706 \tTime\t 29.28182101249695\n",
            "Epoch\t 73 \tLoss\t -3.216433183963482 \tTime\t 28.98388123512268\n",
            "Epoch\t 74 \tLoss\t -3.2137531794034517 \tTime\t 29.25724744796753\n",
            "Epoch\t 75 \tLoss\t -3.211817660698524 \tTime\t 29.153923273086548\n",
            "Epoch\t 76 \tLoss\t -3.2150810877482097 \tTime\t 29.307141065597534\n",
            "Epoch\t 77 \tLoss\t -3.2107606093088785 \tTime\t 29.095253944396973\n",
            "Epoch\t 78 \tLoss\t -3.217387395027356 \tTime\t 29.532509326934814\n",
            "Epoch\t 79 \tLoss\t -3.2177998921810054 \tTime\t 29.138801097869873\n",
            "Epoch\t 80 \tLoss\t -3.2189953559484237 \tTime\t 29.159013032913208\n",
            "Epoch\t 81 \tLoss\t -3.2247082967024583 \tTime\t 29.401424884796143\n",
            "Epoch\t 82 \tLoss\t -3.2241340881738907 \tTime\t 29.56339144706726\n",
            "Epoch\t 83 \tLoss\t -3.2235157978840365 \tTime\t 29.421677350997925\n",
            "Epoch\t 84 \tLoss\t -3.2279926960284895 \tTime\t 29.439046382904053\n",
            "Epoch\t 85 \tLoss\t -3.2247493719443296 \tTime\t 29.091808319091797\n",
            "Epoch\t 86 \tLoss\t -3.221001986968212 \tTime\t 29.375101804733276\n",
            "Epoch\t 87 \tLoss\t -3.2256890235803066 \tTime\t 29.17115831375122\n",
            "Epoch\t 88 \tLoss\t -3.230095267907167 \tTime\t 29.583224534988403\n",
            "Epoch\t 89 \tLoss\t -3.2288943315163636 \tTime\t 28.906543970108032\n",
            "Epoch\t 90 \tLoss\t -3.226216372465476 \tTime\t 29.40376353263855\n",
            "Epoch\t 91 \tLoss\t -3.233029076991937 \tTime\t 29.386263132095337\n",
            "Epoch\t 92 \tLoss\t -3.2279919624328612 \tTime\t 29.422788381576538\n",
            "Epoch\t 93 \tLoss\t -3.2305625206384905 \tTime\t 29.294666290283203\n",
            "Epoch\t 94 \tLoss\t -3.232698430770483 \tTime\t 29.762504816055298\n",
            "Epoch\t 95 \tLoss\t -3.2329153476617276 \tTime\t 29.480405569076538\n",
            "Epoch\t 96 \tLoss\t -3.237797518265553 \tTime\t 30.019385814666748\n",
            "Epoch\t 97 \tLoss\t -3.2338838723989634 \tTime\t 29.179864406585693\n",
            "Epoch\t 98 \tLoss\t -3.2378825309949044 \tTime\t 29.333024978637695\n",
            "Epoch\t 99 \tLoss\t -3.2327104837466507 \tTime\t 29.338449954986572\n",
            "Epoch\t 100 \tLoss\t -3.238590112099281 \tTime\t 29.789788961410522\n",
            "Finished training. Train time was: 2924.91828083992\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Y5bvN78pYXo"
      },
      "source": [
        "### Step 4. Run fine-tuning step"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GGJQMMfJbBX"
      },
      "source": [
        "def finetune(net, loader, test_loader):\n",
        "\n",
        "    ### IMPORTANT ###\n",
        "    # When fine-tuning your network, all parameters except the linear classifier must be frozen\n",
        "    # f, g, and q in the BYOLNet instance will not be updated\n",
        "    for p in net.online_net.parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "    # To make sure, pass only the classifier parameters to optimizer\n",
        "    params = list(net.h.parameters())\n",
        "    optimizer = torch.optim.Adam(params, 3e-4)\n",
        "\n",
        "    train_start = time.time()\n",
        "\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    global_iter = 0\n",
        "    total_iter = 100 * len(loader)\n",
        "\n",
        "    for epoch in range(1, 100 + 1):\n",
        "\n",
        "        train_loss = 0\n",
        "        net.train()\n",
        "        net.f.eval() # batch 통계를 쌓는 학습도 못하게끔 한다. reauires_grad를 false로 했지만 추가적으로 해야되는듯\n",
        "        net.g.eval()\n",
        "        net.q.eval()\n",
        "        epoch_start = time.time()\n",
        "        for idx, (data, target) in enumerate(loader):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            x, target = data.cuda(), target.cuda()\n",
        "\n",
        "            y = net.finetune_forward(x)\n",
        "            # Or use this\n",
        "            # y = net.finetune_forward_no_grad(x)\n",
        "\n",
        "            loss = loss_fn(y, target)\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            global_iter += 1\n",
        "\n",
        "        train_loss /= (idx + 1)\n",
        "\n",
        "        epoch_time = time.time() - epoch_start\n",
        "        print(\"Epoch\\t\", epoch,\n",
        "              \"\\tLoss\\t\", train_loss,\n",
        "              \"\\tTime\\t\", epoch_time,\n",
        "              \"\\tAcc.\", test(net, test_loader),\n",
        "             )\n",
        "\n",
        "    elapsed_train_time = time.time() - train_start\n",
        "    print('Finished training. Train time was:', elapsed_train_time)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQ4aJAKptUpI"
      },
      "source": [
        "def test(net, test_loader):\n",
        "    net.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for idx, (data, target) in enumerate(test_loader):\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "\n",
        "            y = net.finetune_forward(data)\n",
        "\n",
        "            correct += (y.argmax(1) == target).sum().item()\n",
        "            total += target.size(0)\n",
        "\n",
        "    return correct / total"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jiFJLykCt6lx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3d85071-3545-44cd-dc20-a6c3c77b42e6"
      },
      "source": [
        "finetune(net, finetune_loader, test_loader)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch\t 1 \tLoss\t 2.2328192918728558 \tTime\t 3.480626106262207 \tAcc. 0.31688\n",
            "Epoch\t 2 \tLoss\t 2.031244943080804 \tTime\t 3.5269806385040283 \tAcc. 0.40492\n",
            "Epoch\t 3 \tLoss\t 1.8883617645654922 \tTime\t 3.4910523891448975 \tAcc. 0.44382\n",
            "Epoch\t 4 \tLoss\t 1.7817719685725677 \tTime\t 3.679769992828369 \tAcc. 0.46428\n",
            "Epoch\t 5 \tLoss\t 1.7005156419216059 \tTime\t 3.3515536785125732 \tAcc. 0.47822\n",
            "Epoch\t 6 \tLoss\t 1.636566051458701 \tTime\t 3.615812063217163 \tAcc. 0.49102\n",
            "Epoch\t 7 \tLoss\t 1.5849179335129566 \tTime\t 3.928023338317871 \tAcc. 0.49926\n",
            "Epoch\t 8 \tLoss\t 1.542242597310971 \tTime\t 3.803346633911133 \tAcc. 0.5061\n",
            "Epoch\t 9 \tLoss\t 1.506787805679517 \tTime\t 3.5196211338043213 \tAcc. 0.51242\n",
            "Epoch\t 10 \tLoss\t 1.4767181451504048 \tTime\t 3.4133498668670654 \tAcc. 0.51494\n",
            "Epoch\t 11 \tLoss\t 1.4508566893064059 \tTime\t 3.496912717819214 \tAcc. 0.52068\n",
            "Epoch\t 12 \tLoss\t 1.4282485637909328 \tTime\t 3.5577592849731445 \tAcc. 0.5226\n",
            "Epoch\t 13 \tLoss\t 1.4083687329903627 \tTime\t 3.5265772342681885 \tAcc. 0.52704\n",
            "Epoch\t 14 \tLoss\t 1.390850370969528 \tTime\t 3.6110880374908447 \tAcc. 0.52996\n",
            "Epoch\t 15 \tLoss\t 1.3750781695048013 \tTime\t 3.4786040782928467 \tAcc. 0.5332\n",
            "Epoch\t 16 \tLoss\t 1.3613058334741837 \tTime\t 3.446439266204834 \tAcc. 0.53654\n",
            "Epoch\t 17 \tLoss\t 1.3487954769379054 \tTime\t 3.555790424346924 \tAcc. 0.53814\n",
            "Epoch\t 18 \tLoss\t 1.3371492850474822 \tTime\t 3.453209400177002 \tAcc. 0.54194\n",
            "Epoch\t 19 \tLoss\t 1.3269010085325974 \tTime\t 3.6775569915771484 \tAcc. 0.54296\n",
            "Epoch\t 20 \tLoss\t 1.317298382979173 \tTime\t 3.3788979053497314 \tAcc. 0.54368\n",
            "Epoch\t 21 \tLoss\t 1.3084468609247453 \tTime\t 3.4414825439453125 \tAcc. 0.54732\n",
            "Epoch\t 22 \tLoss\t 1.3005591789881388 \tTime\t 3.4843173027038574 \tAcc. 0.54844\n",
            "Epoch\t 23 \tLoss\t 1.2931908723635552 \tTime\t 3.4146645069122314 \tAcc. 0.54982\n",
            "Epoch\t 24 \tLoss\t 1.286281730578496 \tTime\t 3.599743604660034 \tAcc. 0.55264\n",
            "Epoch\t 25 \tLoss\t 1.2798169233860113 \tTime\t 3.481801986694336 \tAcc. 0.5546\n",
            "Epoch\t 26 \tLoss\t 1.2738888049737 \tTime\t 3.4532454013824463 \tAcc. 0.55482\n",
            "Epoch\t 27 \tLoss\t 1.2681151707967122 \tTime\t 3.545444965362549 \tAcc. 0.5564\n",
            "Epoch\t 28 \tLoss\t 1.2627051414587558 \tTime\t 3.4558186531066895 \tAcc. 0.55892\n",
            "Epoch\t 29 \tLoss\t 1.2578764891013121 \tTime\t 3.6654491424560547 \tAcc. 0.55926\n",
            "Epoch\t 30 \tLoss\t 1.2534286278944748 \tTime\t 3.4723997116088867 \tAcc. 0.5612\n",
            "Epoch\t 31 \tLoss\t 1.2484035883194362 \tTime\t 3.402604818344116 \tAcc. 0.56212\n",
            "Epoch\t 32 \tLoss\t 1.244455741613339 \tTime\t 3.5346031188964844 \tAcc. 0.56264\n",
            "Epoch\t 33 \tLoss\t 1.240615451030242 \tTime\t 3.409575939178467 \tAcc. 0.56388\n",
            "Epoch\t 34 \tLoss\t 1.2368528158236773 \tTime\t 3.6458358764648438 \tAcc. 0.56448\n",
            "Epoch\t 35 \tLoss\t 1.23335909476647 \tTime\t 3.463904619216919 \tAcc. 0.56574\n",
            "Epoch\t 36 \tLoss\t 1.229627517553476 \tTime\t 3.664689302444458 \tAcc. 0.5665\n",
            "Epoch\t 37 \tLoss\t 1.2264136344958574 \tTime\t 3.447697401046753 \tAcc. 0.5681\n",
            "Epoch\t 38 \tLoss\t 1.2235562403996785 \tTime\t 3.4493844509124756 \tAcc. 0.56784\n",
            "Epoch\t 39 \tLoss\t 1.2205347434068337 \tTime\t 3.499892234802246 \tAcc. 0.56916\n",
            "Epoch\t 40 \tLoss\t 1.2175241231918335 \tTime\t 3.4992058277130127 \tAcc. 0.56918\n",
            "Epoch\t 41 \tLoss\t 1.2152226814856897 \tTime\t 3.6899821758270264 \tAcc. 0.57022\n",
            "Epoch\t 42 \tLoss\t 1.2124131135451488 \tTime\t 3.46091365814209 \tAcc. 0.57136\n",
            "Epoch\t 43 \tLoss\t 1.21012004277645 \tTime\t 3.5227527618408203 \tAcc. 0.5719\n",
            "Epoch\t 44 \tLoss\t 1.2078541541710879 \tTime\t 3.56229829788208 \tAcc. 0.57248\n",
            "Epoch\t 45 \tLoss\t 1.2050563567723984 \tTime\t 3.414027452468872 \tAcc. 0.57318\n",
            "Epoch\t 46 \tLoss\t 1.2029582818349203 \tTime\t 3.63179874420166 \tAcc. 0.57338\n",
            "Epoch\t 47 \tLoss\t 1.200685867285117 \tTime\t 3.5779638290405273 \tAcc. 0.57426\n",
            "Epoch\t 48 \tLoss\t 1.1985625254802215 \tTime\t 3.5535807609558105 \tAcc. 0.57484\n",
            "Epoch\t 49 \tLoss\t 1.196650804617466 \tTime\t 3.470463514328003 \tAcc. 0.57546\n",
            "Epoch\t 50 \tLoss\t 1.194856221247942 \tTime\t 3.4485795497894287 \tAcc. 0.57552\n",
            "Epoch\t 51 \tLoss\t 1.1929374419725858 \tTime\t 3.752814769744873 \tAcc. 0.5768\n",
            "Epoch\t 52 \tLoss\t 1.1912443050971397 \tTime\t 3.507289409637451 \tAcc. 0.57726\n",
            "Epoch\t 53 \tLoss\t 1.1894073761426485 \tTime\t 3.5324769020080566 \tAcc. 0.5776\n",
            "Epoch\t 54 \tLoss\t 1.1878704333916688 \tTime\t 3.4581172466278076 \tAcc. 0.5787\n",
            "Epoch\t 55 \tLoss\t 1.1858134178014903 \tTime\t 3.5025296211242676 \tAcc. 0.57868\n",
            "Epoch\t 56 \tLoss\t 1.184127765435439 \tTime\t 3.55711030960083 \tAcc. 0.57916\n",
            "Epoch\t 57 \tLoss\t 1.1829007420784388 \tTime\t 3.4353466033935547 \tAcc. 0.57918\n",
            "Epoch\t 58 \tLoss\t 1.1809229080493633 \tTime\t 3.688580274581909 \tAcc. 0.58012\n",
            "Epoch\t 59 \tLoss\t 1.1796278012104524 \tTime\t 3.4281136989593506 \tAcc. 0.58084\n",
            "Epoch\t 60 \tLoss\t 1.1780836832829011 \tTime\t 3.519101142883301 \tAcc. 0.58112\n",
            "Epoch\t 61 \tLoss\t 1.1767925354150626 \tTime\t 3.512450695037842 \tAcc. 0.58136\n",
            "Epoch\t 62 \tLoss\t 1.1749515380614843 \tTime\t 3.516707420349121 \tAcc. 0.5818\n",
            "Epoch\t 63 \tLoss\t 1.1738256928248283 \tTime\t 3.6495518684387207 \tAcc. 0.58208\n",
            "Epoch\t 64 \tLoss\t 1.1724845198484568 \tTime\t 3.472351551055908 \tAcc. 0.5827\n",
            "Epoch\t 65 \tLoss\t 1.1713325219276625 \tTime\t 3.4508986473083496 \tAcc. 0.58312\n",
            "Epoch\t 66 \tLoss\t 1.169895490621909 \tTime\t 3.545768976211548 \tAcc. 0.58392\n",
            "Epoch\t 67 \tLoss\t 1.1689736916468694 \tTime\t 3.363624334335327 \tAcc. 0.584\n",
            "Epoch\t 68 \tLoss\t 1.1675820442346425 \tTime\t 3.5836641788482666 \tAcc. 0.58566\n",
            "Epoch\t 69 \tLoss\t 1.166129868152814 \tTime\t 3.5259547233581543 \tAcc. 0.58522\n",
            "Epoch\t 70 \tLoss\t 1.1652667497977232 \tTime\t 3.515737771987915 \tAcc. 0.58544\n",
            "Epoch\t 71 \tLoss\t 1.1637562880149255 \tTime\t 3.534684181213379 \tAcc. 0.5866\n",
            "Epoch\t 72 \tLoss\t 1.16273595675444 \tTime\t 3.5099306106567383 \tAcc. 0.58706\n",
            "Epoch\t 73 \tLoss\t 1.1617049449529404 \tTime\t 3.6303069591522217 \tAcc. 0.5877\n",
            "Epoch\t 74 \tLoss\t 1.1606329324917914 \tTime\t 3.563798427581787 \tAcc. 0.587\n",
            "Epoch\t 75 \tLoss\t 1.1593404311400193 \tTime\t 3.609473705291748 \tAcc. 0.58846\n",
            "Epoch\t 76 \tLoss\t 1.1586786881471292 \tTime\t 3.4080746173858643 \tAcc. 0.58754\n",
            "Epoch\t 77 \tLoss\t 1.1573329971386837 \tTime\t 3.470231771469116 \tAcc. 0.58892\n",
            "Epoch\t 78 \tLoss\t 1.156084045385703 \tTime\t 3.664355516433716 \tAcc. 0.5894\n",
            "Epoch\t 79 \tLoss\t 1.1554698748466297 \tTime\t 3.5188939571380615 \tAcc. 0.58958\n",
            "Epoch\t 80 \tLoss\t 1.1543725218528356 \tTime\t 3.572829484939575 \tAcc. 0.58946\n",
            "Epoch\t 81 \tLoss\t 1.1535346446893153 \tTime\t 3.4693007469177246 \tAcc. 0.58954\n",
            "Epoch\t 82 \tLoss\t 1.1523403730147923 \tTime\t 3.414108991622925 \tAcc. 0.59028\n",
            "Epoch\t 83 \tLoss\t 1.151752754358145 \tTime\t 3.688276529312134 \tAcc. 0.5909\n",
            "Epoch\t 84 \tLoss\t 1.1504840587958312 \tTime\t 3.606675863265991 \tAcc. 0.59098\n",
            "Epoch\t 85 \tLoss\t 1.1496493312028737 \tTime\t 3.727959394454956 \tAcc. 0.59236\n",
            "Epoch\t 86 \tLoss\t 1.148862263178214 \tTime\t 3.4370925426483154 \tAcc. 0.5918\n",
            "Epoch\t 87 \tLoss\t 1.148056472876133 \tTime\t 3.515723943710327 \tAcc. 0.59254\n",
            "Epoch\t 88 \tLoss\t 1.147051350887005 \tTime\t 3.54270076751709 \tAcc. 0.59294\n",
            "Epoch\t 89 \tLoss\t 1.1465770880381265 \tTime\t 3.4777915477752686 \tAcc. 0.59378\n",
            "Epoch\t 90 \tLoss\t 1.1454799187489044 \tTime\t 3.6834137439727783 \tAcc. 0.59312\n",
            "Epoch\t 91 \tLoss\t 1.1443849300726867 \tTime\t 3.4961700439453125 \tAcc. 0.5934\n",
            "Epoch\t 92 \tLoss\t 1.1438640686181876 \tTime\t 3.4410808086395264 \tAcc. 0.59362\n",
            "Epoch\t 93 \tLoss\t 1.1433984496654608 \tTime\t 3.4490907192230225 \tAcc. 0.59322\n",
            "Epoch\t 94 \tLoss\t 1.1424705334198781 \tTime\t 3.4670703411102295 \tAcc. 0.59452\n",
            "Epoch\t 95 \tLoss\t 1.1412174328779563 \tTime\t 3.6505916118621826 \tAcc. 0.59462\n",
            "Epoch\t 96 \tLoss\t 1.1410605069918511 \tTime\t 3.420232057571411 \tAcc. 0.59526\n",
            "Epoch\t 97 \tLoss\t 1.1398669340671637 \tTime\t 3.7805633544921875 \tAcc. 0.5958\n",
            "Epoch\t 98 \tLoss\t 1.1390313075138971 \tTime\t 3.4396753311157227 \tAcc. 0.596\n",
            "Epoch\t 99 \tLoss\t 1.1387781681158604 \tTime\t 3.5399115085601807 \tAcc. 0.59606\n",
            "Epoch\t 100 \tLoss\t 1.1379416239567293 \tTime\t 3.734969139099121 \tAcc. 0.59638\n",
            "Finished training. Train time was: 704.0472855567932\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDDXtJvuNwcF"
      },
      "source": [],
      "execution_count": 12,
      "outputs": []
    }
  ]
}