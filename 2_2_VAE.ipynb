{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/donghuna/AI-Expert/blob/main/2_2_VAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convolutional Variational Autoencoder\n",
        "\n",
        "이번 실습에서는 Convolutional Variational Autoencoder를 구현하고, MNIST data를 이용하여 학습해보도록 하겠습니다.  \n",
        "Input을 latent vector에 바로 매핑하는 Autoencoder와 달리, Variational Autoencoder는 입력 데이터를 확률 분포의 매개변수에 매핑합니다.  \n",
        "  \n",
        "첫 번째 실습에서는 encoder, decoder layer에 linear layer를 사용했지만, 본 실습에서는 이미지 처리에 더 적합한 convolution layer를 사용하여 encoder, decoder를 구현하도록 하겠습니다.\n",
        "\n",
        "- 이 실습자료는 [텐서플로우 Convolutional Variational Autoencoder 실습자료](https://www.tensorflow.org/tutorials/generative/cvae)를 기반으로 작성되었습니다."
      ],
      "metadata": {
        "id": "M9DvMlVdcSzl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from tqdm import tqdm\n",
        "from torchvision.utils import save_image, make_grid\n",
        "\n",
        "DEVICE = torch.device(\"cuda\")"
      ],
      "metadata": {
        "id": "Y9021AhTcqwR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the MNIST dataset\n",
        "\n",
        "MNIST 데이터셋은 28x28 크기의 그레이 스케일 이미지로 구성된 데이터셋입니다.\n",
        "Fashion MNIST와 달리, 0~9 까지의 숫자 이미지들에 대한 데이터셋입니다.\n",
        "총 10개의 클래스를 합쳐 60,000개의 train set, 10,000개의 test set으로 구성되어있습니다.  \n",
        "본 실습은 MNIST 이미지를 latent 압축/복원하는 convolutional variational autoencoder를 구현해보는 실습입니다.  \n",
        "\n",
        "실습에 앞서 필요한 데이터를 로드하도록 하겠습니다.\n",
        "아래 코드는 MNIST 데이터를 로드하는 코드입니다."
      ],
      "metadata": {
        "id": "ZF8EsAtucwHC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.datasets import MNIST\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "dataset_path = '~/datasets'\n",
        "batch_size = 100\n",
        "test_batch_size = 1000\n",
        "\n",
        "mnist_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True}\n",
        "\n",
        "train_dataset = MNIST(dataset_path, transform=mnist_transform, train=True, download=True)\n",
        "test_dataset  = MNIST(dataset_path, transform=mnist_transform, train=False, download=True)\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, **kwargs)\n",
        "test_loader  = DataLoader(dataset=test_dataset,  batch_size=test_batch_size, shuffle=False, **kwargs)"
      ],
      "metadata": {
        "id": "s_hgErq4cwqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convolutional variational auto encoder 구현\n",
        "\n",
        "아래 코드는 convolutional variational auto encoder 구현 코드입니다.\n",
        "\n",
        "Implementation\n",
        "* Encoder: batch x 1 x 28 x 28 shape의 input을 batch x (2 x latent_dim) shape으로 mapping하는 convolutional network  \n",
        "    * Convolution network를 통해 downsampling을 해가며 구현\n",
        "        * Conv1 : 2D Conv with in_channels 1 / out_channels hidden_dims / kernel_size 4, stride 2, padding 1\n",
        "        * Conv2 : 2D Conv with in_channels hidden_dims / out_channels hidden_dims*2 / kernel_size 4, stride 2, padding 1\n",
        "        * fc_mu : fc with in_features hidden_dims*2*7*7 / out_features latent_dims\n",
        "        * fc_logvar : fc with in_features hidden_dims*2*7*7 / out_features latent_dims\n",
        "    * 2 x latent_dim = latent_dim(mean) + latent_dim(logvar)\n",
        "\n",
        "* Deocder: latent $z$로부터 $\\hat{x}$를 복원\n",
        "    * upscaling을 위해 Transposed convolutional network를 사용하여 구현\n",
        "        * fc: fc with in_features latent_dims / out_features hidden_dims*2*7*7\n",
        "        * Conv1: 2D TransposeConv with in_channels hidden_dims*2 / out_channels hidden_dims / kernel_size 4, stride 2, padding 1\n",
        "        * Conv2: 2D TransposeConv with in_channels hidden_dims / out_channels 1 / kernel_size 4, stride 2, padding 1\n",
        "        \n",
        "* reparameterize: latent의 mean과 logvar로부터 latent $z$ sampling\n",
        "    * z = mu + std * ϵ\n",
        "        * mu : mean\n",
        "        * std: standard deviation, exp(logvar) ** 1/2\n",
        "        * epsilon : sample from normal distribution (torch.randn_like)"
      ],
      "metadata": {
        "id": "38CmDnixc6is"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "latent_dims = 32\n",
        "height = 28\n",
        "width = 28\n",
        "channel = 1\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, hidden_dims = 64):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.hidden_dims = hidden_dims\n",
        "        ############# ToDo ################\n",
        "        self.conv1 =\n",
        "        self.conv2 =\n",
        "        self.fc_mu =\n",
        "        self.fc_logvar =\n",
        "        ###################################\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = x.view(x.size(0), -1) # flatten batch of multi-channel feature maps to a batch of feature vectors\n",
        "        x_mu = self.fc_mu(x)\n",
        "        x_logvar = self.fc_logvar(x)\n",
        "        return x_mu, x_logvar\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, hidden_dims=64):\n",
        "        super(Decoder, self).__init__()\n",
        "        ############# ToDo ################\n",
        "        self.hidden_dims = hidden_dims\n",
        "        self.fc =\n",
        "        self.conv1 =\n",
        "        self.conv2 =\n",
        "        ###################################\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc(x)\n",
        "        x = x.view(x.size(0), self.hidden_dims*2, height//4, width//4) # unflatten batch of feature vectors to a batch of multi-channel feature maps\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = torch.sigmoid(self.conv2(x)) # last layer before output is sigmoid, since we are using BCE as reconstruction loss\n",
        "        return x\n",
        "\n",
        "class VariationalAutoencoder(nn.Module):\n",
        "    def __init__(self, hidden_dims):\n",
        "        super(VariationalAutoencoder, self).__init__()\n",
        "        self.encoder = Encoder(hidden_dims)\n",
        "        self.decoder = Decoder(hidden_dims)\n",
        "\n",
        "    def forward(self, x):\n",
        "        latent_mu, latent_logvar = self.encoder(x)\n",
        "        latent = self.latent_sample(latent_mu, latent_logvar)\n",
        "        x_recon = self.decoder(latent)\n",
        "        return x_recon, latent_mu, latent_logvar\n",
        "\n",
        "    def latent_sample(self, mu, logvar):\n",
        "        if self.training:\n",
        "            ############# ToDo ################\n",
        "            # the reparameterization trick\n",
        "            std =\n",
        "            eps =\n",
        "            return\n",
        "            ###################################\n",
        "        else:\n",
        "            return mu\n",
        "\n"
      ],
      "metadata": {
        "id": "0IGLGtAIc8Wr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_dims = 64\n",
        "vae = VariationalAutoencoder(hidden_dims).to(DEVICE)"
      ],
      "metadata": {
        "id": "QaMeLMIDdUYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## loss function 정의\n",
        "\n",
        "VAE의 학습은 objective는 크게 두 가지로 구성됩니다.\n",
        "\n",
        "1. **minimize KL Divergence**\n",
        "<br>\n",
        "<br>\n",
        "$$D_{\\mathrm{KL}}[q_\\phi(z|x)||p(z)]=\\mathbb{E}_{\\mathrm{z}\\sim q_\\phi(z|x)}[\\log q_\\phi(z|x)-\\log p(z)]$$\n",
        "<br>\n",
        "$$q_{\\phi}(z|x)={N}(z;\\mu(x),\\Sigma(x))$$\n",
        "<br>\n",
        "$$p(z)={N}(z;\\mathbf{0,I})$$\n",
        "<br>\n",
        "2. **minimize reconstruction loss** $$\\mathbb{E}_{\\mathrm{z}\\sim q_\\phi(z|x)}[\\log p_\\theta(x|z)]$$  \n",
        "<br>\n",
        "<br>\n",
        "\n",
        "3. **Total Loss (ELBO)**\n",
        "$${L}_{\\mathrm{VAE}}=\\mathbb{E}_{\\mathrm{z}\\sim q_\\phi(z|x)}[\\log q_{\\phi}(z|x)-\\log p(z)+\\mathrm{BCE}(\\hat{x}, x)]$$  \n",
        "각 $x$에 대해, Loss에 대한 single sample Monte Carlo estimator를 최적화함을 통해 모델 학습을 진행하게 됩니다."
      ],
      "metadata": {
        "id": "doBf2SfHdjat"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vae_loss(recon_x, x, mu, logvar):\n",
        "    #################### ToDo ####################\n",
        "    recon_loss =\n",
        "\n",
        "    kldivergence =\n",
        "    ###############################################\n",
        "\n",
        "    return (recon_loss + kldivergence) / x.shape[0]"
      ],
      "metadata": {
        "id": "g8T8t8a7deAw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generating images\n",
        "\n",
        "아래 코드는 위 loss를 활용하여 VAE를 학습하는 코드입니다.\n"
      ],
      "metadata": {
        "id": "bVPzEKD7e7rX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 1e-3\n",
        "optimizer = torch.optim.Adam(params=vae.parameters(), lr=learning_rate, weight_decay=1e-5)"
      ],
      "metadata": {
        "id": "ZJnIRnqwdyOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 30\n",
        "\n",
        "train_loss_avg = []\n",
        "\n",
        "print('Training ...')\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss_avg.append(0)\n",
        "    num_batches = 0\n",
        "\n",
        "    for x, y in train_loader:\n",
        "\n",
        "        x = x.to(DEVICE)\n",
        "\n",
        "        # vae reconstruction\n",
        "        x_hat, z_mu, z_logvar = vae(x)\n",
        "\n",
        "        # reconstruction error\n",
        "        loss = vae_loss(x_hat, x, z_mu, z_logvar)\n",
        "\n",
        "        # backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # one step of the optmizer (using the gradients from backpropagation)\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss_avg[-1] += loss.item()\n",
        "        num_batches += 1\n",
        "\n",
        "    train_loss_avg[-1] /= num_batches\n",
        "    print('Epoch [%d / %d] average reconstruction error: %f' % (epoch+1, num_epochs, train_loss_avg[-1]))"
      ],
      "metadata": {
        "id": "wSkiVXcjdrja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.ion()\n",
        "\n",
        "fig = plt.figure()\n",
        "plt.plot(train_loss_avg)\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1Gbu8sYLfNjQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 학습 결과 Display\n"
      ],
      "metadata": {
        "id": "iDHekbX6f4Sv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "학습된 모델의 encoder, decoder를 통과한 Reconstruction 성능입니다."
      ],
      "metadata": {
        "id": "o9RZnvVEgsC1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "plt.ion()\n",
        "\n",
        "import torchvision.utils\n",
        "\n",
        "vae.eval()\n",
        "\n",
        "# This function takes as an input the images to reconstruct\n",
        "# and the name of the model with which the reconstructions\n",
        "# are performed\n",
        "def to_img(x):\n",
        "    x = x.clamp(0, 1)\n",
        "    return x\n",
        "\n",
        "def show_image(img):\n",
        "    img = to_img(img)\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "\n",
        "def visualise_output(images, model):\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        images = images.to(DEVICE)\n",
        "        images, _, _ = model(images)\n",
        "        images = images.cpu()\n",
        "        images = to_img(images)\n",
        "        np_imagegrid = torchvision.utils.make_grid(images[:50], 10, 5).numpy()\n",
        "        plt.imshow(np.transpose(np_imagegrid, (1, 2, 0)))\n",
        "        plt.show()\n",
        "\n",
        "x_test, y_test = next(iter(test_loader))\n",
        "\n",
        "# First visualise the original images\n",
        "print('Original images')\n",
        "show_image(torchvision.utils.make_grid(x_test[:50],10,5))\n",
        "plt.show()\n",
        "\n",
        "# Reconstruct and visualise the images using the vae\n",
        "print('VAE reconstruction:')\n",
        "visualise_output(x_test, vae)"
      ],
      "metadata": {
        "id": "QXM4s4VuglC6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "학습된 모델의 encoder로 뽑은 latent 간의 interpolation을 적용한 결과입니다."
      ],
      "metadata": {
        "id": "jTjIgBc7g5dc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def interpolation(lambda1, model, img1, img2):\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        # latent vector of first image\n",
        "        img1 = img1.to(DEVICE)\n",
        "        latent_1, _ = model.encoder(img1)\n",
        "\n",
        "        # latent vector of second image\n",
        "        img2 = img2.to(DEVICE)\n",
        "        latent_2, _ = model.encoder(img2)\n",
        "\n",
        "        # interpolation of the two latent vectors\n",
        "        inter_latent = lambda1* latent_1 + (1- lambda1) * latent_2\n",
        "\n",
        "        # reconstruct interpolated image\n",
        "        inter_image = model.decoder(inter_latent)\n",
        "        inter_image = inter_image.cpu()\n",
        "\n",
        "        return inter_image\n",
        "\n",
        "# sort part of test set by digit\n",
        "digits = [[] for _ in range(10)]\n",
        "for x_test, y_test in test_loader:\n",
        "    for i in range(x_test.size(0)):\n",
        "        digits[y_test[i]].append(x_test[i:i+1])\n",
        "    if sum(len(d) for d in digits) >= 1000:\n",
        "        break;\n",
        "\n",
        "# interpolation lambdas\n",
        "lambda_range=np.linspace(0,1,10)\n",
        "\n",
        "fig, axs = plt.subplots(1,10, figsize=(24, 6))\n",
        "fig.subplots_adjust(hspace = .5, wspace=.01)\n",
        "axs = axs.ravel()\n",
        "\n",
        "for ind,l in enumerate(lambda_range):\n",
        "    inter_image=interpolation(float(l), vae, digits[6][0], digits[8][0])\n",
        "\n",
        "    inter_image = to_img(inter_image)\n",
        "\n",
        "    image = inter_image.numpy()\n",
        "\n",
        "    axs[ind].imshow(image[0,0,:,:], cmap='gray')\n",
        "    axs[ind].set_title('lambda_val='+str(round(l,1)))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ioEOzbS5hADU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "아래 내용은 학습된 모델의 Inference 결과를 확인하는 코드입니다."
      ],
      "metadata": {
        "id": "CrSjTFMKhD-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vae.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "    # sample latent vectors from the normal distribution\n",
        "    latent = torch.randn(128, latent_dims, device=DEVICE)\n",
        "\n",
        "    # reconstruct images from the latent vectors\n",
        "    img_recon = vae.decoder(latent)\n",
        "    img_recon = img_recon.cpu()\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(5, 5))\n",
        "    show_image(torchvision.utils.make_grid(img_recon.data[:100],10,5))\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "-tu186fLgRM2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Latent 분포 시각화\n",
        "\n",
        "latent의 분포를 시각화해보도록 하겠습니다."
      ],
      "metadata": {
        "id": "98nd4FHwgT6m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "vae.eval()\n",
        "\n",
        "n_samples = 1000\n",
        "with torch.no_grad():\n",
        "    z_mu_test, z_test_logvar = vae.encoder(x_test[:n_samples].to(DEVICE))\n",
        "    z_test = vae.latent_sample(z_mu_test, z_test_logvar).cpu().numpy()\n",
        "\n",
        "def plot_tsne(inputs, labels):\n",
        "  n_samples = len(labels)\n",
        "  tsne = TSNE(n_components=2, perplexity=10, n_iter=300)\n",
        "  tsne_features = tsne.fit_transform(inputs[:n_samples].reshape(n_samples, -1))\n",
        "  df = pd.DataFrame({\n",
        "      'x': tsne_features[:,0],\n",
        "      'y': tsne_features[:,1],\n",
        "      'label': labels[:n_samples]\n",
        "  })\n",
        "\n",
        "  sns.scatterplot(data=df, x='x', y='y', hue='label', palette=\"deep\")\n",
        "\n",
        "plot_tsne(z_test, y_test)"
      ],
      "metadata": {
        "id": "-Gs48mdKgU0Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}