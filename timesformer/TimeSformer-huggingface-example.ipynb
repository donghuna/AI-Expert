{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyOQKHRsr/bqOv9WI6nkfxEr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "60c7d20048f24d579c76a280f574ce48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1dafea682f0c47c5b16781a0528910b5",
              "IPY_MODEL_75b57e820232406c944fb30fca3caf3d",
              "IPY_MODEL_201ae0a2a7464b0789809e527f502174"
            ],
            "layout": "IPY_MODEL_27974bd7254043e78f4e459e80a080f3"
          }
        },
        "1dafea682f0c47c5b16781a0528910b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_11b54fb5065c4c97a5a49f2496f299ce",
            "placeholder": "​",
            "style": "IPY_MODEL_1911fbc83c81489d9a79f6764be4edc1",
            "value": "config.json: 100%"
          }
        },
        "75b57e820232406c944fb30fca3caf3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3324eaffc8b840bba802f6bd5a809a76",
            "max": 22723,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3d549c24d7684e459a6dd98bb480cbbe",
            "value": 22723
          }
        },
        "201ae0a2a7464b0789809e527f502174": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_943874a525904f6eb242b6050176ef2a",
            "placeholder": "​",
            "style": "IPY_MODEL_d6ea4b4df70446d3b69453c722f0ca3e",
            "value": " 22.7k/22.7k [00:00&lt;00:00, 1.46MB/s]"
          }
        },
        "27974bd7254043e78f4e459e80a080f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "11b54fb5065c4c97a5a49f2496f299ce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1911fbc83c81489d9a79f6764be4edc1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3324eaffc8b840bba802f6bd5a809a76": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d549c24d7684e459a6dd98bb480cbbe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "943874a525904f6eb242b6050176ef2a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d6ea4b4df70446d3b69453c722f0ca3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "87de4704e6574e3996bde9c9fa43481e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_33da606b85bf4dcbab05c5fe5e0231be",
              "IPY_MODEL_080973ff6da041a8948ded00099c35e6",
              "IPY_MODEL_569c489282444dbabed9aa4ef8e9bf80"
            ],
            "layout": "IPY_MODEL_86447116dd784c918e77dd7f83442c84"
          }
        },
        "33da606b85bf4dcbab05c5fe5e0231be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_59abbff68feb4606b8a8f82e15676adf",
            "placeholder": "​",
            "style": "IPY_MODEL_9c9d7f98a4b946299cde46308584c78a",
            "value": "pytorch_model.bin: 100%"
          }
        },
        "080973ff6da041a8948ded00099c35e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_785f2c777be641fab1a1ee70f4fbdadb",
            "max": 486348721,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bcf54c752c3f4cb2b73232a82bb4378f",
            "value": 486348721
          }
        },
        "569c489282444dbabed9aa4ef8e9bf80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ecec63061334785a096404d9f6c389e",
            "placeholder": "​",
            "style": "IPY_MODEL_082ee40f36d24317acb7a1875e91c525",
            "value": " 486M/486M [00:08&lt;00:00, 54.5MB/s]"
          }
        },
        "86447116dd784c918e77dd7f83442c84": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "59abbff68feb4606b8a8f82e15676adf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c9d7f98a4b946299cde46308584c78a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "785f2c777be641fab1a1ee70f4fbdadb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bcf54c752c3f4cb2b73232a82bb4378f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6ecec63061334785a096404d9f6c389e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "082ee40f36d24317acb7a1875e91c525": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/donghuna/AI-Expert/blob/main/timesformer/TimeSformer-huggingface-example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install av"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iP7zgLRCSpk3",
        "outputId": "e7987df3-786a-4d5f-e969-52dae5c54b77"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting av\n",
            "  Downloading av-12.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.3/34.3 MB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: av\n",
            "Successfully installed av-12.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ftplib import FTP\n",
        "import io\n",
        "import random\n",
        "\n",
        "# FTP 서버 정보\n",
        "ftp_server = \"121.136.96.223\"  # Synology NAS의 IP 주소\n",
        "ftp_port = 21  # 기본 포트 21, 다른 포트를 사용하는 경우 해당 포트 번호 입력\n",
        "ftp_user = \"donghuna_ftp\"  # NAS 로그인 사용자명\n",
        "ftp_password = \"Dlehdgns0892!@!?n\"  # NAS 로그인 비밀번호\n",
        "# file_path = \"homes/donghuna/database/Diving48_rgb/rgb/_8Vy3dlHg2w_00000.mp4\"\n",
        "# local_file_path = \"eating_spaghetti.mp4\"  # 로컬에 저장할 파일 이름\n",
        "folder_path = \"homes/donghuna/database/Diving48_rgb/rgb/\"  # 동영상 파일이 있는 폴더 경로\n",
        "\n",
        "\n",
        "# FTP 연결 설정\n",
        "ftp = FTP()\n",
        "ftp.connect(ftp_server, ftp_port)\n",
        "ftp.login(user=ftp_user, passwd=ftp_password)\n",
        "\n",
        "# 액티브 모드 설정 (패시브 모드 비활성화)\n",
        "ftp.set_pasv(True)\n",
        "\n",
        "# 파일 다운로드\n",
        "# with open(local_file_path, 'wb') as local_file:\n",
        "#     ftp.retrbinary(f'RETR {file_path}', local_file.write)\n",
        "\n",
        "# 동영상을 메모리에 저장\n",
        "# video_data = io.BytesIO()\n",
        "# ftp.retrbinary(f'RETR {file_path}', video_data.write)\n",
        "# video_data.seek(0)\n",
        "\n",
        "# 연결 종료\n",
        "# ftp.quit()\n",
        "\n",
        "# print(f\"File downloaded successfully to {local_file_path}\")\n",
        "\n",
        "# 폴더 내 동영상 파일 목록 가져오기\n",
        "ftp.cwd(folder_path)\n",
        "file_list = ftp.nlst()  # 폴더 내 파일 목록\n",
        "\n",
        "# 동영상 파일 필터링 (mp4 확장자)\n",
        "video_files = [file for file in file_list if file.endswith('.mp4')]\n",
        "\n",
        "# 임의로 10개 파일 선택\n",
        "random_video_files = random.sample(video_files, min(10, len(video_files)))\n",
        "\n"
      ],
      "metadata": {
        "id": "62cWKO05eGml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import av # 동영상 파일을 처리하기 위한 라이브러리.\n",
        "import torch # PyTorch 라이브러리로, 모델 학습 및 추론에 사용.\n",
        "import numpy as np # 배열 및 수학적 연산을 위한 라이브러리.\n",
        "\n",
        "from transformers import AutoImageProcessor, TimesformerForVideoClassification # Hugging Face Transformers 라이브러리로, 여기서는 Timesformer 모델을 로드하고 사용.\n",
        "from huggingface_hub import hf_hub_download # Hugging Face Hub에서 데이터를 다운로드하기 위한 함수.\n",
        "\n",
        "np.random.seed(0)\n",
        "\n",
        "# PyAV를 사용하여 지정된 인덱스의 프레임을 읽어오는 함수입니다.\n",
        "# container는 PyAV의 동영상 컨테이너 객체이고, indices는 읽어올 프레임 인덱스 리스트입니다.\n",
        "def read_video_pyav(container, indices):\n",
        "    '''\n",
        "    Decode the video with PyAV decoder.\n",
        "    Args:\n",
        "        container (`av.container.input.InputContainer`): PyAV container.\n",
        "        indices (`List[int]`): List of frame indices to decode.\n",
        "    Returns:\n",
        "        result (np.ndarray): np array of decoded frames of shape (num_frames, height, width, 3).\n",
        "    '''\n",
        "    frames = []\n",
        "    container.seek(0)\n",
        "    start_index = indices[0]\n",
        "    end_index = indices[-1]\n",
        "    for i, frame in enumerate(container.decode(video=0)):\n",
        "        if i > end_index:\n",
        "            break\n",
        "        if i >= start_index and i in indices:\n",
        "            frames.append(frame)\n",
        "    return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n",
        "\n",
        "\n",
        "def sample_frame_indices(clip_len, frame_sample_rate, seg_len):\n",
        "    '''\n",
        "    Sample a given number of frame indices from the video.\n",
        "    Args:\n",
        "        clip_len (`int`): Total number of frames to sample.\n",
        "        frame_sample_rate (`int`): Sample every n-th frame.\n",
        "        seg_len (`int`): Maximum allowed index of sample's last frame.\n",
        "    Returns:\n",
        "        indices (`List[int]`): List of sampled frame indices\n",
        "    '''\n",
        "    converted_len = int(clip_len * frame_sample_rate)\n",
        "    end_idx = np.random.randint(converted_len, seg_len)\n",
        "    start_idx = end_idx - converted_len\n",
        "    indices = np.linspace(start_idx, end_idx, num=clip_len)\n",
        "    indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n",
        "    return indices\n",
        "\n",
        "# 이미지 전처리 및 모델 로드\n",
        "image_processor = AutoImageProcessor.from_pretrained(\"facebook/timesformer-base-finetuned-k400\")\n",
        "model = TimesformerForVideoClassification.from_pretrained(\"facebook/timesformer-base-finetuned-k400\")\n",
        "\n",
        "# 선택된 파일들 처리\n",
        "for file_name in random_video_files:\n",
        "    print(f\"Processing {file_name}...\")\n",
        "\n",
        "    # 동영상을 메모리에 저장\n",
        "    video_data = io.BytesIO()\n",
        "    ftp.retrbinary(f'RETR {file_name}', video_data.write)\n",
        "    video_data.seek(0)\n",
        "\n",
        "    # PyAV를 사용하여 메모리에서 동영상 읽기\n",
        "    container = av.open(video_data, format='mp4')\n",
        "\n",
        "    # 프레임 샘플링\n",
        "    indices = sample_frame_indices(clip_len=8, frame_sample_rate=1, seg_len=container.streams.video[0].frames)\n",
        "    video = read_video_pyav(container, indices)\n",
        "\n",
        "    # 모델 입력 준비\n",
        "    inputs = image_processor(list(video), return_tensors=\"pt\")\n",
        "\n",
        "    # 모델 추론\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "\n",
        "    # 예측 결과 확인\n",
        "    predicted_label = logits.argmax(-1).item()\n",
        "    print(f\"Predicted label for {file_name}: {model.config.id2label[predicted_label]}\")\n",
        "\n",
        "# 연결 종료\n",
        "ftp.quit()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "ZbQYCaJvSfZ3",
        "outputId": "06b3dc66-5f54-48c7-96da-ab3644043384"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing yHo2oERGo7A_00037.mp4...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/feature_extraction_utils.py:141: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:274.)\n",
            "  return torch.tensor(value)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted label for yHo2oERGo7A_00037.mp4: springboard diving\n",
            "Processing fohMq9tOn6E_00010.mp4...\n",
            "Predicted label for fohMq9tOn6E_00010.mp4: springboard diving\n",
            "Processing Q--70-ELqLw_00162.mp4...\n",
            "Predicted label for Q--70-ELqLw_00162.mp4: springboard diving\n",
            "Processing Y7QZcr24ye0_01024.mp4...\n",
            "Predicted label for Y7QZcr24ye0_01024.mp4: springboard diving\n",
            "Processing nOlRwoxsDJ0_00080.mp4...\n",
            "Predicted label for nOlRwoxsDJ0_00080.mp4: swimming backstroke\n",
            "Processing D6zILEKIJbk_00022.mp4...\n",
            "Predicted label for D6zILEKIJbk_00022.mp4: springboard diving\n",
            "Processing zBXtMcI41z8_00007.mp4...\n",
            "Predicted label for zBXtMcI41z8_00007.mp4: pull ups\n",
            "Processing D8YKHC5hmUs_00217.mp4...\n",
            "Predicted label for D8YKHC5hmUs_00217.mp4: springboard diving\n",
            "Processing Y4ARBzok9aU_00024.mp4...\n",
            "Predicted label for Y4ARBzok9aU_00024.mp4: springboard diving\n",
            "Processing c7FHotPDZIw_00096.mp4...\n",
            "Predicted label for c7FHotPDZIw_00096.mp4: trapezing\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'221 Goodbye. You uploaded 0 bytes and downloaded 5710.10 KB.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import random\n",
        "from ftplib import FTP\n",
        "import io\n",
        "import numpy as np\n",
        "import av\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.utils import save_image\n",
        "from torchvision import transforms\n",
        "from transformers import TimesformerForVideoClassification, get_linear_schedule_with_warmup, AdamW\n",
        "\n",
        "# FTP 서버 정보\n",
        "ftp_server = \"121.136.96.223\"\n",
        "ftp_port = 21\n",
        "ftp_user = \"donghuna_ftp\"\n",
        "ftp_password = \"Dlehdgns0892!@!?n\"\n",
        "folder_path = \"homes/donghuna/database/Diving48_rgb/rgb/\"\n",
        "\n",
        "# FTP 연결 설정\n",
        "ftp = FTP()\n",
        "ftp.connect(ftp_server, ftp_port)\n",
        "ftp.login(user=ftp_user, passwd=ftp_password)\n",
        "ftp.set_pasv(True)\n",
        "\n",
        "# 동영상 데이터셋 경로\n",
        "train_json_path = \"Diving48_V2_train.json\"\n",
        "test_json_path = \"Diving48_V2_test.json\"\n",
        "\n",
        "with open(train_json_path, 'wb') as local_file:\n",
        "    ftp.retrbinary(f'RETR {\"homes/donghuna/database/Diving48_rgb/Diving48_V2_train.json\"}', local_file.write)\n",
        "\n",
        "with open(test_json_path, 'wb') as local_file:\n",
        "    ftp.retrbinary(f'RETR {\"homes/donghuna/database/Diving48_rgb/Diving48_V2_test.json\"}', local_file.write)\n",
        "\n",
        "# 동영상 데이터를 읽어오기 위한 함수\n",
        "def read_video_from_ftp(ftp, file_path, start_frame, end_frame):\n",
        "    video_data = io.BytesIO()\n",
        "    ftp.retrbinary(f'RETR {file_path}', video_data.write)\n",
        "    video_data.seek(0)\n",
        "    container = av.open(video_data, format='mp4')\n",
        "    frames = []\n",
        "    for i, frame in enumerate(container.decode(video=0)):\n",
        "        if i > end_frame:\n",
        "            break\n",
        "        if i >= start_frame:\n",
        "            frame_np = frame.to_ndarray(format=\"rgb24\")\n",
        "            frames.append(frame_np.astype(np.uint8))\n",
        "    return np.stack(frames, axis=0)\n",
        "\n",
        "# 변환 함수 정의\n",
        "def pad_and_resize(frames, target_size=(224, 224), num_frames=100):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToPILImage(),\n",
        "        transforms.Resize(target_size),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "    processed_frames = [transform(frame) for frame in frames]\n",
        "\n",
        "    # Pad or trim the sequence to the desired number of frames\n",
        "    if len(processed_frames) < num_frames:\n",
        "        padding = [torch.zeros_like(processed_frames[0]) for _ in range(num_frames - len(processed_frames))]\n",
        "        processed_frames.extend(padding)\n",
        "    else:\n",
        "        processed_frames = processed_frames[:num_frames]\n",
        "\n",
        "    return torch.stack(processed_frames)\n",
        "\n",
        "# Diving48 데이터셋 클래스 정의\n",
        "class Diving48Dataset(Dataset):\n",
        "    def __init__(self, json_path, ftp, folder_path, target_size=(224, 224), num_frames=20):\n",
        "        with open(json_path, 'r') as f:\n",
        "            self.data = json.load(f)\n",
        "        self.ftp = ftp\n",
        "        self.folder_path = folder_path\n",
        "        self.target_size = target_size\n",
        "        self.num_frames = num_frames\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        vid_info = self.data[idx]\n",
        "        vid_name = vid_info['vid_name']\n",
        "        label = vid_info['label']\n",
        "        start_frame = vid_info['start_frame']\n",
        "        end_frame = vid_info['end_frame']\n",
        "        file_path = os.path.join(self.folder_path, f\"{vid_name}.mp4\")\n",
        "\n",
        "        video = read_video_from_ftp(self.ftp, file_path, start_frame, end_frame)\n",
        "        video = pad_and_resize(video, target_size=self.target_size, num_frames=self.num_frames)\n",
        "        # video = video.permute(1, 0, 2, 3)  # (T, C, H, W) -> (C, T, H, W)\n",
        "\n",
        "        return video, label\n",
        "\n",
        "# 모델 로드\n",
        "model = TimesformerForVideoClassification.from_pretrained(\"facebook/timesformer-base-finetuned-k400\")\n",
        "\n",
        "# 데이터셋 및 데이터로더 생성\n",
        "train_dataset = Diving48Dataset(train_json_path, ftp, folder_path)\n",
        "test_dataset = Diving48Dataset(test_json_path, ftp, folder_path)\n",
        "\n",
        "def collate_fn(batch):\n",
        "    videos, labels = zip(*batch)\n",
        "    videos = torch.stack(videos)\n",
        "    labels = torch.tensor(labels)\n",
        "    return videos, labels\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "# 학습 루프 구현\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "num_epochs = 5\n",
        "num_training_steps = num_epochs * len(train_loader)\n",
        "lr_scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
        "\n",
        "# 손실 함수 정의\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# 변환된 동영상 저장 함수\n",
        "def save_transformed_video(video_tensor, filename):\n",
        "    # (C, T, H, W) -> (T, C, H, W)\n",
        "    video_tensor = video_tensor.permute(1, 0, 2, 3)\n",
        "    for i, frame in enumerate(video_tensor):\n",
        "        save_image(frame, f\"{filename}_frame_{i}.png\")\n",
        "\n",
        "# 학습 루프\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    loop_time = 0\n",
        "\n",
        "    for batch in train_loader:\n",
        "        videos, labels = batch\n",
        "        videos = videos.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(videos)\n",
        "        loss = loss_fn(outputs.logits, labels)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        loop_time += 1\n",
        "        if loop_time > 30:\n",
        "            break\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    # 모델 파라미터 저장\n",
        "    torch.save(model.state_dict(), f\"model_epoch_{epoch+1}.pt\")\n",
        "\n",
        "# 평가 루프\n",
        "model.eval()\n",
        "total_correct = 0\n",
        "total_samples = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        videos, labels = batch\n",
        "        videos = videos.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(videos)\n",
        "        _, predicted = torch.max(outputs.logits, 1)\n",
        "        total_correct += (predicted == labels).sum().item()\n",
        "        total_samples += labels.size(0)\n",
        "\n",
        "accuracy = total_correct / total_samples\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# 변환된 동영상 저장 (테스트 데이터셋의 첫 번째 비디오)\n",
        "video, label = test_dataset[0]\n",
        "save_transformed_video(video, \"transformed_video\")\n",
        "\n",
        "# 연결 종료\n",
        "ftp.quit()\n"
      ],
      "metadata": {
        "id": "GWxcdZd8SiHq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 754,
          "referenced_widgets": [
            "60c7d20048f24d579c76a280f574ce48",
            "1dafea682f0c47c5b16781a0528910b5",
            "75b57e820232406c944fb30fca3caf3d",
            "201ae0a2a7464b0789809e527f502174",
            "27974bd7254043e78f4e459e80a080f3",
            "11b54fb5065c4c97a5a49f2496f299ce",
            "1911fbc83c81489d9a79f6764be4edc1",
            "3324eaffc8b840bba802f6bd5a809a76",
            "3d549c24d7684e459a6dd98bb480cbbe",
            "943874a525904f6eb242b6050176ef2a",
            "d6ea4b4df70446d3b69453c722f0ca3e",
            "87de4704e6574e3996bde9c9fa43481e",
            "33da606b85bf4dcbab05c5fe5e0231be",
            "080973ff6da041a8948ded00099c35e6",
            "569c489282444dbabed9aa4ef8e9bf80",
            "86447116dd784c918e77dd7f83442c84",
            "59abbff68feb4606b8a8f82e15676adf",
            "9c9d7f98a4b946299cde46308584c78a",
            "785f2c777be641fab1a1ee70f4fbdadb",
            "bcf54c752c3f4cb2b73232a82bb4378f",
            "6ecec63061334785a096404d9f6c389e",
            "082ee40f36d24317acb7a1875e91c525"
          ]
        },
        "outputId": "1027d106-1dd1-4231-941b-5ddf6e5eb188"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/22.7k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "60c7d20048f24d579c76a280f574ce48"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/486M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "87de4704e6574e3996bde9c9fa43481e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
            "  return F.conv2d(input, weight, bias, self.stride,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5], Loss: 0.0274\n",
            "Epoch [2/5], Loss: 0.0195\n",
            "Epoch [3/5], Loss: 0.0178\n",
            "Epoch [4/5], Loss: 0.0174\n",
            "Epoch [5/5], Loss: 0.0160\n",
            "Test Accuracy: 0.0350\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'save_image' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-9cd6f791068c>\u001b[0m in \u001b[0;36m<cell line: 182>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;31m# 변환된 동영상 저장 (테스트 데이터셋의 첫 번째 비디오)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0mvideo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m \u001b[0msave_transformed_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"transformed_video\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;31m# 연결 종료\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-9cd6f791068c>\u001b[0m in \u001b[0;36msave_transformed_video\u001b[0;34m(video_tensor, filename)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0mvideo_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvideo_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m         \u001b[0msave_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"{filename}_frame_{i}.png\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;31m# 학습 루프\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'save_image' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.utils import save_image\n",
        "save_transformed_video(video, \"transformed_video\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "id": "V-my9N7uw8P8",
        "outputId": "3ad42c5e-7fdb-4a38-c5e1-429a1e7191b9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Cannot handle this data type: (1, 1, 20), |u1",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   3079\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3080\u001b[0;31m             \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrawmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_fromarray_typemap\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtypekey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3081\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: ((1, 1, 20), '|u1')",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-cc41abb04b5b>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msave_image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msave_transformed_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"transformed_video\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-2-9cd6f791068c>\u001b[0m in \u001b[0;36msave_transformed_video\u001b[0;34m(video_tensor, filename)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0mvideo_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvideo_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m         \u001b[0msave_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"{filename}_frame_{i}.png\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;31m# 학습 루프\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/utils.py\u001b[0m in \u001b[0;36msave_image\u001b[0;34m(tensor, fp, format, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;31m# Add 0.5 after unnormalizing to [0, 255] to round to the nearest integer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0mndarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m     \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   3081\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3082\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Cannot handle this data type: %s, %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtypekey\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3083\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3084\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3085\u001b[0m         \u001b[0mrawmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Cannot handle this data type: (1, 1, 20), |u1"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tCdDQuv1PCOv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}