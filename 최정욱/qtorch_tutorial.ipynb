{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/donghuna/AI-Expert/blob/main/%EC%B5%9C%EC%A0%95%EC%9A%B1/qtorch_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhWwfK2rbxhl"
      },
      "source": [
        "# Getting Started: Install QPyTorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A1CDHP_XbJoO",
        "outputId": "d0dfa988-8bcf-4b67-8bea-40cab890d3cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# 자신의 Google Drive 를 Google Collab 에 마운트 시켜줍니다.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PniwZ4TTbDLh",
        "outputId": "2fc3084c-3477-4098-cc8d-c71e594c490d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/qpytorch_lab\n",
            "data  PreResNet_fp32.pth  QPyTorch  qtorch_tutorial.ipynb\n"
          ]
        }
      ],
      "source": [
        "# Google Drive 폴더로 이동합니다.\n",
        "%cd /content/drive/MyDrive/qpytorch_lab/\n",
        "# 폴더 내 파일 목록 확인\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kFvLKcLbPY1s",
        "outputId": "fe84d717-9966-414f-d8d2-9c8703d4c6ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sphinx>=1.4 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (8.0.2)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (6.29.0)\n",
            "Requirement already satisfied: nbsphinx in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (0.9.5)\n",
            "Requirement already satisfied: Ninja in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (1.11.1.1)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (0.16.2+cu118)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (4.66.1)\n",
            "Requirement already satisfied: transformers==4.37.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (4.37.0)\n",
            "Requirement already satisfied: qtorch==0.3.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (0.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.0->-r requirements.txt (line 8)) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.0->-r requirements.txt (line 8)) (0.20.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.0->-r requirements.txt (line 8)) (1.24.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.0->-r requirements.txt (line 8)) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.0->-r requirements.txt (line 8)) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.0->-r requirements.txt (line 8)) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.0->-r requirements.txt (line 8)) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.0->-r requirements.txt (line 8)) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.0->-r requirements.txt (line 8)) (0.4.1)\n",
            "Requirement already satisfied: torch>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from qtorch==0.3.0->-r requirements.txt (line 9)) (2.1.2+cu118)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.4->-r requirements.txt (line 1)) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.4->-r requirements.txt (line 1)) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.4->-r requirements.txt (line 1)) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.4->-r requirements.txt (line 1)) (2.1.0)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.9 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.4->-r requirements.txt (line 1)) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.4->-r requirements.txt (line 1)) (2.0.0)\n",
            "Requirement already satisfied: Jinja2>=3.1 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.4->-r requirements.txt (line 1)) (3.1.2)\n",
            "Requirement already satisfied: Pygments>=2.17 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.4->-r requirements.txt (line 1)) (2.17.2)\n",
            "Requirement already satisfied: docutils<0.22,>=0.20 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.4->-r requirements.txt (line 1)) (0.21.2)\n",
            "Requirement already satisfied: snowballstemmer>=2.2 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.4->-r requirements.txt (line 1)) (2.2.0)\n",
            "Requirement already satisfied: babel>=2.13 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.4->-r requirements.txt (line 1)) (2.14.0)\n",
            "Requirement already satisfied: alabaster>=0.7.14 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.4->-r requirements.txt (line 1)) (1.0.0)\n",
            "Requirement already satisfied: imagesize>=1.3 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.4->-r requirements.txt (line 1)) (1.4.1)\n",
            "Requirement already satisfied: tomli>=2 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.4->-r requirements.txt (line 1)) (2.0.1)\n",
            "Requirement already satisfied: comm>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from ipykernel->-r requirements.txt (line 2)) (0.2.1)\n",
            "Requirement already satisfied: debugpy>=1.6.5 in /usr/local/lib/python3.10/dist-packages (from ipykernel->-r requirements.txt (line 2)) (1.8.0)\n",
            "Requirement already satisfied: ipython>=7.23.1 in /usr/local/lib/python3.10/dist-packages (from ipykernel->-r requirements.txt (line 2)) (8.20.0)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.10/dist-packages (from ipykernel->-r requirements.txt (line 2)) (8.6.0)\n",
            "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.10/dist-packages (from ipykernel->-r requirements.txt (line 2)) (5.7.1)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.10/dist-packages (from ipykernel->-r requirements.txt (line 2)) (0.1.6)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from ipykernel->-r requirements.txt (line 2)) (1.5.9)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ipykernel->-r requirements.txt (line 2)) (5.9.8)\n",
            "Requirement already satisfied: pyzmq>=24 in /usr/local/lib/python3.10/dist-packages (from ipykernel->-r requirements.txt (line 2)) (25.1.2)\n",
            "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.10/dist-packages (from ipykernel->-r requirements.txt (line 2)) (6.4)\n",
            "Requirement already satisfied: traitlets>=5.4.0 in /usr/local/lib/python3.10/dist-packages (from ipykernel->-r requirements.txt (line 2)) (5.14.1)\n",
            "Requirement already satisfied: nbconvert!=5.4,>=5.3 in /usr/local/lib/python3.10/dist-packages (from nbsphinx->-r requirements.txt (line 3)) (7.14.2)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.10/dist-packages (from nbsphinx->-r requirements.txt (line 3)) (5.9.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->-r requirements.txt (line 5)) (9.3.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.0->qtorch==0.3.0->-r requirements.txt (line 9)) (4.4.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.0->qtorch==0.3.0->-r requirements.txt (line 9)) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.0->qtorch==0.3.0->-r requirements.txt (line 9)) (3.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.0->qtorch==0.3.0->-r requirements.txt (line 9)) (2023.12.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.0->qtorch==0.3.0->-r requirements.txt (line 9)) (2.1.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=7.23.1->ipykernel->-r requirements.txt (line 2)) (5.1.1)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython>=7.23.1->ipykernel->-r requirements.txt (line 2)) (0.19.1)\n",
            "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /usr/local/lib/python3.10/dist-packages (from ipython>=7.23.1->ipykernel->-r requirements.txt (line 2)) (3.0.43)\n",
            "Requirement already satisfied: stack-data in /usr/local/lib/python3.10/dist-packages (from ipython>=7.23.1->ipykernel->-r requirements.txt (line 2)) (0.6.3)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from ipython>=7.23.1->ipykernel->-r requirements.txt (line 2)) (1.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=7.23.1->ipykernel->-r requirements.txt (line 2)) (4.9.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.1->sphinx>=1.4->-r requirements.txt (line 1)) (2.1.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from jupyter-client>=6.1.12->ipykernel->-r requirements.txt (line 2)) (2.8.2)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->-r requirements.txt (line 2)) (4.1.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert!=5.4,>=5.3->nbsphinx->-r requirements.txt (line 3)) (4.12.3)\n",
            "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert!=5.4,>=5.3->nbsphinx->-r requirements.txt (line 3)) (6.1.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert!=5.4,>=5.3->nbsphinx->-r requirements.txt (line 3)) (0.7.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert!=5.4,>=5.3->nbsphinx->-r requirements.txt (line 3)) (0.3.0)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.10/dist-packages (from nbconvert!=5.4,>=5.3->nbsphinx->-r requirements.txt (line 3)) (3.0.2)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert!=5.4,>=5.3->nbsphinx->-r requirements.txt (line 3)) (0.9.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert!=5.4,>=5.3->nbsphinx->-r requirements.txt (line 3)) (1.5.1)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert!=5.4,>=5.3->nbsphinx->-r requirements.txt (line 3)) (1.2.1)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.10/dist-packages (from nbformat->nbsphinx->-r requirements.txt (line 3)) (2.19.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat->nbsphinx->-r requirements.txt (line 3)) (4.21.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.37.0->-r requirements.txt (line 8)) (2.1.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.37.0->-r requirements.txt (line 8)) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.37.0->-r requirements.txt (line 8)) (1.26.13)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.37.0->-r requirements.txt (line 8)) (2022.12.7)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from bleach!=5.0.0->nbconvert!=5.4,>=5.3->nbsphinx->-r requirements.txt (line 3)) (1.16.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach!=5.0.0->nbconvert!=5.4,>=5.3->nbsphinx->-r requirements.txt (line 3)) (0.5.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->-r requirements.txt (line 2)) (0.8.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->nbsphinx->-r requirements.txt (line 3)) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->nbsphinx->-r requirements.txt (line 3)) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->nbsphinx->-r requirements.txt (line 3)) (0.32.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->nbsphinx->-r requirements.txt (line 3)) (0.17.1)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel->-r requirements.txt (line 2)) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel->-r requirements.txt (line 2)) (0.2.13)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert!=5.4,>=5.3->nbsphinx->-r requirements.txt (line 3)) (2.5)\n",
            "Requirement already satisfied: executing>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from stack-data->ipython>=7.23.1->ipykernel->-r requirements.txt (line 2)) (2.0.1)\n",
            "Requirement already satisfied: asttokens>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from stack-data->ipython>=7.23.1->ipykernel->-r requirements.txt (line 2)) (2.4.1)\n",
            "Requirement already satisfied: pure-eval in /usr/local/lib/python3.10/dist-packages (from stack-data->ipython>=7.23.1->ipykernel->-r requirements.txt (line 2)) (0.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.5.0->qtorch==0.3.0->-r requirements.txt (line 9)) (1.3.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXNTg39objEC"
      },
      "source": [
        "# Lab 1. Quantizer Tutorial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b3YXBtJdPnmV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from qtorch.quant import Quantizer, quantizer\n",
        "from qtorch import FloatingPoint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOG26L_hlyjO"
      },
      "source": [
        "### Input Tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MeKTakK2vy_G",
        "outputId": "e63a1907-105f-4e5d-ba5f-ab855cf7b506"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0.1942, 0.5044, 0.6839],\n",
            "        [0.6397, 0.8765, 0.8245],\n",
            "        [0.0716, 0.7862, 0.6679]])\n"
          ]
        }
      ],
      "source": [
        "random_input = torch.rand([3,3])\n",
        "print(random_input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EApY1jmgPnmY",
        "outputId": "66f59382-80b7-4257-be4b-86e1c0f8a263"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 255.0000, -255.0000],\n",
            "        [   1.4000,    1.6000]])\n"
          ]
        }
      ],
      "source": [
        "constant_input = torch.tensor([[255., -255.],[1.4, 1.6]])\n",
        "print(constant_input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6oOFQx-lyjO"
      },
      "source": [
        "### Quantization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jDdkM_DIPnmZ"
      },
      "outputs": [],
      "source": [
        "# Example: FP4 (E2M1) Quantization\n",
        "bit = FloatingPoint(exp=2, man=1)\n",
        "quant = quantizer(forward_number=bit, forward_rounding=\"nearest\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AhIfnjodPnmZ",
        "outputId": "477d257f-0182-4ac4-cb37-2a2fcc79dc62"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0.0000, 0.5000, 0.5000],\n",
            "        [0.5000, 1.0000, 1.0000],\n",
            "        [0.0000, 1.0000, 0.5000]])\n"
          ]
        }
      ],
      "source": [
        "random_output = quant(random_input)\n",
        "print(random_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQvwyONzPnma",
        "outputId": "527d8fc5-43f6-4d99-be21-7d3c4c10a444"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 6.0000, -6.0000],\n",
            "        [ 1.5000,  1.5000]])\n"
          ]
        }
      ],
      "source": [
        "constant_output = quant(constant_input)\n",
        "print(constant_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFxoNyqQc3z7"
      },
      "source": [
        "# Lab 2. Reduced-Precision Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G0yRBZ2PW4Ay"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from qtorch.quant import Quantizer, quantizer\n",
        "from qtorch.optim import OptimLP\n",
        "from torch.optim import SGD\n",
        "from qtorch import FloatingPoint\n",
        "from tqdm import tqdm\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XESR-_sAW4A1",
        "outputId": "54e9b20f-d2d1-4cf7-9755-b9d65afe2aa9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "# loading data\n",
        "ds = torchvision.datasets.CIFAR10\n",
        "path = os.path.join(\"./data\", \"CIFAR10\")\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "train_set = ds(path, train=True, download=True, transform=transform_train)\n",
        "test_set = ds(path, train=False, download=True, transform=transform_test)\n",
        "loaders = {\n",
        "        'train': torch.utils.data.DataLoader(\n",
        "            train_set,\n",
        "            batch_size=128,\n",
        "            shuffle=True,\n",
        "            num_workers=4,\n",
        "            pin_memory=True\n",
        "        ),\n",
        "        'test': torch.utils.data.DataLoader(\n",
        "            test_set,\n",
        "            batch_size=128,\n",
        "            num_workers=4,\n",
        "            pin_memory=True\n",
        "        )\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mI5Eqkp0W4A3"
      },
      "outputs": [],
      "source": [
        "# define two floating point formats\n",
        "bit_8 = FloatingPoint(exp=5, man=2)\n",
        "bit_16 = FloatingPoint(exp=6, man=9)\n",
        "\n",
        "# define quantization functions\n",
        "weight_quant = quantizer(forward_number=bit_8,\n",
        "                        forward_rounding=\"nearest\")\n",
        "grad_quant = quantizer(forward_number=bit_8,\n",
        "                        forward_rounding=\"nearest\")\n",
        "momentum_quant = quantizer(forward_number=bit_16,\n",
        "                        forward_rounding=\"stochastic\")\n",
        "acc_quant = quantizer(forward_number=bit_16,\n",
        "                        forward_rounding=\"stochastic\")\n",
        "\n",
        "# define a lambda function so that the Quantizer module can be duplicated easily\n",
        "act_error_quant = lambda : Quantizer(forward_number=bit_8, backward_number=bit_8,\n",
        "                        forward_rounding=\"nearest\", backward_rounding=\"nearest\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HOImoaAsW4A4"
      },
      "outputs": [],
      "source": [
        "# Define model\n",
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=1, bias=False)\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, quant, stride=1, downsample=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.bn1 = nn.BatchNorm2d(inplanes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "        self.quant = quant()\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        ####### FIXME #######\n",
        "        out = self.bn1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv1(out)\n",
        "\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        #####################\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "\n",
        "        return out\n",
        "\n",
        "class PreResNet(nn.Module):\n",
        "\n",
        "    def __init__(self,quant, num_classes=10, depth=20):\n",
        "\n",
        "        super(PreResNet, self).__init__()\n",
        "        assert (depth - 2) % 6 == 0, 'depth should be 6n+2'\n",
        "        n = (depth - 2) // 6\n",
        "\n",
        "        block = BasicBlock\n",
        "\n",
        "        self.inplanes = 16\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1,\n",
        "                               bias=False)\n",
        "        self.layer1 = self._make_layer(block, 16, n, quant)\n",
        "        self.layer2 = self._make_layer(block, 32, n, quant, stride=2)\n",
        "        self.layer3 = self._make_layer(block, 64, n, quant, stride=2)\n",
        "        self.bn = nn.BatchNorm2d(64 * block.expansion)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.avgpool = nn.AvgPool2d(8)\n",
        "        self.fc = nn.Linear(64 * block.expansion, num_classes)\n",
        "        self.quant = quant()\n",
        "        IBM_half = FloatingPoint(exp=6, man=9)\n",
        "        self.quant_half = Quantizer(IBM_half, IBM_half, \"nearest\", \"nearest\")\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, quant, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "            )\n",
        "\n",
        "        layers = list()\n",
        "        layers.append(block(self.inplanes, planes, quant , stride, downsample))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes, quant))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.quant_half(x)\n",
        "        x = self.conv1(x)\n",
        "        x = self.quant(x)\n",
        "\n",
        "        x = self.layer1(x)  # 32x32\n",
        "        x = self.layer2(x)  # 16x16\n",
        "        x = self.layer3(x)  # 8x8\n",
        "        x = self.bn(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.quant(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        x = self.quant_half(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "14jRstmDW4A5"
      },
      "outputs": [],
      "source": [
        "model = PreResNet(act_error_quant)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X9AmXt_zW4A5"
      },
      "outputs": [],
      "source": [
        "device = 'cuda' # change device to 'cuda' if you want to run this example on cuda\n",
        "model = model.to(device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g-pH0RDHW4A5"
      },
      "outputs": [],
      "source": [
        "optimizer = SGD(model.parameters(), lr=0.05, momentum=0.9, weight_decay=5e-4)\n",
        "optimizer = OptimLP(optimizer,\n",
        "                    weight_quant=weight_quant,\n",
        "                    grad_quant=grad_quant,\n",
        "                    momentum_quant=momentum_quant,\n",
        "                    acc_quant=acc_quant,\n",
        "                    grad_scaling=1/1000 # do loss scaling\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p0VLSx1oW4A5"
      },
      "outputs": [],
      "source": [
        "def run_epoch(loader, model, criterion, optimizer=None, phase=\"train\"):\n",
        "    assert phase in [\"train\", \"eval\"], \"invalid running phase\"\n",
        "    loss_sum = 0.0\n",
        "    correct = 0.0\n",
        "\n",
        "    if phase==\"train\": model.train()\n",
        "    elif phase==\"eval\": model.eval()\n",
        "\n",
        "    ttl = 0\n",
        "    with torch.autograd.set_grad_enabled(phase==\"train\"):\n",
        "        for i, (input, target) in tqdm(enumerate(loader), total=len(loader)):\n",
        "            input = input.to(device=device)\n",
        "            target = target.to(device=device)\n",
        "            output = model(input)\n",
        "            loss = criterion(output, target)\n",
        "            loss_sum += loss.cpu().item() * input.size(0)\n",
        "            pred = output.data.max(1, keepdim=True)[1]\n",
        "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
        "            ttl += input.size()[0]\n",
        "\n",
        "            if phase==\"train\":\n",
        "                loss = loss * 1000 # do loss scaling\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "    correct = correct.cpu().item()\n",
        "    return {\n",
        "        'loss': loss_sum / float(ttl),\n",
        "        'accuracy': correct / float(ttl) * 100.0,\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOm1gruUlyjQ"
      },
      "source": [
        "### Accuracy Before Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_7bw-5x0lyjR",
        "outputId": "fe9881c8-6180-4eea-bf9a-f9163ea18b1f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:00<00:00, 84.25it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 2.848639715576172, 'accuracy': 13.96}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "test_res = run_epoch(loaders['test'], model, F.cross_entropy,\n",
        "                        optimizer=optimizer, phase=\"eval\")\n",
        "print(test_res)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Grgd_LPplyjR"
      },
      "source": [
        "### 3-Epoch Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbgMAJtoW4A6",
        "outputId": "28499849-a11d-471c-c6d2-5a295534581c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 391/391 [00:09<00:00, 40.85it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=====> Epoch 0\n",
            "Train loss    : 1.6531598581314086\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 391/391 [00:09<00:00, 41.09it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=====> Epoch 1\n",
            "Train loss    : 1.1806247606658935\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 391/391 [00:09<00:00, 39.57it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=====> Epoch 2\n",
            "Train loss    : 0.9946341005134582\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|                                                                                                                                                                                                                                                              | 0/391 [00:00<?, ?it/s]\n",
            "Exception in thread Thread-10 (_pin_memory_loop):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\", line 761, in run_closure\n",
            "    _threading_Thread_run(self)\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 953, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/pin_memory.py\", line 54, in _pin_memory_loop\n",
            "    do_one_step()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/pin_memory.py\", line 31, in do_one_step\n",
            "    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)\n",
            "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 122, in get\n",
            "    return _ForkingPickler.loads(res)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/multiprocessing/reductions.py\", line 355, in rebuild_storage_fd\n",
            "    fd = df.detach()\n",
            "  File \"/usr/lib/python3.10/multiprocessing/resource_sharer.py\", line 57, in detach\n",
            "    with _resource_sharer.get_connection(self._id) as conn:\n",
            "  File \"/usr/lib/python3.10/multiprocessing/resource_sharer.py\", line 86, in get_connection\n",
            "    c = Client(address, authkey=process.current_process().authkey)\n",
            "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 502, in Client\n",
            "    c = SocketClient(address)\n",
            "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 630, in SocketClient\n",
            "    s.connect(address)\n",
            "FileNotFoundError: [Errno 2] No such file or directory\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[19], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     train_res \u001b[38;5;241m=\u001b[39m \u001b[43mrun_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloaders\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                                \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mphase\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m=====> Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain loss    : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_res[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
            "Cell \u001b[0;32mIn[17], line 11\u001b[0m, in \u001b[0;36mrun_epoch\u001b[0;34m(loader, model, criterion, optimizer, phase)\u001b[0m\n\u001b[1;32m      9\u001b[0m ttl \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_grad_enabled(phase\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 11\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, (\u001b[38;5;28minput\u001b[39m, target) \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28menumerate\u001b[39m(loader), total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(loader)):\n\u001b[1;32m     12\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m     13\u001b[0m         target \u001b[38;5;241m=\u001b[39m target\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice)\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py:1182\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1181\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1182\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1185\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:1328\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1327\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1328\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1329\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1331\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:1284\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1282\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m   1283\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_thread\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[0;32m-> 1284\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1285\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1286\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:1132\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1120\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1121\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1129\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1131\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1132\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1133\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1134\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1135\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
            "File \u001b[0;32m/usr/lib/python3.10/queue.py:180\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m    179\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m--> 180\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get()\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_full\u001b[38;5;241m.\u001b[39mnotify()\n",
            "File \u001b[0;32m/usr/lib/python3.10/threading.py:324\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 324\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    326\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "for epoch in range(3):\n",
        "    train_res = run_epoch(loaders['train'], model, F.cross_entropy,\n",
        "                                optimizer=optimizer, phase=\"train\")\n",
        "    print(f'=====> Epoch {epoch}')\n",
        "    print(f'Train loss    : {train_res[\"loss\"]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6T5qCj6XlyjR"
      },
      "source": [
        "### Accuracy After Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X8-KyxWdlyjR",
        "outputId": "f829b54f-691e-4965-a644-14bad8ca6588"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:00<00:00, 99.80it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 1.0952622838974, 'accuracy': 61.1}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "test_res = run_epoch(loaders['test'], model, F.cross_entropy,\n",
        "                        optimizer=optimizer, phase=\"eval\")\n",
        "print(test_res)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_MP5qXQdK20"
      },
      "source": [
        "# Lab 3.1. Trans-Precision Inference on CNN Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WrNdpm-WXr7n"
      },
      "outputs": [],
      "source": [
        "# Define model\n",
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=1, bias=False)\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, quant, stride=1, downsample=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.bn1 = nn.BatchNorm2d(inplanes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "        self.quant = quant()\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.bn1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.quant(out)\n",
        "        out = self.conv1(out)\n",
        "        out = self.quant(out)\n",
        "\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.quant(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.quant(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "\n",
        "        return out\n",
        "\n",
        "class PreResNet(nn.Module):\n",
        "\n",
        "    def __init__(self,quant, num_classes=10, depth=20):\n",
        "\n",
        "        super(PreResNet, self).__init__()\n",
        "        assert (depth - 2) % 6 == 0, 'depth should be 6n+2'\n",
        "        n = (depth - 2) // 6\n",
        "\n",
        "        block = BasicBlock\n",
        "\n",
        "        self.inplanes = 16\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1,\n",
        "                               bias=False)\n",
        "        self.layer1 = self._make_layer(block, 16, n, quant)\n",
        "        self.layer2 = self._make_layer(block, 32, n, quant, stride=2)\n",
        "        self.layer3 = self._make_layer(block, 64, n, quant, stride=2)\n",
        "        self.bn = nn.BatchNorm2d(64 * block.expansion)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.avgpool = nn.AvgPool2d(8)\n",
        "        self.fc = nn.Linear(64 * block.expansion, num_classes)\n",
        "        self.quant = quant()\n",
        "        IBM_half = FloatingPoint(exp=6, man=9)\n",
        "        self.quant_half = Quantizer(IBM_half, IBM_half, \"nearest\", \"nearest\")\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, quant, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "            )\n",
        "\n",
        "        layers = list()\n",
        "        layers.append(block(self.inplanes, planes, quant , stride, downsample))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes, quant))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.quant_half(x)\n",
        "        x = self.conv1(x)\n",
        "        x = self.quant(x)\n",
        "\n",
        "        x = self.layer1(x)  # 32x32\n",
        "        x = self.layer2(x)  # 16x16\n",
        "        x = self.layer3(x)  # 8x8\n",
        "        x = self.bn(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.quant(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        x = self.quant_half(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Q0sOJlxXr7m"
      },
      "outputs": [],
      "source": [
        "# define Three floating point formats\n",
        "bit_8 = FloatingPoint(exp=5, man=2)\n",
        "bit_16 = FloatingPoint(exp=6, man=9)\n",
        "bit_32 = FloatingPoint(exp=8, man=23)\n",
        "\n",
        "# define a lambda function so that the Quantizer module can be duplicated easily\n",
        "act_error_quant = lambda : Quantizer(forward_number=bit_8, backward_number=bit_8,\n",
        "                        forward_rounding=\"nearest\", backward_rounding=\"nearest\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Df_x9rYgXr7n"
      },
      "outputs": [],
      "source": [
        "model = PreResNet(act_error_quant)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6RYFGnwXr7o",
        "outputId": "4e32796d-e30f-4145-8787-8a1f5690b86f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "checkpoint = torch.load('./PreResNet_fp32.pth')\n",
        "model.load_state_dict(checkpoint['model'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iw_TioBwXr7o"
      },
      "outputs": [],
      "source": [
        "device = 'cuda' # change device to 'cuda' if you want to run this example on cuda\n",
        "model = model.to(device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rd5cwB5oXr7p"
      },
      "outputs": [],
      "source": [
        "def run_test(loader, model, criterion, optimizer=None):\n",
        "    loss_sum = 0.0\n",
        "    correct = 0.0\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    ttl = 0\n",
        "    with torch.no_grad():\n",
        "        for i, (input, target) in tqdm(enumerate(loader), total=len(loader)):\n",
        "            input = input.to(device=device)\n",
        "            target = target.to(device=device)\n",
        "            output = model(input)\n",
        "            loss = criterion(output, target)\n",
        "            loss_sum += loss.cpu().item() * input.size(0)\n",
        "            pred = output.data.max(1, keepdim=True)[1]\n",
        "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
        "            ttl += input.size()[0]\n",
        "\n",
        "    correct = correct.cpu().item()\n",
        "    return {\n",
        "        'loss': loss_sum / float(ttl),\n",
        "        'accuracy': correct / float(ttl) * 100.0,\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dD1nssfBXr7p",
        "outputId": "b9b2d2a2-9030-48f8-f794-383c5c71cc86"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:00<00:00, 80.73it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.2666136680841446, 'accuracy': 92.15}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "test_res = run_test(loaders['test'], model, F.cross_entropy, optimizer=optimizer)\n",
        "print(test_res)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRAbPrBglyjS"
      },
      "source": [
        "# Lab 3.2. Trans-Precision Inference on LLMs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8WYdtXFwlyjS"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wo96ZePjlyjS",
        "outputId": "8fb67b35-f32b-4347-c3bc-4bdc862f6d12"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ],
      "source": [
        "model_id = \"Qwen/Qwen2-0.5B-Instruct\"\n",
        "device = \"cuda\" # the device to load the model onto\n",
        "\n",
        "LLM = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.float32,\n",
        "    attn_implementation=\"eager\",\n",
        "    device_map=device\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UFH37gMzlyjW"
      },
      "outputs": [],
      "source": [
        "prompt = \"Give me a short introduction of Samsung Electronics.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2vaDGX0_lyjW",
        "outputId": "3d045f80-6b0f-4d06-f4e7-f1e9504f280f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Samsung Electronics is one of the world's largest and most successful electronics companies, with a presence in over 190 countries. The company was founded in 1938 by Lee Dong-hwan and has since grown to become one of the world's leading producers of consumer electronics, including smartphones, TVs, computers, and other devices.\n",
            "\n",
            "Samsung Electronics operates several major divisions, including the Mobile Phone Division, which produces smartphones, tablets, and other mobile devices; the TV Division, which produces TVs, LCD monitors, and other home entertainment products; the Consumer Electronics Division, which produces a wide range of consumer electronics products, including televisions\n"
          ]
        }
      ],
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": prompt}\n",
        "]\n",
        "\n",
        "text = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True\n",
        ")\n",
        "model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
        "\n",
        "generated_ids = LLM.generate(\n",
        "    model_inputs.input_ids,\n",
        "    num_beams=1,\n",
        "    do_sample=False,\n",
        "    max_new_tokens=128\n",
        ")\n",
        "generated_ids = [\n",
        "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
        "]\n",
        "\n",
        "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VH8-a1XulyjW"
      },
      "source": [
        "## Define Quantized Modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fb-aFkmBlyjW"
      },
      "outputs": [],
      "source": [
        "from typing import List, Optional, Tuple, Union\n",
        "from transformers.cache_utils import Cache, DynamicCache\n",
        "\n",
        "class MatMul(nn.Module):\n",
        "    def forward(self, A, B):\n",
        "        return A @ B\n",
        "\n",
        "class QuantMatMul(nn.Module):\n",
        "    def __init__(self, qbit):\n",
        "        super().__init__()\n",
        "        self.quantizer = Quantizer(forward_number=qbit, backward_number=qbit,\n",
        "                     forward_rounding=\"nearest\", backward_rounding=\"nearest\")\n",
        "    def forward(self, A, B):\n",
        "        qA = self.quantizer.quantize(A.data)\n",
        "        qB = self.quantizer.quantize(B.data)\n",
        "        return qA @ qB\n",
        "\n",
        "from transformers.models.qwen2.modeling_qwen2 import rotate_half, apply_rotary_pos_emb, repeat_kv\n",
        "\n",
        "def attn_forward(\n",
        "    self,\n",
        "    hidden_states: torch.Tensor,\n",
        "    attention_mask: Optional[torch.Tensor] = None,\n",
        "    position_ids: Optional[torch.LongTensor] = None,\n",
        "    past_key_value: Optional[Cache] = None,\n",
        "    output_attentions: bool = False,\n",
        "    use_cache: bool = False,\n",
        "    **kwargs,\n",
        ") -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
        "    if \"padding_mask\" in kwargs:\n",
        "        warnings.warn(\n",
        "            \"Passing `padding_mask` is deprecated and will be removed in v4.37. Please make sure use `attention_mask` instead.`\"\n",
        "        )\n",
        "    bsz, q_len, _ = hidden_states.size()\n",
        "\n",
        "    query_states = self.q_proj(hidden_states)\n",
        "    key_states = self.k_proj(hidden_states)\n",
        "    value_states = self.v_proj(hidden_states)\n",
        "\n",
        "    query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "    key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
        "    value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "    kv_seq_len = key_states.shape[-2]\n",
        "    if past_key_value is not None:\n",
        "        if self.layer_idx is None:\n",
        "            raise ValueError(\n",
        "                f\"The cache structure has changed since version v4.36. If you are using {self.__class__.__name__} \"\n",
        "                \"for auto-regressive decoding with k/v caching, please make sure to initialize the attention class \"\n",
        "                \"with a layer index.\"\n",
        "            )\n",
        "        kv_seq_len += past_key_value.get_usable_length(kv_seq_len, self.layer_idx)\n",
        "    cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)\n",
        "    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n",
        "\n",
        "    if past_key_value is not None:\n",
        "        cache_kwargs = {\"sin\": sin, \"cos\": cos}  # Specific to RoPE models\n",
        "        key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n",
        "\n",
        "    # repeat k/v heads if n_kv_heads < n_heads\n",
        "    key_states = repeat_kv(key_states, self.num_key_value_groups)\n",
        "    value_states = repeat_kv(value_states, self.num_key_value_groups)\n",
        "\n",
        "    attn_weights = self.matmul1(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
        "\n",
        "    if attn_weights.size() != (bsz, self.num_heads, q_len, kv_seq_len):\n",
        "        raise ValueError(\n",
        "            f\"Attention weights should be of size {(bsz, self.num_heads, q_len, kv_seq_len)}, but is\"\n",
        "            f\" {attn_weights.size()}\"\n",
        "        )\n",
        "\n",
        "    if attention_mask is not None:\n",
        "        if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):\n",
        "            raise ValueError(\n",
        "                f\"Attention mask should be of size {(bsz, 1, q_len, kv_seq_len)}, but is {attention_mask.size()}\"\n",
        "            )\n",
        "\n",
        "        attn_weights = attn_weights + attention_mask\n",
        "\n",
        "    # upcast attention to fp32\n",
        "    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
        "    attn_weights = nn.functional.dropout(attn_weights, p=self.attention_dropout, training=self.training)\n",
        "    attn_output = self.matmul2(attn_weights, value_states)\n",
        "\n",
        "    if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n",
        "        raise ValueError(\n",
        "            f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n",
        "            f\" {attn_output.size()}\"\n",
        "        )\n",
        "\n",
        "    attn_output = attn_output.transpose(1, 2).contiguous()\n",
        "    attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
        "\n",
        "    attn_output = self.o_proj(attn_output)\n",
        "\n",
        "    if not output_attentions:\n",
        "        attn_weights = None\n",
        "\n",
        "    return attn_output, attn_weights, past_key_value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gkZr456QlyjX"
      },
      "outputs": [],
      "source": [
        "class QuantLinear(nn.Linear):\n",
        "    def __init__(self,in_features,out_features,bias,qbit):\n",
        "        super().__init__(in_features,out_features,bias)\n",
        "        self.quantizer_x = Quantizer(forward_number=qbit, backward_number=qbit,\n",
        "                     forward_rounding=\"nearest\", backward_rounding=\"nearest\")\n",
        "        self.quantizer_w = Quantizer(forward_number=qbit, backward_number=qbit,\n",
        "                     forward_rounding=\"nearest\", backward_rounding=\"nearest\")\n",
        "    def forward(self,x):\n",
        "        w = self.weight.clone()\n",
        "        qx = self.quantizer_x.quantize(x.data)\n",
        "        qw = self.quantizer_w.quantize(w.data)\n",
        "        return F.linear(qx,qw,self.bias)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EO6p4RAylyjX"
      },
      "outputs": [],
      "source": [
        "from types import MethodType\n",
        "\n",
        "def get_QLLM(LLM, qbit):\n",
        "    from transformers.models.qwen2.modeling_qwen2 import Qwen2Attention\n",
        "    for name, module in LLM.named_modules():\n",
        "        if isinstance(module, Qwen2Attention):\n",
        "            setattr(module, \"matmul1\", QuantMatMul(qbit))\n",
        "            setattr(module, \"matmul2\", QuantMatMul(qbit))\n",
        "            module.forward = MethodType(attn_forward, module)\n",
        "\n",
        "    wrapped_modules={}\n",
        "    module_dict={}\n",
        "    it=[(name,m) for name,m in LLM.named_modules()]\n",
        "    for name,m in it:\n",
        "        module_dict[name]=m\n",
        "        idx=name.rfind('.')\n",
        "        if idx==-1:\n",
        "            idx=0\n",
        "        father_name=name[:idx]\n",
        "        if father_name in module_dict:\n",
        "            father_module=module_dict[father_name]\n",
        "        else:\n",
        "            raise RuntimeError(f\"father module {father_name} not found\")\n",
        "        if isinstance(m,nn.Linear) and 'head' not in name:\n",
        "            idx = idx+1 if idx != 0 else idx\n",
        "            new_m = QuantLinear(m.in_features,m.out_features,m.bias is not None,qbit=qbit)\n",
        "            new_m.weight.data=m.weight.data\n",
        "            new_m.bias=m.bias\n",
        "            replace_m=new_m\n",
        "            wrapped_modules[name] = new_m\n",
        "            setattr(father_module,name[idx:],replace_m)\n",
        "    LLM.eval()\n",
        "    return wrapped_modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LLdQfr23lyjX"
      },
      "outputs": [],
      "source": [
        "qbit = FloatingPoint(exp=5, man=2)\n",
        "wrapped_modules = get_QLLM(LLM, qbit)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6RcqDu9_lyjX"
      },
      "outputs": [],
      "source": [
        "prompt = \"Give me a short introduction of Samsung Electronics.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G1_Px9PNlyjX",
        "outputId": "18cff8a8-2dfc-4d96-9875-932c7bc4292e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Samsung Electronics is a South Korean multinational corporation headquartered in Seoul, South Korea that specializes in the design and manufacturing of consumer electronics products such as smartphones, smart TVs, home appliances, and personal computers. The company was founded in 1978 by Lee Jae-yong Kim and his brother-in-law, Lee Dong-hoon, who later became CEO. The company's flagship product line includes smartphones, smart TVs, home appliances, and personal computers. It also produces various other consumer electronics products like laptops, gaming consoles, and accessories. Samsung Electronics has been recognized for its commitment to innovation and sustainability, with a focus on producing environmentally friendly products\n"
          ]
        }
      ],
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": prompt}\n",
        "]\n",
        "\n",
        "text = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True\n",
        ")\n",
        "model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
        "\n",
        "generated_ids = LLM.generate(\n",
        "    model_inputs.input_ids,\n",
        "    num_beams=1,\n",
        "    do_sample=False,\n",
        "    max_new_tokens=128\n",
        ")\n",
        "generated_ids = [\n",
        "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
        "]\n",
        "\n",
        "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kR4ZCBhYlyjX"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}