{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "izvDY1yPG1g7",
        "BNskrxjIjjzx",
        "itR2lCdujEJ0",
        "7UyJYyj9jQK_",
        "oiKpeQRyHGxB",
        "4ItRriX3HIy_"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/donghuna/AI-Expert/blob/main/Language_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install packages"
      ],
      "metadata": {
        "id": "izvDY1yPG1g7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate bitsandbytes transformers"
      ],
      "metadata": {
        "id": "XuCR2sD--KAU",
        "outputId": "941d4a1b-b526-46c4-c6a3-59be4f1c0cfb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting accelerate\n",
            "  Downloading accelerate-0.31.0-py3-none-any.whl (309 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.4/309.4 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.3.0+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.23.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.14.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.6.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, bitsandbytes, accelerate\n",
            "Successfully installed accelerate-0.31.0 bitsandbytes-0.43.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing"
      ],
      "metadata": {
        "id": "BNskrxjIjjzx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import warnings\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from typing import Optional, Tuple\n",
        "import math\n",
        "from transformers.utils import logging\n",
        "from transformers.cache_utils import Cache\n",
        "from transformers.models.llama.configuration_llama import LlamaConfig\n",
        "from transformers.models.llama.modeling_llama import (\n",
        "    LlamaMLP,\n",
        "    LlamaRMSNorm,\n",
        "    apply_rotary_pos_emb,\n",
        ")\n",
        "from transformers import (\n",
        "    LlamaForCausalLM,\n",
        "    AutoTokenizer,\n",
        ")\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "0VbAyRS-jjWH"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Llama attention"
      ],
      "metadata": {
        "id": "itR2lCdujEJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logger = logging.get_logger(__name__)\n",
        "\n",
        "\n",
        "def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n",
        "    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n",
        "    \"\"\"\n",
        "    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n",
        "    if n_rep == 1:\n",
        "        return hidden_states\n",
        "    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n",
        "    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n",
        "\n",
        "\n",
        "class LlamaAttention(nn.Module):\n",
        "    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n",
        "\n",
        "    def __init__(self, config: LlamaConfig, layer_idx: Optional[int] = None):\n",
        "        super().__init__(config, layer_idx)\n",
        "        self.config = config\n",
        "        self.layer_idx = layer_idx\n",
        "        if layer_idx is None:\n",
        "            logger.warning_once(\n",
        "                f\"Instantiating {self.__class__.__name__} without passing a `layer_idx` is not recommended and will \"\n",
        "                \"lead to errors during the forward call if caching is used. Please make sure to provide a `layer_idx` \"\n",
        "                \"when creating this class.\"\n",
        "            )\n",
        "\n",
        "        self.attention_dropout = config.attention_dropout\n",
        "        ##########################################################\n",
        "        self.hidden_size = config.hidden_size\n",
        "        self.num_heads = config.num_attention_heads\n",
        "        self.head_dim = self.hidden_size // self.num_heads\n",
        "        self.num_key_value_heads = config.num_key_value_heads # k,v head 개수\n",
        "        self.num_key_value_groups =                           # 한 그룹 내의 head 개수\n",
        "        ##########################################################\n",
        "        self.max_position_embeddings = config.max_position_embeddings\n",
        "        self.rope_theta = config.rope_theta\n",
        "        self.is_causal = True\n",
        "\n",
        "        if (self.head_dim * self.num_heads) != self.hidden_size:\n",
        "            raise ValueError(\n",
        "                f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}\"\n",
        "                f\" and `num_heads`: {self.num_heads}).\"\n",
        "            )\n",
        "\n",
        "        ##########################################################\n",
        "        # q,k,v projection layer와 output projection layer\n",
        "        # query projection layer를 선언하세요.\n",
        "        self.q_proj =\n",
        "        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n",
        "        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n",
        "        self.o_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=config.attention_bias)\n",
        "        ##########################################################\n",
        "        self._init_rope()\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states: torch.Tensor,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        position_ids: Optional[torch.LongTensor] = None,\n",
        "        past_key_value: Optional[Cache] = None,\n",
        "        output_attentions: bool = False,\n",
        "        use_cache: bool = False,\n",
        "        cache_position: Optional[torch.LongTensor] = None,\n",
        "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
        "        bsz, q_len, _ = hidden_states.size()\n",
        "\n",
        "        if self.config.pretraining_tp > 1:\n",
        "            key_value_slicing = (self.num_key_value_heads * self.head_dim) // self.config.pretraining_tp\n",
        "            query_slices = self.q_proj.weight.split(\n",
        "                (self.num_heads * self.head_dim) // self.config.pretraining_tp, dim=0\n",
        "            )\n",
        "            key_slices = self.k_proj.weight.split(key_value_slicing, dim=0)\n",
        "            value_slices = self.v_proj.weight.split(key_value_slicing, dim=0)\n",
        "\n",
        "            query_states = [F.linear(hidden_states, query_slices[i]) for i in range(self.config.pretraining_tp)]\n",
        "            query_states = torch.cat(query_states, dim=-1)\n",
        "\n",
        "            key_states = [F.linear(hidden_states, key_slices[i]) for i in range(self.config.pretraining_tp)]\n",
        "            key_states = torch.cat(key_states, dim=-1)\n",
        "\n",
        "            value_states = [F.linear(hidden_states, value_slices[i]) for i in range(self.config.pretraining_tp)]\n",
        "            value_states = torch.cat(value_states, dim=-1)\n",
        "\n",
        "        else:\n",
        "            ##########################################################\n",
        "            # q,k,v projection layer를 통과시킵니다.\n",
        "            query_states =\n",
        "            key_states =\n",
        "            value_states =\n",
        "            ##########################################################\n",
        "\n",
        "        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
        "        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        cos, sin = self.rotary_emb(value_states, position_ids)\n",
        "        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n",
        "\n",
        "        if past_key_value is not None:\n",
        "            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n",
        "            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n",
        "            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n",
        "\n",
        "        ##########################################################\n",
        "        # GQA 연산을 위해 k,v 행렬을 q 행렬과 같은 크기로 반복하여 만들어줍니다. (repeat_kv 함수 사용)\n",
        "        # 참고: 현재 key_states 와 value_states의 shape은 (bsz, self.num_key_value_heads, q_len, self.head_dim)\n",
        "        key_states =\n",
        "        value_states =\n",
        "        ##########################################################\n",
        "\n",
        "        ##########################################################\n",
        "        # 어텐션 웨이트를 계산합니다.\n",
        "        attn_weights =\n",
        "        ##########################################################\n",
        "\n",
        "        if attention_mask is not None:  # no matter the length, we just slice it\n",
        "            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n",
        "            attn_weights = attn_weights + causal_mask\n",
        "\n",
        "        # upcast attention to fp32\n",
        "        ##########################################################\n",
        "        # 위에서 구한 어텐션 웨이트에 softmax를 적용하세요.\n",
        "        # 이때 데이터 타입은 torch.float32로 지정하여 계산하세요.\n",
        "        # 구해진 softmax을 다시 query_states와 같은 데이터 타입으로 변경하세요.\n",
        "        attn_weights =\n",
        "        attn_weights = nn.functional.dropout(attn_weights, p=self.attention_dropout, training=self.training)\n",
        "        # 어텐션 웨이트와 v 행렬을 이용해 어텐션 output을 구하세요\n",
        "        attn_output =\n",
        "        ##########################################################\n",
        "\n",
        "        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n",
        "            raise ValueError(\n",
        "                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n",
        "                f\" {attn_output.size()}\"\n",
        "            )\n",
        "\n",
        "        attn_output = attn_output.transpose(1, 2).contiguous()\n",
        "\n",
        "        attn_output = attn_output.reshape(bsz, q_len, -1)\n",
        "\n",
        "        if self.config.pretraining_tp > 1:\n",
        "            attn_output = attn_output.split(self.hidden_size // self.config.pretraining_tp, dim=2)\n",
        "            o_proj_slices = self.o_proj.weight.split(self.hidden_size // self.config.pretraining_tp, dim=1)\n",
        "            attn_output = sum([F.linear(attn_output[i], o_proj_slices[i]) for i in range(self.config.pretraining_tp)])\n",
        "        else:\n",
        "            ##########################################################\n",
        "            # 마지막으로 output projection layer를 통과시킵니다.\n",
        "            attn_output =\n",
        "            ##########################################################\n",
        "\n",
        "        if not output_attentions:\n",
        "            attn_weights = None\n",
        "\n",
        "        return attn_output, attn_weights, past_key_value"
      ],
      "metadata": {
        "id": "TV1M7Sncg9Up"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Llama decoder layer"
      ],
      "metadata": {
        "id": "7UyJYyj9jQK_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LlamaDecoderLayer(nn.Module):\n",
        "    def __init__(self, config: LlamaConfig, layer_idx: int):\n",
        "        super().__init__(config, layer_idx)\n",
        "        self.hidden_size = config.hidden_size\n",
        "        ##########################################################\n",
        "        self.self_attn = LlamaAttention(config=config, layer_idx=layer_idx)\n",
        "        self.mlp = LlamaMLP(config)\n",
        "        self.input_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps) # 가장 처음 적용하는 norm\n",
        "        self.post_attention_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps) # 어텐션 후에 적용하는 norm\n",
        "        ##########################################################\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states: torch.Tensor,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        position_ids: Optional[torch.LongTensor] = None,\n",
        "        past_key_value: Optional[Cache] = None,\n",
        "        output_attentions: Optional[bool] = False,\n",
        "        use_cache: Optional[bool] = False,\n",
        "        cache_position: Optional[torch.LongTensor] = None,\n",
        "    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n",
        "            attention_mask (`torch.FloatTensor`, *optional*):\n",
        "                attention mask of size `(batch_size, sequence_length)` if flash attention is used or `(batch_size, 1,\n",
        "                query_sequence_length, key_sequence_length)` if default attention is used.\n",
        "            output_attentions (`bool`, *optional*):\n",
        "                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
        "                returned tensors for more detail.\n",
        "            use_cache (`bool`, *optional*):\n",
        "                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n",
        "                (see `past_key_values`).\n",
        "            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n",
        "        \"\"\"\n",
        "        ##########################################################\n",
        "        residual = hidden_states\n",
        "\n",
        "        # norm을 적용하세요\n",
        "        hidden_states =\n",
        "\n",
        "        # Self Attention\n",
        "        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n",
        "            hidden_states=hidden_states,\n",
        "            attention_mask=attention_mask,\n",
        "            position_ids=position_ids,\n",
        "            past_key_value=past_key_value,\n",
        "            output_attentions=output_attentions,\n",
        "            use_cache=use_cache,\n",
        "            cache_position=cache_position,\n",
        "        )\n",
        "\n",
        "        # residual을 적용하세요.\n",
        "        hidden_states =\n",
        "\n",
        "        # Fully Connected\n",
        "        # norm, ffn, residual을 적용하세요.\n",
        "        residual = hidden_states\n",
        "        hidden_states =\n",
        "        hidden_states =\n",
        "        hidden_states =\n",
        "        ##########################################################\n",
        "\n",
        "        outputs = (hidden_states,)\n",
        "\n",
        "        if output_attentions:\n",
        "            outputs += (self_attn_weights,)\n",
        "\n",
        "        if use_cache:\n",
        "            outputs += (present_key_value,)\n",
        "\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "F_j6BTd0jStq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load model"
      ],
      "metadata": {
        "id": "oiKpeQRyHGxB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = \"lmsys/vicuna-7b-v1.5\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "model = LlamaForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    load_in_4bit=True,\n",
        "    device_map=\"auto\",\n",
        ")"
      ],
      "metadata": {
        "id": "VFeVg5DxFmtr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text generation"
      ],
      "metadata": {
        "id": "4ItRriX3HIy_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: generate any text from above code\n",
        "input_text = \"USER: Who is the best soccer player in the world?\\nASSISTANT:\"\n",
        "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.cuda()\n",
        "\n",
        "# Generate text\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    output = model.generate(input_ids, max_new_tokens=100)\n",
        "\n",
        "# Decode and print the generated text\n",
        "generated_text = tokenizer.batch_decode(output, skip_special_tokens=True)[0]\n",
        "print(generated_text)\n"
      ],
      "metadata": {
        "id": "A_Z2wAZE9-mc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}