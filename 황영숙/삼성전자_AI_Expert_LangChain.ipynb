{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "CMkF-i7dR7Fi"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/donghuna/AI-Expert/blob/main/%ED%99%A9%EC%98%81%EC%88%99/%EC%82%BC%EC%84%B1%EC%A0%84%EC%9E%90_AI_Expert_LangChain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LangChain 사용환경 설정\n",
        "\n",
        "*   Langchain과 langchain-openai 라이브러리 패키지 설치\n",
        "    -  pip install langchain\n",
        "    -  pip install langchain-openai\n",
        "* langchain python라이브러리로 프롬프트, 에이전트, 체인 관련 패키지 모음\n",
        "    - pip install langchainhub"
      ],
      "metadata": {
        "id": "6F_9saMZv9dz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain\n",
        "!pip install langchain-openai"
      ],
      "metadata": {
        "id": "qdId3Hi7wt5o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchainhub"
      ],
      "metadata": {
        "id": "VPCXmFHZxVO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import hub  # 다양한 prompt를 가져오거나 등록해서 사용할 수 있는 패키지\n",
        "\n",
        "from langchain_openai import ChatOpenAI  # for chat model\n",
        "from langchain_openai import OpenAI     #for LLM\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field  # 출력 포맷을 지정을 위해 사용하는 클래스와 메서드 집합\n",
        "from langchain.schema import (\n",
        "    AIMessage,\n",
        "    HumanMessage,\n",
        "    SystemMessage\n",
        ")\n",
        "from langchain_core.prompts import (\n",
        "    ChatPromptTemplate,\n",
        "    PromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        "    PipelinePromptTemplate,\n",
        "    FewShotPromptTemplate\n",
        ")\n",
        "from langchain_core.output_parsers import (\n",
        "    StrOutputParser,\n",
        "    CommaSeparatedListOutputParser,\n",
        "    JsonOutputParser\n",
        ")"
      ],
      "metadata": {
        "id": "etBC-AuEOoP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# OpenAI 인증키 OPEN_API_KEY 등록\n",
        "import os\n",
        "\n",
        "OPENAI_API_KEY = 'Your Key'\n",
        "organization_id = 'Your Key'\n"
      ],
      "metadata": {
        "id": "1TkaA6mFyfDk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ynwOJSHlyhYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 기본 LLM\n",
        "\n",
        "\n",
        "\n",
        "*   ChatOpenAI : 모델을 불러오는 클래스\n",
        "*   ChatPromptTemplate : prompt 템플릿을 제공해주는 클래스\n",
        "*   ChatPromptTemplate.from_template() : 문자열 형태의 템플릿을 인자로 받아, 해당 형식에 맞는 프롬프트 객체를 생성\n",
        "*   StrOutputParaser : 모델을 출력을 문자열 형태로 파싱하여 최종 결과를 반환\n",
        "*   invoke : chain을 실행하는 메서드\n",
        "*   batch : Bach Prompt를 구성하고 LLM 호출을 방법\n"
      ],
      "metadata": {
        "id": "ctAeRLB3ya37"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ChatOpenAI()를 이용하여 클라이언트 인스턴스를 생성할 때 OPENAI_API_KEY, organization_id, project_id를 입력하여 인스턴스를 생상하는 것으로 해봅니다.\n",
        "\n",
        "llm = OpenAI(api_key= OPENAI_API_KEY\n",
        "                , organization=organization_id\n",
        "                )"
      ],
      "metadata": {
        "id": "nUEZyxWr9Vpi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SkvGa5iS9rrt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt template 도입\n",
        "#prompt = ChatPromptTemplate.from_template(\"You are an expert in Generative AI . Answer the question. <Question>: {input}\")\n",
        "\n",
        "prompt = f\"\"\"\n",
        "  You are an expert in Generative AI . Answer the question.\n",
        "  <Question> : LangChain을 사용해서 언어모델로 서비스를 만들어보려고 해. LangChain API을 처음 사용하는 사람은 무엇부터 해야하지?\n",
        "\"\"\"\n",
        "\n",
        "#chain = prompt | llm\n",
        "llm.invoke(prompt)"
      ],
      "metadata": {
        "id": "TSMwqWigy7ou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Bach Prompt를 구성하고 LLM 호출을 방법: batch()"
      ],
      "metadata": {
        "id": "7Lpx6zAS-eMF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch\n",
        "prompts = [\n",
        "    \"What is top 5 Korean Street food?\",\n",
        "    \"What is most famous place in Seoul?\",\n",
        "    \"What is the popular K-Pop group?\"\n",
        "]\n",
        "llm.batch(prompts)\n"
      ],
      "metadata": {
        "id": "HMUbqQMb-dMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  토큰 사용량 추적\n",
        "\n",
        "   - Langchain 에서는 LLM 제공자가 제공하는 콜백함수를 통해서 LLM 호출건별 토큰 수를 카운트 할 수 있다.\n",
        "   -  openai에서 제공하는 callback 함수를 이용하여 토큰수를 출력하는 코드:\n",
        "\n",
        "    - \\\"with get_openai_callback() as callback:” 코드 블럭내에 LLM 호출 코드를 작성하면 LLM 을 호출한후에, callback에 호출에 대한 메타 정보를 담아서 리턴한다.\n",
        "\n"
      ],
      "metadata": {
        "id": "c--ToHD7AzRj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.callbacks import get_openai_callback\n",
        "\n",
        "with get_openai_callback() as callback:\n",
        "    prompt = \"LangChain을 사용해서 언어모델로 서비스를 만들어보려고 해. LangChain API을 처음 사용하는 사람은 무엇부터 해야하지? \"\n",
        "    llm.invoke(prompt)\n",
        "    print(callback)\n",
        "    print(\"Total Tokens:\",callback.total_tokens)\n"
      ],
      "metadata": {
        "id": "13yaeSM3AQam"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_model = ChatOpenAI(model=\"gpt-3.5-turbo-0125\"\n",
        "                , api_key= OPENAI_API_KEY\n",
        "                , organization=organization_id\n",
        "            )"
      ],
      "metadata": {
        "id": "m_T8GtGs7Kwk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chain으로 연결\n",
        "\n",
        "# prompt template 도입\n",
        "prompt = ChatPromptTemplate.from_template(\"You are an expert in Generative AI . Answer the question. <Question>: {input}\")\n",
        "prompt\n",
        "\n",
        "chain = prompt | chat_model\n",
        "\n",
        "chain.invoke({\"input\": \"반도체 제조 산업에서 거대언어 모델을 활용한 대표적인 사례를 알려줘  \"})"
      ],
      "metadata": {
        "id": "pi3svxE0z8mr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eE-hizNRcPJ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LangChain에서 Prompt를 입력 받아 LLM을 호출하는 방법: 직접 prompt를 전달하거나, ChatModel에서 HumanMessage에 Prompt를 할당하여 호출하는 방식 모두 가능\n",
        "prompt = \"What would be a good company name for a company that makes AI solutions for Semiconductor manufacturing industry?\"\n",
        "messages = [HumanMessage(content=prompt)]\n",
        "\n",
        "chat_model.invoke(prompt)\n",
        "\n",
        "# llm.invoke(prompt) 와 llm.invoke(messages) 는 동일한 요청을 LLM에 보낸다.\n",
        "llm.invoke(messages)"
      ],
      "metadata": {
        "id": "fsJ5--1Acf7i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompt Template\n",
        "\n",
        "*  PromptTemplate은 문자열 프롬프트에 대한 템플릿을 생성하는 데 사용\n",
        "*  기본적으로 템플릿 작성에는 Python의 str.format 구문을 사용"
      ],
      "metadata": {
        "id": "IEwv78xnAk3S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt template 사용, template 변수를 동적으로 입력받음\n",
        "prompt = PromptTemplate.from_template(\"What is a good name for a company that makes {product}?\")\n",
        "text=prompt.format(product=\"AI solution for manufacturing industry\")\n",
        "\n",
        "llm.invoke(text)"
      ],
      "metadata": {
        "id": "d1TyZJOCMhkw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt template 사용, template 변수를 동적으로 입력받고, 출력 결과를 output parser로 정리하여 결과만 출력\n",
        "\n",
        "prompt = PromptTemplate.from_template(\"What is a good name for a company that makes {product}?\")\n",
        "text=prompt.format(product=\"AI solution for manufacturing industry\")\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "chain = prompt | llm | output_parser\n",
        "chain.invoke(text)"
      ],
      "metadata": {
        "id": "2rKeZnFF_5az"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lh0Kj6qwBULr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 문자열 템플릿 결합\n",
        "\n",
        "template_text = \"안녕하세요, 제 이름은 {name}이고, 나이는 {age}살입니다.\"\n",
        "\n",
        "prompt_template = PromptTemplate.from_template(template_text)\n",
        "\n",
        "filled_prompt = prompt_template.format(name=\"홍길동\", age=30)\n",
        "\n",
        "print(filled_prompt, \"\\n\", \"-\"*20)\n",
        "\n",
        "combined_prompt = (\n",
        "              prompt_template\n",
        "              + PromptTemplate.from_template(\"\\n\\n아버지를 아버지라 부를 수 없습니다.\")\n",
        "              + \"\\n\\n{language}로 번역해주세요.\"\n",
        ")\n",
        "\n",
        "combined_text=combined_prompt.format(name=\"이강산\", age=30, language=\"영어\")\n",
        "print(combined_text, \"\\n\", \"-\"*20)\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
        "chain = combined_prompt | llm | StrOutputParser()\n",
        "chain.invoke({\"age\":30, \"language\":\"영어\", \"name\":\"홍길동\"})"
      ],
      "metadata": {
        "id": "H-NHk8weBXG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "## ChatPromptTemplate\n",
        "\n",
        "*  각 채팅 메시지는 content와 추가적인 매개변수 role이 연결됨\n",
        "*  OpenAI Chat Completions API 에서 채팅 메시지는 AI Assitant, Human,  System 역할과 연결\n",
        "\n",
        "* ChatPromptTemplate는 대화형 상황에서 여러 메시지 입력을 기반으로 단일 메시지 응답을 생성하는 데 사용.\n",
        "\n",
        "    - ChatPromptTemplate.from_messages : 메시지 리스트(혹은 튜플)을 기반으로 프롬프트를 구성함.\n",
        "    - ChatPromptTemplate.format_messages : 사용자의 입력을 프롬프트에 동적으로 삽입하여, 최종적으로 대화형 상황을 반영한 메시지 리스트를 생성\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "chat_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a helpful AI bot. Your name is {name}.\"),\n",
        "        (\"human\", \"Hello, how are you doing?\"),\n",
        "        (\"ai\", \"I'm doing well, thanks!\"),\n",
        "        (\"human\", \"{user_input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "messages = chat_template.format_messages(name=\"Bob\", user_input=\"What is your name?\")\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iAsEhPQmrOZS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts.chat import ChatPromptTemplate\n",
        "\n",
        "template = \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
        "human_template = \"{text}\"\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", template),\n",
        "    (\"human\", human_template),\n",
        "])\n",
        "\n",
        "messages=chat_prompt.format_messages(input_language=\"English\", output_language=\"Korean\", text=\"I am an AI expert. I love chatting with ChatGPT.\")\n",
        "print(messages)"
      ],
      "metadata": {
        "id": "5Slb-f9dezwq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_model.invoke(messages)"
      ],
      "metadata": {
        "id": "5o2ph3TJe9OD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
        "\n",
        "output_parser = CommaSeparatedListOutputParser()\n",
        "output_parser.parse(\"hi, bye\")\n",
        "# >> ['hi', 'bye']"
      ],
      "metadata": {
        "id": "ndZrMvPKf5u2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3lnnLTj-GDEQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# OpenAI Chat API 프롬프트 세팅\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"너는 월드 클래스 급의 문서 작성 전문가야. 한번 잘 써봐\"),\n",
        "    (\"user\", \"{input}\")\n",
        "])\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "chain = prompt | chat_model | output_parser\n",
        "\n",
        "# prompt 포함 생성\n",
        "chain.invoke({\"input\": \"LanhChain과 GPT-4를 이용하여 반도체 제조공정에 대한 교육 자료를 중학생이 이해할 수 있는 수준으로 작성해줘?\"})"
      ],
      "metadata": {
        "id": "PeySN8arPOBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LangChain Memory 사용\n",
        "\n",
        "* 대화 기록을 저장하고, 문맥으로 활용할 수 있음\n",
        "* langchain.memory 패키지에서 ChatMessageHistory import 하여 사용\n",
        "\n",
        "  - ChatMessageHistory() : 대화기록을 유지하기 위해 랭체인이 제공하는 클래스\n",
        "    - add_user_messgge() : 사용자 메시지를 대화기록 객체에 추가하기 위한 메서드\n",
        "    - add_user_messgge() : ai 가 답변한 대화내용을 대화기록 개첵에 추가하는 메서드\n"
      ],
      "metadata": {
        "id": "Cf74sKqYLrgk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ChatMessageHistory is used if you are managing memory outside of a chian directly\n",
        "from langchain.memory import ChatMessageHistory\n",
        "\n",
        "history = ChatMessageHistory()\n",
        "history.add_user_message(\"Where is the top 3 popular space for tourist in Seoul?\")\n",
        "aiMessage = llm.invoke(history.messages)\n",
        "history.add_ai_message(aiMessage.content)\n",
        "print(aiMessage.content)\n",
        "print(\"-\"*20)\n",
        "\n",
        "history.add_user_message(\"Which transport can I use to visit the places?\")\n",
        "#print(history.messages)\n",
        "\n",
        "aiMessage = chat_model.invoke(history.messages)\n",
        "history.add_ai_message(aiMessage.content)\n",
        "print(aiMessage.content)\n"
      ],
      "metadata": {
        "id": "_qv4L4-wJKFE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 여러분의 대화 시나리오를 만들고 코드로 만들어봅니다."
      ],
      "metadata": {
        "id": "gcjuLyhpNRNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JTmFTFvNNQ6v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# LLMChain 과 PromptTemplate 사용\n",
        "\n",
        " - LLM Chain은 프롬프트 템플릿을 LLM에 합쳐서 컴포넌트화 한 것으로,\n",
        " - 입력값으로 문자열을 넣으면 프롬프트 템플릿에 의해서 프롬프트가 자동으로 완성되고,\n",
        " - LLM 모델을 호출하여 텍스트 출력을 내주는 기능을 한다\n"
      ],
      "metadata": {
        "id": "CMkF-i7dR7Fi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install langchain-community langchain-core"
      ],
      "metadata": {
        "id": "uP5KCco766Hu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "#model = ChatOpenAI()\n",
        "prompt = PromptTemplate.from_template(\"{city}에서 가장 유명한 관광지 3개를 추천해줘\")\n",
        "chain = LLMChain(llm=llm, prompt=prompt)\n",
        "city = \"서울\"\n",
        "chain.run(city)\n",
        "\n"
      ],
      "metadata": {
        "id": "eyW0vUTqQ29D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Sequential Chain 사용\n",
        "\n",
        "* LLMChain 컴포넌트를 만들고 난 뒤, LLMChain들을 서로 연결하는 방법은 여러 Chain을 순차적으로 연결하게 해주는 컴포넌트인 SequentialChain을 사용하는 것이다.\n",
        "\n",
        "    - 아래 예제는 먼저 도시 이름 {city}을 입력 받은 후에, 첫번째 chain에서 그 도시의 유명한 관광지 이름을 {place}로 출력하도록 한다.\n",
        "    - 다음 두번째 chain에서는 관광지 이름 {place}를 앞의 chain에서 입력 받고, 추가적으로 교통편 {transport}를 입력받아서, 그 관광지까지 가기 위한 교통편 정보를 최종 출력으로 제공한다.\n",
        "\n",
        "    - 이때  첫번쨰 chain1 생성시에 output_key를 명시적으로 place로 지정해 준 것에 주의하자.\n"
      ],
      "metadata": {
        "id": "fcCELBm9SjDB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import SequentialChain\n",
        "\n",
        "prompt1 = PromptTemplate.from_template(\"{city}에서 가장 유명한 관광명소를 추가적인 설명없이 장소 이름만으로 추천해줘.\")\n",
        "prompt2 = PromptTemplate.from_template(\"{place}을 {transport} 편으로 가는 방법을 알려줄래?\")\n",
        "chain1 = LLMChain(llm=llm,prompt=prompt1,output_key=\"place\",verbose=True)\n",
        "chain2 = LLMChain(llm=llm,prompt=prompt2,verbose=True)\n",
        "\n",
        "chain = SequentialChain(chains=[chain1,chain2], input_variables=[\"city\",\"transport\"],verbose=True)\n",
        "chain.run({'city':'서울','transport':'전철'})\n"
      ],
      "metadata": {
        "id": "A3tddfUlRuhr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt Serialization\n",
        "\n",
        " 프롬프트를 수시로 변경해야 하는 경우에 프롬프트 템플릿을 별도의 파일로 저장하여, 애플리케이션에서 로드하여 사용하는 방법\n",
        "\n",
        "* 프롬프트 템플릿을 저장하는 방법\n",
        " - PrompteTemplate객체를 생성한후에 save(“{JSON 파일명}”)을 해주면 해당 템플릿이 파일로 저장\n",
        "\n",
        "* 저장된 프롬프트 템플릿을 Load하여 사용하기\n",
        "  - 저장된 템플릿은 langchain.prompts 패키지의 load_prompt 함수를 이용하여 다시 불러와 사용 가능\n"
      ],
      "metadata": {
        "id": "n7NLa30DWZBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate\n",
        "\n",
        "template = PromptTemplate.from_template(\n",
        "    \"Tell me a {adjective} {topic} in {city} in 300 characters.\"\n",
        ")\n",
        "template.save(\"template.json\")\n",
        "\n"
      ],
      "metadata": {
        "id": "pdkj85l4GkOQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import load_prompt\n",
        "\n",
        "loaded_template = load_prompt(\"template.json\")\n",
        "prompt = loaded_template.format(adjective=\"popular\", topic=\"cafe\", city=\"San francisco\")\n",
        "print(prompt)\n",
        "\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "chain = llm | output_parser\n",
        "chain.invoke(prompt)"
      ],
      "metadata": {
        "id": "oR0xke_yXToZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XI6LEiMSYUV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* FewShotPromptTemplate"
      ],
      "metadata": {
        "id": "25wGy8bnPxl-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "examples = [\n",
        "    {\n",
        "        \"question\": \"스티브 잡스와 아인슈타인 중 누가 더 오래 살았나요?\",\n",
        "        \"answer\": \"\"\"\n",
        "                  이 질문에 추가 질문이 필요한가요: 예.\n",
        "                  추가 질문: 스티브 잡스는 몇 살에 사망했나요?\n",
        "                  중간 답변: 스티브 잡스는 56세에 사망했습니다.\n",
        "                  추가 질문: 아인슈타인은 몇 살에 사망했나요?\n",
        "                  중간 답변: 아인슈타인은 76세에 사망했습니다.\n",
        "                  최종 답변은: 아인슈타인\n",
        "                  \"\"\",\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"네이버의 창립자는 언제 태어났나요?\",\n",
        "        \"answer\": \"\"\"\n",
        "                  이 질문에 추가 질문이 필요한가요: 예.\n",
        "                  추가 질문: 네이버의 창립자는 누구인가요?\n",
        "                  중간 답변: 네이버는 이해진에 의해 창립되었습니다.\n",
        "                  추가 질문: 이해진은 언제 태어났나요?\n",
        "                  중간 답변: 이해진은 1967년 6월 22일에 태어났습니다.\n",
        "                  최종 답변은: 1967년 6월 22일\n",
        "                  \"\"\",\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"율곡 이이의 어머니가 태어난 해의 통치하던 왕은 누구인가요?\",\n",
        "        \"answer\": \"\"\"\n",
        "                  이 질문에 추가 질문이 필요한가요: 예.\n",
        "                  추가 질문: 율곡 이이의 어머니는 누구인가요?\n",
        "                  중간 답변: 율곡 이이의 어머니는 신사임당입니다.\n",
        "                  추가 질문: 신사임당은 언제 태어났나요?\n",
        "                  중간 답변: 신사임당은 1504년에 태어났습니다.\n",
        "                  추가 질문: 1504년에 조선을 통치한 왕은 누구인가요?\n",
        "                  중간 답변: 1504년에 조선을 통치한 왕은 연산군입니다.\n",
        "                  최종 답변은: 연산군\n",
        "                  \"\"\",\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"올드보이와 기생충의 감독이 같은 나라 출신인가요?\",\n",
        "        \"answer\": \"\"\"\n",
        "                  이 질문에 추가 질문이 필요한가요: 예.\n",
        "                  추가 질문: 올드보이의 감독은 누구인가요?\n",
        "                  중간 답변: 올드보이의 감독은 박찬욱입니다.\n",
        "                  추가 질문: 박찬욱은 어느 나라 출신인가요?\n",
        "                  중간 답변: 박찬욱은 대한민국 출신입니다.\n",
        "                  추가 질문: 기생충의 감독은 누구인가요?\n",
        "                  중간 답변: 기생충의 감독은 봉준호입니다.\n",
        "                  추가 질문: 봉준호는 어느 나라 출신인가요?\n",
        "                  중간 답변: 봉준호는 대한민국 출신입니다.\n",
        "                  최종 답변은: 예\n",
        "                  \"\"\",\n",
        "    },\n",
        "]"
      ],
      "metadata": {
        "id": "R0LUNc18P34X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* PromptTemplate의 input_variables, template 을 이용하여 prompt를 example_prompt를 생성하고"
      ],
      "metadata": {
        "id": "Ku7e8NNWBA4t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_prompt = PromptTemplate(\n",
        "    input_variables=[\"question\", \"answer\"],\n",
        "    template=\"Question: {question}\\n{answer}\"\n",
        ")\n",
        "\n",
        "prompt = FewShotPromptTemplate(\n",
        "    examples=examples,\n",
        "    example_prompt=example_prompt,\n",
        "    suffix=\"Question: {question}\",\n",
        "    input_variables=[\"question\"], #프롬프트가 수신할 입력 항목\n",
        ")\n",
        "\n",
        "question = \"Google이 창립된 연도에 Bill Gates의 나이는 몇 살인가요?\"\n",
        "final_prompt = prompt.format(question=question)\n",
        "print(final_prompt)"
      ],
      "metadata": {
        "id": "yRmTHuFnP62M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = llm | StrOutputParser()\n",
        "\n",
        "print(chain.invoke(final_prompt))"
      ],
      "metadata": {
        "id": "4Xvi80f_QD7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iKdEGEHrWF4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 모델 파라미터 주기"
      ],
      "metadata": {
        "id": "5dNvPV1_ghQP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 생성 단계에서 주기\n",
        "params = {\n",
        "    \"temperature\": 0.7,\n",
        "    \"max_tokens\": 100,\n",
        "}\n",
        "\n",
        "kwargs = {\n",
        "    \"frequency_penalty\": 0.5,\n",
        "    \"presence_penalty\": 0.5,\n",
        "    \"stop\": [\"\\n\"]\n",
        "\n",
        "}\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", **params, model_kwargs = kwargs)\n",
        "\n",
        "question = \"태양계에서 가장 큰 행성은 무엇인가요?\"\n",
        "response = llm.invoke(input=question)\n",
        "\n",
        "print(response)"
      ],
      "metadata": {
        "id": "_8ggfDy-gfwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 호출 단계에서 주기\n",
        "params = {\n",
        "    \"temperature\": 0.7,\n",
        "    \"max_tokens\": 10,\n",
        "}\n",
        "\n",
        "response = llm.invoke(input=question, **params)\n",
        "\n",
        "print(response.content)"
      ],
      "metadata": {
        "id": "Skz0JiwegmAc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bind Method를 이용한 파라미터 추가 설정"
      ],
      "metadata": {
        "id": "88n2S2YNSYh-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# bind 메서드를 통해 모델 인스턴스에 파라미터를 추가로 제공할 수 있음\n",
        "# 특수한 상황에서만 일부 파라미터를 다르게 적용할 수 있도록 해줌.\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"이 시스템은 천문학 질문에 답변할 수 있습니다.\"),\n",
        "    (\"user\", \"{user_input}\"),\n",
        "])\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", max_tokens=100)\n",
        "\n",
        "messages = prompt.format_messages(user_input=\"태양계에서 가장 큰 행성은 무엇인가요?\")\n",
        "\n",
        "before_answer = model.invoke(messages)\n",
        "\n",
        "print(before_answer)\n",
        "\n",
        "chain = prompt | model.bind(max_tokens=10)\n",
        "\n",
        "after_answer = chain.invoke({\"user_input\": \"태양계에서 가장 큰 행성은 무엇인가요?\"})\n",
        "\n",
        "print(after_answer)"
      ],
      "metadata": {
        "id": "koUMFafQQqrk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parser\n",
        "\n",
        "*   CSV Parser\n",
        "  *  CommaSeparatedListOutputParser : 모델이 생성한 텍스트에서 쉼표로 구분된 항목을 추출\n",
        "  * get_format_instructions : 모델에 전달할 포멧 지시사항을 얻는 메서드\n",
        "*   Json Parser\n",
        "  * JsonOutputParser : 모델의 출력을 json으로 해석하며, Pydantic 모델에 맞게 데이터를 구조화하여 제공\n",
        "  * get_format_instructions : 모델에 전달할 포멧 지시사항을 얻는 메서드\n"
      ],
      "metadata": {
        "id": "1CzGj9q0QsK0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 프롬프트 조합과 Partial Prompt Template 사용\n",
        "\n",
        "* Partial Prompt Template\n",
        "  - 프롬프트의 변수 값을 한번에 채워 넣는 것이 아니라, 이 중 일부만 먼저 채워 넣고 나머지는 나중에 채워 넣는 방식\n",
        "\n",
        "  - 애플리케이션 코드내에서 템플릿 생성시 이미 알고 있는 값이 있을때 사용하기 편리한 방식\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KCR07yh2cx7c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 콤마로 분리된 형태로 결과를 출력하는 CommaSeparatedListOutputParser"
      ],
      "metadata": {
        "id": "g2XVp39WlMLB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_parser = CommaSeparatedListOutputParser()\n",
        "format_instructions = output_parser.get_format_instructions()\n",
        "\n",
        "print(format_instructions)"
      ],
      "metadata": {
        "id": "NQtIoHoZQsz8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = PromptTemplate(\n",
        "    template=\"List five {subject}.\\n{format_instructions}\",\n",
        "    input_variables=[\"subject\"],\n",
        "    partial_variables={\"format_instructions\": format_instructions},\n",
        ")\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
        "\n",
        "chain = prompt | llm | output_parser\n",
        "\n",
        "chain.invoke({\"subject\": \"popular Korean cusine\"})\n"
      ],
      "metadata": {
        "id": "Z5LcZrMwQ5MS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " - JSON 포맷을 사용하여 출력하는 JsonOutpoutParser\n",
        "    - PydanticOutputParser를 이용하여 출력을 변환하여 원하는 구조 정보로 정의하여 사용\n",
        "    - 원하는 구조정보 정의는 BaseModel과 Field를 이용하여 클래스로 정의하여 전달함"
      ],
      "metadata": {
        "id": "SsNV8fKmkxU-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#자료구조 정의\n",
        "class CusineRecipe(BaseModel):\n",
        "    name: str = Field(description=\"name of a cusine\")\n",
        "    recipe: str = Field(description=\"recipe to cook the cusine\")\n",
        "\n",
        "output_parser = JsonOutputParser(pydantic_object=CusineRecipe)\n",
        "\n",
        "format_instructions = output_parser.get_format_instructions()  # get_formet_instuctions() : 언어 모델이 출력해야 할 정보의 형식을 정의하는 지침을 제공\n",
        "\n",
        "print(format_instructions)"
      ],
      "metadata": {
        "id": "8BYxwaL5Q7wE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LangChain Agents"
      ],
      "metadata": {
        "id": "rF6yu6skkR5G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wikipedia"
      ],
      "metadata": {
        "id": "9UFPMIwMgYf1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "llm = ChatOpenAI(temperature=0,  # 창의성 0으로 설정\n",
        "                 model_name='gpt-4',  # 모델명\n",
        "                )\n",
        "\n",
        "from langchain.agents import load_tools\n",
        "from langchain.agents import initialize_agent\n",
        "from langchain.agents import AgentType\n",
        "\n",
        "tools = load_tools([\"wikipedia\", \"llm-math\"], llm=llm) #llm-math의 경우 나이 계산을 위해 사용\n",
        "agent = initialize_agent(tools,\n",
        "                         llm,\n",
        "                         agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
        "                         description='위키피이아에서 정보를 검색하고 계산이 필요할 때 사용',\n",
        "                         verbose=True)\n",
        "\n",
        "\n",
        "agent.run(\"gpt-4o는 언제 출시되었어?\")\n",
        "\n",
        "#agent.run(\"에드 시런이 태어난 해는? 2024년도 현재 에드 시런은 몇 살?\")"
      ],
      "metadata": {
        "id": "yqQNv7RHaq4c"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}