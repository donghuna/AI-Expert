{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/donghuna/AI-Expert/blob/main/%EC%96%91%EC%9D%B8%EC%88%9C/trpo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOLkEzexcvHm"
      },
      "source": [
        "# Trust Region Policy Optimization Practice"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nTDAymrIVwI"
      },
      "source": [
        "# -1. Setting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXX4sZB3c0SP"
      },
      "source": [
        "If you run in jupyter, turn\n",
        "\n",
        "```\n",
        "colab = False\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEtOZ-YkcwJG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fe397f5-1624-45bd-aaf6-135b7f988084"
      },
      "source": [
        "colab = True\n",
        "if colab:\n",
        "    !pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "    !apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "    !apt-get update > /dev/null 2>&1\n",
        "    !apt-get install cmake > /dev/null 2>&1\n",
        "    !pip install --upgrade setuptools 2>&1\n",
        "    !pip install ez_setup > /dev/null 2>&1\n",
        "    !pip install swig\n",
        "    !pip3 install box2d-py\n",
        "    !pip3 install gym[Box_2D]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (57.4.0)\n",
            "Collecting setuptools\n",
            "  Downloading setuptools-67.2.0-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: setuptools\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 57.4.0\n",
            "    Uninstalling setuptools-57.4.0:\n",
            "      Successfully uninstalled setuptools-57.4.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.9.0 requires jedi>=0.10, which is not installed.\n",
            "cvxpy 1.2.3 requires setuptools<=64.0.2, but you have setuptools 67.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed setuptools-67.2.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting swig\n",
            "  Downloading swig-4.1.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: swig\n",
            "Successfully installed swig-4.1.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting box2d-py\n",
            "  Downloading box2d-py-2.3.8.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.5/374.5 KB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: box2d-py\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.8-cp38-cp38-linux_x86_64.whl size=2834395 sha256=4ad057ee9418f2c54e2817b1eb4c257b9a33d499ffed1d487f41b6531bda18d6\n",
            "  Stored in directory: /root/.cache/pip/wheels/cc/4f/d6/44eb0a9e6fea384e58f19cb0c4125e46a23af2b33fe3a7e81c\n",
            "Successfully built box2d-py\n",
            "Installing collected packages: box2d-py\n",
            "Successfully installed box2d-py-2.3.8\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[Box_2D] in /usr/local/lib/python3.8/dist-packages (0.25.2)\n",
            "\u001b[33mWARNING: gym 0.25.2 does not provide the extra 'box_2d'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.8/dist-packages (from gym[Box_2D]) (1.21.6)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.8/dist-packages (from gym[Box_2D]) (0.0.8)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gym[Box_2D]) (2.2.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.8/dist-packages (from gym[Box_2D]) (6.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.8.0->gym[Box_2D]) (3.12.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HQocIk1dO8E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a3adb00-08f0-41ec-ed7b-599e99c00188"
      },
      "source": [
        "if colab:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    %cd /content/drive/MyDrive/drlcourse-main/day4/trpo\n",
        "    !ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/drlcourse-main/day4/trpo\n",
            "chap7_trpo_ppo.pdf  ppo_learning_curves  __pycache__\t  trpo.ipynb\n",
            "learning_curves     ppo.py\t\t snapshots\t  utils.py\n",
            "memory.py\t    ppo_snapshots\t trpo_full.ipynb  video\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7pnRhmQQcvHn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1183c2c9-b5e5-437d-cdb9-196bc3497d68"
      },
      "source": [
        "import numpy as np\n",
        "import time\n",
        "import csv\n",
        "import torch\n",
        "import os\n",
        "import copy\n",
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Independent\n",
        "from torch.distributions.normal import Normal\n",
        "from torch.optim import Adam\n",
        "from memory import OnPolicyMemory\n",
        "from utils import *"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/drive/MyDrive/drlcourse-main/day4/trpo/utils.py:86: DeprecationWarning: invalid escape sequence \\p\n",
            "  \"\"\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1ndOboccvHo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5c46bff-dc4a-4744-97da-da77b0cc4f2b"
      },
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print('current device : ', device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "current device :  cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0R0gU2pcvHp"
      },
      "source": [
        "# 0. Network Architectures"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NX1R4ZqNcvHq"
      },
      "source": [
        "class Actor(nn.Module):\n",
        "    def __init__(self, obs_dim, act_dim, hidden1, hidden2):\n",
        "        # actor f_\\phi(s)\n",
        "        super(Actor, self).__init__()\n",
        "        self.fc1 = nn.Linear(obs_dim, hidden1)\n",
        "        self.fc2 = nn.Linear(hidden1, hidden2)\n",
        "        # TODO_1 : Define two fc layers fc3 and fc4 for \\mu, \\sigma\n",
        "        self.fc3 = nn.Linear(hidden2, act_dim)  # for \\mu\n",
        "        self.fc4 = nn.Linear(hidden2, act_dim) # for \\sigma\n",
        "\n",
        "    def forward(self, obs):\n",
        "        x = torch.tanh(self.fc1(obs))\n",
        "        x = torch.tanh(self.fc2(x))\n",
        "\n",
        "        mu = self.fc3(x)\n",
        "        # Not \"sigma = self.fc4(x)\" - Why?\n",
        "        log_sigma = self.fc4(x)\n",
        "\n",
        "        sigma = torch.exp(log_sigma)\n",
        "\n",
        "        return mu, sigma\n",
        "\n",
        "    def log_prob(self, obs, act):\n",
        "        mu, sigma = self.forward(obs)\n",
        "        act_distribution = Independent(Normal(mu, sigma), 1)\n",
        "        log_prob = act_distribution.log_prob(act)\n",
        "\n",
        "        return log_prob\n",
        "\n",
        "class Critic(nn.Module):\n",
        "    # critic V(s ; \\theta)\n",
        "    def __init__(self, obs_dim, hidden1, hidden2):\n",
        "        super(Critic, self).__init__()\n",
        "        self.fc1 = nn.Linear(obs_dim, hidden1)\n",
        "        self.fc2 = nn.Linear(hidden1, hidden2)\n",
        "        # TODO_2 : Define fc3 layer, of which output is v(s)\n",
        "        # Hint: What should the output dim of fc3 layer be?\n",
        "        self.fc3 = nn.Linear(hidden2, 1)\n",
        "\n",
        "    def forward(self, obs):\n",
        "        x = torch.tanh(self.fc1(obs))\n",
        "        x = torch.tanh(self.fc2(x))\n",
        "        v = self.fc3(x)\n",
        "\n",
        "        return v"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4h65Dy-cvHq"
      },
      "source": [
        "# 1. Agent Definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-oN94SZcvHr"
      },
      "source": [
        "class TRPOAgent:\n",
        "    def __init__(\n",
        "                 self,\n",
        "                 obs_dim,\n",
        "                 act_dim,\n",
        "                 hidden1=64,\n",
        "                 hidden2=32,\n",
        "                 ):\n",
        "\n",
        "        self.obs_dim = obs_dim\n",
        "        self.act_dim = act_dim\n",
        "        self.hidden1 = hidden1\n",
        "        self.hidden2 = hidden2\n",
        "\n",
        "        self.pi = Actor(obs_dim, act_dim, hidden1, hidden2).to(device)\n",
        "        self.V = Critic(obs_dim, hidden1, hidden2).to(device)\n",
        "\n",
        "    def act(self, obs, deterministic=False):\n",
        "        obs = torch.tensor(obs, dtype=torch.float).to(device)\n",
        "        with torch.no_grad():\n",
        "            # TODO_3 : Get mu and sigma from actor network\n",
        "            mu, sigma = self.pi(obs)\n",
        "            if deterministic:\n",
        "                action = mu\n",
        "                log_prob = None\n",
        "                val = None\n",
        "            else:\n",
        "                # TODO_4 : Following TA, get action from action distribution N(mu, sigma)\n",
        "                act_distribution = Independent(Normal(mu, sigma), 1)\n",
        "                action = act_distribution.sample()\n",
        "\n",
        "                log_prob = act_distribution.log_prob(action)\n",
        "\n",
        "                # TODO_5 : Get v(s) from critic network\n",
        "                val = self.V(obs)\n",
        "                log_prob = log_prob.cpu().numpy()\n",
        "                val = val.cpu().numpy()\n",
        "\n",
        "        action = action.cpu().numpy()\n",
        "\n",
        "        return action, log_prob, val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xweZIpcicvHr"
      },
      "source": [
        "# 2. Policy & Value Function Approximation Update"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-aJyKaVEyds8"
      },
      "source": [
        "Objective:\n",
        "\\begin{align*}\n",
        "g = \\nabla_\\phi J(\\phi) &\\approx \\nabla_\\phi \\mathbb{E}_{s \\sim \\rho_{\\phi_{\\text{old}}}, a \\sim \\pi_{\\phi_{\\text{old}}}}\\left( \\frac{\\pi_{\\phi}(s, a)}{\\pi_{\\phi_{\\text{old}}}(s, a)} A^{\\pi_{\\phi_{\\text{old}}}}(s, a) \\right) \\\\\n",
        "&\\approx \\nabla_\\phi \\frac{1}{N} \\sum_{i = 1}^N \\left( \\frac{\\pi_{\\phi}(s_i, a_i)}{\\pi_{\\phi_{\\text{old}}}(s_i, a_i)} \\hat A(s_i, a_i) \\right).\n",
        "\\end{align*} \\\\\n",
        "Since we take into account approximated trust region constraint, the final update direction is\n",
        "\\begin{equation*}\n",
        "s = H^{-1}g, \\quad H s = g,\n",
        "\\end{equation*}\n",
        " and the stepsize is\n",
        " \\begin{equation*}\n",
        "\\alpha = \\sqrt{\\frac{2\\delta}{g^\\top H^{-1} g}}.\n",
        " \\end{equation*}\n",
        " Thus, the update is done as follows:\n",
        " \\begin{gather*}\n",
        " \\phi_{\\text{old}} \\longleftarrow \\phi, \\\\\n",
        "\\phi \\longleftarrow \\phi + \\alpha \\cdot s.\n",
        " \\end{gather*}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nowAebz5cvHs"
      },
      "source": [
        "def update(agent, memory, critic_optim, delta, num_updates):\n",
        "\n",
        "    batch = memory.load()\n",
        "\n",
        "    states = torch.Tensor(batch['state']).to(device)\n",
        "    actions = torch.Tensor(batch['action']).to(device)\n",
        "    target_v = torch.Tensor(batch['val']).to(device)\n",
        "    A = torch.Tensor(batch['A']).to(device)\n",
        "    old_log_probs = torch.Tensor(batch['log_prob']).to(device)\n",
        "\n",
        "    for _ in range(num_updates):\n",
        "        ################\n",
        "        # train critic #\n",
        "        ################\n",
        "        # TODO_6 : Implement training code for critic network\n",
        "        # 1) Get output of critic network\n",
        "        # 2) Define critic loss (MSE)\n",
        "        # 3) Three lines needed for backprop\n",
        "        out = agent.V(states)\n",
        "        critic_loss = torch.mean((out - target_v)**2)\n",
        "\n",
        "        critic_optim.zero_grad()\n",
        "        critic_loss.backward()\n",
        "        critic_optim.step()\n",
        "\n",
        "        ###################\n",
        "        # policy gradient #\n",
        "        ###################\n",
        "        log_probs = agent.pi.log_prob(states, actions)\n",
        "        # TODO_7 : Calculate probabiltiy ratio, \\pi(a_t | s_t ; \\phi) / \\pi(a_t | s_t ; \\phi_old) and actor loss\n",
        "        # Hint : Use log_probs and old_log_probs\n",
        "        prob_ratio = torch.exp(log_probs - old_log_probs)\n",
        "        actor_loss = torch.mean(prob_ratio * A)\n",
        "\n",
        "        # TODO_8 : Calculate gradient of loss\n",
        "        # Hint : Use torch.autograd.grad()\n",
        "        loss_grad = torch.autograd.grad(actor_loss, agent.pi.parameters())\n",
        "\n",
        "        # flatten gradients of params\n",
        "        g = torch.cat([grad.view(-1) for grad in loss_grad]).data\n",
        "\n",
        "        s = cg(fisher_vector_product, g, agent.pi, states)\n",
        "\n",
        "        sAs = torch.sum(fisher_vector_product(s, agent.pi, states) * s, dim=0, keepdim=True)\n",
        "        step_size = torch.sqrt(2 * delta / sAs)[0]    # stepsize : move as far as possible within trust region\n",
        "        step = step_size * s\n",
        "\n",
        "        old_actor = Actor(agent.obs_dim, agent.act_dim, agent.hidden1, agent.hidden2).to(device)\n",
        "        old_actor.load_state_dict(agent.pi.state_dict())\n",
        "\n",
        "        params = flat_params(agent.pi)\n",
        "\n",
        "        # TODO_9, 10 : Implement line search algorithm in utils.py\n",
        "        backtracking_line_search(old_actor, agent.pi, actor_loss, g,\n",
        "                                 old_log_probs, params, step, delta, A, states, actions)    # line search => for improvement guarantee!\n",
        "\n",
        "    return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EpuMGH3fcvHt"
      },
      "source": [
        "def evaluate(agent, env, num_episodes=5):\n",
        "\n",
        "    scores = np.zeros(num_episodes)\n",
        "    for i in range(num_episodes):\n",
        "        obs = env.reset()\n",
        "        done = False\n",
        "        score = 0.\n",
        "        while not done:\n",
        "            action = agent.act(obs, deterministic=True)[0]\n",
        "            obs, rew, done, _ = env.step(action)\n",
        "            score += rew\n",
        "\n",
        "        scores[i] = score\n",
        "    avg_score = np.mean(scores)\n",
        "    std_score = np.std(scores)\n",
        "\n",
        "    return avg_score, std_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfzZ3SU-cvHt"
      },
      "source": [
        "# 3. Training!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLzSmhDocvHu"
      },
      "source": [
        "def train(env, agent, max_iter, gamma=0.99, lr=3e-4, lam=0.95, delta=1e-3, steps_per_epoch=10000, eval_interval=10000, snapshot_interval=10000):\n",
        "\n",
        "    obs_dim = env.observation_space.shape[0]\n",
        "    act_dim = env.action_space.shape[0]\n",
        "    max_ep_len = env._max_episode_steps\n",
        "    memory = OnPolicyMemory(obs_dim, act_dim, gamma, lam, lim=steps_per_epoch)\n",
        "    test_env = gym.make(env.unwrapped.spec.id)\n",
        "    critic_optim = Adam(agent.V.parameters(), lr=lr)\n",
        "\n",
        "    save_path = './snapshots/'\n",
        "    os.makedirs(save_path, exist_ok=True)\n",
        "    os.makedirs('./learning_curves/', exist_ok=True)\n",
        "    log_file = open('./learning_curves/res.csv',\n",
        "                    'w',\n",
        "                    encoding='utf-8',\n",
        "                    newline=''\n",
        "                   )\n",
        "    logger = csv.writer(log_file)\n",
        "    num_epochs = max_iter // steps_per_epoch\n",
        "    total_t = 0\n",
        "    begin = time.time()\n",
        "    for epoch in range(num_epochs):\n",
        "        # start agent-env interaction\n",
        "        state = env.reset()\n",
        "        step_count = 0\n",
        "        ep_reward = 0\n",
        "\n",
        "        for t in range(steps_per_epoch):\n",
        "            # TODO_11 : Collect transition samples by executing the policy\n",
        "            # 1) Choose your agent's action first\n",
        "            # 2) Execute the action and get next state, reward, done signal\n",
        "            action, log_prob, v = agent.act(state)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "            memory.append(state, action, reward, v, log_prob)\n",
        "\n",
        "            ep_reward += reward\n",
        "            step_count += 1\n",
        "\n",
        "            if (step_count == max_ep_len) or (t == steps_per_epoch - 1):\n",
        "                # termination of env by env wrapper, or by truncation due to memory size\n",
        "                s_last = torch.tensor(next_state, dtype=torch.float).to(device)\n",
        "                v_last = agent.V(s_last).item()\n",
        "                memory.compute_values(v_last)\n",
        "            elif done:\n",
        "                # episode done as the agent reach a terminal state\n",
        "                v_last = 0.0\n",
        "                memory.compute_values(v_last)\n",
        "\n",
        "            state = next_state\n",
        "\n",
        "            if done:\n",
        "                state = env.reset()\n",
        "                step_count = 0\n",
        "                ep_reward = 0\n",
        "\n",
        "            if total_t % eval_interval == 0:\n",
        "                avg_score, std_score = evaluate(agent, test_env, num_episodes=5)\n",
        "                elapsed_t = time.time() - begin\n",
        "                print('[elapsed time : {:.1f}s| iter {}] score = {:.2f}'.format(elapsed_t, total_t, avg_score), u'\\u00B1', '{:.4f}'.format(std_score))\n",
        "                evaluation_log = [t, avg_score, std_score]\n",
        "                logger.writerow(evaluation_log)\n",
        "\n",
        "\n",
        "            if total_t % snapshot_interval == 0:\n",
        "                snapshot_path = save_path + 'iter{}_'.format(total_t)\n",
        "                # save weight & training progress\n",
        "                save_snapshot(agent, snapshot_path)\n",
        "\n",
        "            total_t += 1\n",
        "\n",
        "        # train agent at the end of each epoch\n",
        "        update(agent, memory, critic_optim, delta, num_updates=1)\n",
        "\n",
        "    log_file.close()\n",
        "    return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcUXG4EfGt0Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38edf046-2096-46f2-a59b-f6e372a7d2f4"
      },
      "source": [
        "# Let's move to robotic environment!\n",
        "!pip install pybullet"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pybullet\n",
            "  Downloading pybullet-3.2.5-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (91.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.7/91.7 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pybullet\n",
            "Successfully installed pybullet-3.2.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkQr9w8lcvHu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "051f0bf3-3004-4194-8975-ad990a1b3da5"
      },
      "source": [
        "import pybullet_envs\n",
        "\n",
        "env_id = 'HopperBulletEnv-v0'\n",
        "\n",
        "env = gym.make(env_id)\n",
        "obs_dim = env.observation_space.shape[0]\n",
        "act_dim = env.action_space.shape[0]\n",
        "print('observation space dim. : {} / action space dim. : {}'.format(obs_dim, act_dim))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "observation space dim. : 15 / action space dim. : 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/gym/envs/registration.py:440: UserWarning: \u001b[33mWARN: The `registry.env_specs` property along with `EnvSpecTree` is deprecated. Please use `registry` directly as a dictionary instead.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.8/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aS6XCK63cvHv"
      },
      "source": [
        "agent = TRPOAgent(obs_dim, act_dim, hidden1=128, hidden2=128)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubINos42Bqt6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fececc4c-5401-4fdd-b0c7-49f528703b4c"
      },
      "source": [
        "next(agent.pi.parameters()).is_cuda"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9K0q2WTDcvHv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "outputId": "fc8903c5-02e8-4207-9547-46c46bc7f355"
      },
      "source": [
        "train(env, agent, max_iter=20000000, gamma=0.99, lr=5e-4, lam=0.95, delta=1e-3, steps_per_epoch=10000, eval_interval=500000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[elapsed time : 0.7s| iter 0] score = 46.02 ± 14.9625\n",
            "[elapsed time : 1029.7s| iter 500000] score = 23.36 ± 0.4444\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-d11066612829>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20000000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.99\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5e-4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlam\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.95\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-17-459f89befab3>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(env, agent, max_iter, gamma, lr, lam, delta, steps_per_epoch, eval_interval, snapshot_interval)\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0;31m# 2) Execute the action and get next state, reward, done signal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \"\"\"\n\u001b[1;32m     59\u001b[0m         observation, reward, terminated, truncated, info = step_api_compatibility(\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m             \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/gym/wrappers/order_enforcing.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_reset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mResetNeeded\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot call env.step() before calling env.reset()\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/gym/wrappers/step_api_compatibility.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \"\"\"\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mstep_returns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_step_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstep_to_new_api\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_returns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/gym/wrappers/env_checker.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0menv_step_passive_checker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pybullet_envs/gym_locomotion_envs.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m     67\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscene\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrobot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalc_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# also calculates self.joints_at_limit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     self._alive = float(\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pybullet_envs/robot_locomotors.py\u001b[0m in \u001b[0;36mcalc_state\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mbody_pose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrobot_body\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mparts_xyz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxyz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     self.body_xyz = (parts_xyz[0::3].mean(), parts_xyz[1::3].mean(), body_pose.xyz()[2]\n\u001b[0m\u001b[1;32m     46\u001b[0m                     )  # torso z is more informative than mean z\n\u001b[1;32m     47\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbody_real_xyz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbody_pose\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxyz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_mean\u001b[0;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0mrcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_count_reduce_items\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeepdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mrcount\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mwhere\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mumr_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrcount\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m         \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Mean of empty slice.\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRuntimeWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZBv4n0dcvHv"
      },
      "source": [
        "# 4. Watch how your agent solve the task!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "from gym.wrappers.record_video import RecordVideo\n",
        "import os\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode"
      ],
      "metadata": {
        "id": "eCaklLkbszBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib\n",
        "import matplotlib.animation as animation\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import HTML\n",
        "\n",
        "def display_video(frames, framerate=30):\n",
        "    height, width, _ = frames[0].shape\n",
        "    dpi = 70\n",
        "    orig_backend = matplotlib.get_backend()\n",
        "    matplotlib.use('Agg')  # Switch to headless 'Agg' to inhibit figure rendering.\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(width / dpi, height / dpi), dpi=dpi)\n",
        "    matplotlib.use(orig_backend)  # Switch back to the original backend.\n",
        "    ax.set_axis_off()\n",
        "    ax.set_aspect('equal')\n",
        "    ax.set_position([0, 0, 1, 1])\n",
        "    im = ax.imshow(frames[0])\n",
        "    def update(frame):\n",
        "      im.set_data(frame)\n",
        "      return [im]\n",
        "    interval = 1000/framerate\n",
        "    anim = animation.FuncAnimation(fig=fig, func=update, frames=frames,\n",
        "                                   interval=interval, blit=True, repeat=False)\n",
        "    return HTML(anim.to_html5_video())"
      ],
      "metadata": {
        "id": "oqEYzPS2OoqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OoEoj-R-cvHw"
      },
      "source": [
        "env = gym.make('HopperBulletEnv-v0')\n",
        "\n",
        "os.makedirs('./video',exist_ok=True)\n",
        "\n",
        "#env = RecordVideo(env=env,video_folder='./video')\n",
        "\n",
        "load_model(agent, './snapshots/hopper_expert.tar', device)\n",
        "\n",
        "frames = []\n",
        "obs = env.reset()\n",
        "\n",
        "done = False\n",
        "score = 0.\n",
        "#env.start_video_recorder()\n",
        "while not done:\n",
        "    frame = env.render(mode='rgb_array')\n",
        "    frames.append(frame)\n",
        "    #env.render()\n",
        "    obs, rew, done, _ = env.step(agent.act(obs, deterministic=True)[0])\n",
        "    score += rew\n",
        "print('score : ', score)\n",
        "#env.close_video_recorder()\n",
        "display_video(frames=frames, framerate=30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tiZ1VUHUHtfd"
      },
      "source": [
        "# Proximal Policy Optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-U63srv3xcZE"
      },
      "source": [
        "In contrast to TRPO, PPO uses the following simple $1^{\\text{st}}$-order objective!\n",
        "\\begin{equation*}\n",
        "L(\\phi) \\approx \\frac{1}{N} \\sum_{i = 1}^N \\min\\left( r_i(\\phi)\\hat A_i, \\text{clip}(r_i(\\phi), 1 - \\varepsilon, 1 + \\varepsilon) \\hat A_i  \\right).\n",
        "\\end{equation*}\n",
        "While we performed complex parameter updates in TRPO, we just build the above loss and use popular optimizers provided by PyTorch..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXmOA2yWHs2l"
      },
      "source": [
        "from ppo import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OCKvpG4RSS3"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GeUrio16HxSE"
      },
      "source": [
        "ppo_agent = PPOAgent(obs_dim, act_dim, hidden1=128, hidden2=128, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def ppo_update(agent, memory, optimizer, epsilon, num_updates=1, device='cuda'):\n",
        "\n",
        "    batch = memory.load()\n",
        "    states = torch.Tensor(batch['state']).to(device)\n",
        "    actions = torch.Tensor(batch['action']).to(device)\n",
        "    target_v = torch.Tensor(batch['val']).to(device)\n",
        "    A = torch.Tensor(batch['A']).to(device)\n",
        "    old_log_probs = torch.Tensor(batch['log_prob']).to(device)\n",
        "\n",
        "    for _ in range(num_updates):\n",
        "        ################\n",
        "        # train critic #\n",
        "        ################\n",
        "        log_probs, ent = agent.pi.compute_log_prob(states, actions)\n",
        "\n",
        "        # compute prob ratio\n",
        "        # $\\frac{\\pi(a_t | s_t ; \\theta)}{\\pi(a_t | s_t ; \\theta_\\text{old})}$\n",
        "        r = torch.exp()\n",
        "        # construct clipped loss\n",
        "        # $r^\\text{clipped}_t(\\theta) = \\text{clip}(r_t(\\theta), 1 - \\epsilon, 1 + \\epsilon)$\n",
        "        clipped_r = torch.clamp()\n",
        "        # surrogate objective for each $t$\n",
        "        # $\\min \\{ r_t(\\theta) \\hat{A}_t, r^\\text{clipped}_t(\\theta) \\hat{A}_t \\}$\n",
        "        single_step_obj = torch.min()\n",
        "        pi_loss = -torch.mean(single_step_obj)\n",
        "\n",
        "        v = agent.V(states)\n",
        "        V_loss = torch.mean((v - target_v) ** 2)\n",
        "        ent_bonus = torch.mean(ent)\n",
        "\n",
        "        loss = pi_loss + 0.5 * V_loss - 0.01 * ent_bonus\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    return"
      ],
      "metadata": {
        "id": "7cZk8FpKLHNW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3WQ8D5BiNmfd"
      },
      "source": [
        "env = gym.make('HopperBulletEnv-v0')\n",
        "test_env = gym.make('HopperBulletEnv-v0')\n",
        "ppo_train(env, test_env, ppo_agent, max_iter=500000, gamma=0.99, lr=3e-4, lam=0.95, delta=1e-3, epsilon=0.2, steps_per_epoch=10000, eval_interval=10000)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}