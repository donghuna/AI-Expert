{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/donghuna/AI-Expert/blob/main/%EC%96%91%EC%9D%B8%EC%88%9C/tabular_mdp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T00xGXPEKRyb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea72c737-570b-4f63-b8f1-bceb0c072b15"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGRcwjDRKTL_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e10d48fd-3d7b-48ae-e0f3-321f5da48bb6"
      },
      "source": [
        "%cd /content/drive/MyDrive/day1_tabular_mdp\n",
        "%ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/day1_tabular_mdp\n",
            "chap0_gym_intro.pdf    gridworld.py     pendulum.py   tabular_mdp_full.ipynb\n",
            "chap1_tabular_mdp.pdf  gym_intro.ipynb  \u001b[0m\u001b[01;34m__pycache__\u001b[0m/  tabular_mdp.ipynb\n",
            "gridworld.png          gym_test.py      solvers.py    utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlsKwmeOKO4n"
      },
      "source": [
        "import numpy as np\n",
        "import time\n",
        "from utils import *"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axmEbpStKO4p"
      },
      "source": [
        "# 1. Prerequisites"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lj0kbO81KO4p"
      },
      "source": [
        "## 1.1 Toy Example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxd3IDz8KO4p"
      },
      "source": [
        "R = np.array([[-2.0, -0.5],\n",
        "              [-1.0, -3.0]])\n",
        "\n",
        "P = np.array([[0.75, 0.25],\n",
        "              [0.75, 0.25],\n",
        "              [0.25, 0.75],\n",
        "              [0.25, 0.75]])\n",
        "\n",
        "gamma = 0.9"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVJRoOPQKO4q"
      },
      "source": [
        "## 1.2 Define Q-function and Greedy policy\n",
        "First define Q-function from the value function:\n",
        "$$Q(s, a) = r(s, a) + \\gamma \\sum_{s^\\prime} p(s^\\prime \\mid s, a) v(s^\\prime)$$\n",
        "\n",
        "Greedy policy from Q-function: $\\pi_{\\text{greedy}}(s) = \\arg\\max_a Q(s, a)$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4XIq9eOKO4q"
      },
      "source": [
        "def q_ftn(P, R, gamma, v):\n",
        "    \"\"\"\n",
        "    given v, get corresponding q\n",
        "    hint : v's shape = (|S|, 1)\n",
        "           P's shape = (|S| * |A|, |S|)\n",
        "           R's shape = (|S|, |A|)\n",
        "           [Fill here]'s shape = (|S| * |A|, 1)\n",
        "           numpy.matmul(A, x) returns a matrix multiplication result A * x.\n",
        "    S 상태에서 A action을 했을때 받을 수 있는 최대 누적 reward 값\n",
        "\n",
        "    \"\"\"\n",
        "    # TODO 1: Complete the following line.\n",
        "    # return R + gamma * np.matmul(P, v)\n",
        "    return R + gamma * np.reshape(P@v, newshape=R.shape, order='F')\n",
        "\n",
        "def greedy(P, R, gamma, v):\n",
        "    \"\"\"\n",
        "    construct greedy policy by pi(s) = argmax_a q(s, a)\n",
        "    \"\"\"\n",
        "    # TODO 2: Complete the following line.\n",
        "    q = q_ftn(P, R, gamma, v)\n",
        "    pi = np.argmax(q, axis=1)\n",
        "\n",
        "    return pi"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Value Iteration"
      ],
      "metadata": {
        "id": "4edivHKSgPQ_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Value iteration iteratively applies the *Bellman Optimality Operator*\n",
        "\n",
        "\\begin{equation*}\n",
        "\\mathcal{T}v\\,(s) := \\max_{a \\in \\mathcal{A}}\\bigg[ r\\,(s,a) + \\gamma \\sum_{s'\\in\\mathcal{S}} p\\,(s'|s,a)\\;v\\,(s') \\bigg]\n",
        "\\end{equation*}"
      ],
      "metadata": {
        "id": "2mIXHG5KgeYJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From calculated $v^*$, we can calculate $\\pi^*$ by Q-function\n",
        "\n",
        "$$ \\pi^* := \\arg\\max_{a\\in\\mathcal{A}} Q^*(s,a) $$"
      ],
      "metadata": {
        "id": "LOJXmwOYvE4e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bellman_update(P, R, gamma, v):\n",
        "    \"\"\"\n",
        "    implementation of one-step Bellman update\n",
        "    return : vector of shape (|S|, 1) which corresponds to Tv, where T is Bellman operator\n",
        "    \"\"\"\n",
        "\n",
        "    q = q_ftn(P, R, gamma, v)\n",
        "    v_next = np.max(q, axis=1, keepdims=True)   # computation of Bellman operator Tv\n",
        "\n",
        "    return v_next"
      ],
      "metadata": {
        "id": "_CZZeCk3QAdD"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_result(v, pi, mode='discount'):\n",
        "    print('+========== Result ==========+')\n",
        "    nS = v.size\n",
        "    S = range(nS)\n",
        "\n",
        "    print('optimal value function : ')\n",
        "    # print a given value function\n",
        "    for s in S:\n",
        "        print('v(s{}) = {}'.format(s + 1, v[s, 0]))\n",
        "\n",
        "    print('optimal policy : ')\n",
        "    # print a given policy\n",
        "    for s in S:\n",
        "        print('pi(s{}) = a{}'.format(s + 1, pi[s] + 1))\n",
        "    return"
      ],
      "metadata": {
        "id": "InpAv73oRIbp"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqQy3CXaKO4r",
        "outputId": "f4d018c1-ce78-4c86-d297-46bcdc6a44e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "EPS = 1e-6\n",
        "nS, nA = R.shape\n",
        "# Initialize v\n",
        "v = np.zeros(shape=(nS, 1), dtype=float)\n",
        "\n",
        "# Value Iteration\n",
        "count = 0\n",
        "start = time.time()\n",
        "while True:\n",
        "    v_next = bellman_update(P, R, gamma, v)\n",
        "    # TODO 3: Fill the blank below.\n",
        "    if np.linalg.norm(v_next - v, ord=np.inf) < EPS:\n",
        "        break\n",
        "    v = v_next\n",
        "    count += 1\n",
        "print('Iteration terminated in {} steps, Ellapsed time {:4f} sec\\n'.format(count, time.time() - start))\n",
        "\n",
        "# TODO 4: Construct policy obtained v^* .\n",
        "pi = greedy(P, R, gamma, v)\n",
        "print_result(v, pi)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration terminated in 129 steps, Ellapsed time 0.008169 sec\n",
            "\n",
            "+========== Result ==========+\n",
            "optimal value function : \n",
            "v(s1) = -7.327576823826019\n",
            "v(s2) = -7.672404410032915\n",
            "optimal policy : \n",
            "pi(s1) = a2\n",
            "pi(s2) = a1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "+========== Result ==========+  \n",
        "optimal value function :   \n",
        "v(s1) = -7.327576823826019 // s1에 있을 때의 기대치  \n",
        "v(s2) = -7.672404410032915  \n",
        "s1에 있는 것이 좀 더 좋음을 의미  \n",
        "optimal policy :   \n",
        "pi(s1) = a2 // S1에 있다면 A2를 취해라  \n",
        "pi(s2) = a1 // S2에 있다면 A1을 취해라  \n"
      ],
      "metadata": {
        "id": "tRkDufLWSK4i"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkDGsoV0KO4s"
      },
      "source": [
        "# 2. Policy Iteration\n",
        "\n",
        "Policy Iteration iterates between these 2 steps\n",
        "\n",
        "\n",
        "*   Policy Evaluation : calculate policy value function $v^\\pi$\n",
        "*   Policy Update : from calculated $v^\\pi$, update $\\pi$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwkLn2_dapF4"
      },
      "source": [
        "## 2.1 Policy Evaluation: Get induced dynamics\n",
        "\n",
        "Let, start from getting $\\hat{P}$ and $\\hat{R}$\n",
        "\n",
        "Suppose 2 states, 2 actions, and $\\pi = [a_1, a_0]$\n",
        "\n",
        "We should select row vector of $P$\n",
        "\\begin{equation*}\n",
        "p(\\cdot|s_0,a_0)\\\\\n",
        "p(\\cdot|s_1,a_0) \\rightarrow p(\\cdot|s_1,\\pi(s_1))\\\\\n",
        "p(\\cdot|s_0,a_1) \\rightarrow p(\\cdot|s_0,\\pi(s_0))\\\\\n",
        "p(\\cdot|s_1,a_1)\\\\\n",
        "\\end{equation*}\n",
        "\n",
        "So, $P^\\pi$ is a rearranged submatrix of $P$\n",
        "\n",
        "$$P^{\\pi} = P\\;[\\;[2,1], \\; : \\;]$$\n",
        "\n",
        "Note that row number can be obtained via:\n",
        "\n",
        "$$2 = 0(s_0) + 2 \\cdot 1(\\pi(s_0))$$\n",
        "\n",
        "$$1 = 1(s_1) + 2 \\cdot 0(\\pi(s_1))$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6egY9JvUKO4s"
      },
      "source": [
        "def induced_dynamic(nS, P, R, pi):\n",
        "    \"\"\"\n",
        "    given policy pi, compute induced dynamic P^pi & R^pi\n",
        "    \"\"\"\n",
        "    # TODO 5: Compute 'row number' with formulations above.\n",
        "    rows =\n",
        "\n",
        "    # TODO 6: Get P_pi and R_pi from P and R.\n",
        "    P_pi =\n",
        "    R_pi = np.array([[\"\"\"blank\"\"\"]] for s in range(nS)])\n",
        "\n",
        "    return P_pi, R_pi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Policy Evaluation: Calculate $v^\\pi$\n",
        "With calculated induced dynamics, we can compute $v^\\pi$\n",
        "\n",
        "\n",
        "\\begin{equation*}\n",
        "v^\\pi = (I - \\gamma P^\\pi)^{-1} R^{\\pi}\n",
        "\\end{equation*}\n",
        "\n",
        "\n",
        "To solve the above equation, you may use ```numpy.linalg.solve(A, b)```.\n",
        "This returns a solution of the linear equations $A x = b$, i.e., $x = A^{-1}b$.\n",
        "\n",
        "Note that $v^\\pi$ can be attained with iterating the Bellman operator $\\mathcal{T}^\\pi$\n",
        "\n",
        "$$\\mathcal{T}^\\pi v := R^\\pi +\\gamma P^\\pi v^\\pi $$"
      ],
      "metadata": {
        "id": "1W6IxcU5Jxxt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_policy(nS, P, R, gamma, pi):\n",
        "    \"\"\"\n",
        "    policy evaluation\n",
        "    \"\"\"\n",
        "    P_pi, R_pi = induced_dynamic(nS, P, R, pi)\n",
        "\n",
        "    Id = np.identity(nS)\n",
        "    # discounted reward problem\n",
        "    A = Id - gamma * P_pi\n",
        "    b = R_pi\n",
        "    v_pi = np.linalg.solve(A, b)\n",
        "\n",
        "    return v_pi"
      ],
      "metadata": {
        "id": "StatdAcaJrif"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Policy Update\n",
        "\n",
        "Based on Q-function from $v^\\pi$, let's update our policy\n",
        "\n",
        "\\begin{equation*}\n",
        "\\pi'(s) = \\arg\\max_{a \\in \\mathcal{A}} Q^\\pi(s,a)\n",
        "\\end{equation*}"
      ],
      "metadata": {
        "id": "E6CVFEkIhXGr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 Policy Iteration Loop"
      ],
      "metadata": {
        "id": "vzzjm7MOh834"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5m7EZWzKO4t"
      },
      "source": [
        "nS, nA = R.shape\n",
        "\n",
        "# initialize policy\n",
        "pi = np.random.randint(nA, size=nS)\n",
        "\n",
        "count = 0\n",
        "start = time.time()\n",
        "while True:\n",
        "    v = eval_policy(nS, P, R, gamma, pi)\n",
        "    # TODO 7: Update your policy using v.\n",
        "    pi_next =\n",
        "    if (pi_next == \"\"\"blank\"\"\").all():\n",
        "        break\n",
        "    pi =\n",
        "    count += 1\n",
        "print('Iteration terminated in {} steps, Ellapsed time {:4f} sec\\n'.format(count, time.time() - start))\n",
        "\n",
        "print_result(v, pi)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example: Gridworld"
      ],
      "metadata": {
        "id": "azSRTOvVMEPr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gridworld import GridWorld, plot_heatmap\n",
        "\n",
        "P = GridWorld.P\n",
        "R = GridWorld.R\n",
        "\n",
        "gamma = 0.9"
      ],
      "metadata": {
        "id": "z38uVGhYLBOV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from utils import *\n",
        "import time"
      ],
      "metadata": {
        "id": "AOhlFSY9LBRo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Value Iteration**"
      ],
      "metadata": {
        "id": "Da4P2_84MVmR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "IEra4YGCMUfm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPS = 1e-6\n",
        "nS, nA = R.shape\n",
        "# initialize v\n",
        "v = np.zeros(shape=(nS, 1), dtype=float)\n",
        "\n",
        "begin = time.time()\n",
        "count = 0\n",
        "while True:\n",
        "    v_next = bellman_update(P, R, gamma, v)\n",
        "    if np.linalg.norm(v_next - v, ord=np.inf) < EPS:\n",
        "        break\n",
        "    v = v_next\n",
        "    count += 1\n",
        "pi = greedy(P, R, gamma, v)\n",
        "v_vi = v\n",
        "print('Value iteration terminated in {} steps.'.format(count))\n",
        "print('Elapsed time = ', time.time() - begin, 'sec')"
      ],
      "metadata": {
        "id": "z3FvOw9DLBVh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Policy Iteration**"
      ],
      "metadata": {
        "id": "-TAgvipbMavq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nS, nA = R.shape\n",
        "\n",
        "# initialize policy\n",
        "pi = np.random.randint(nA, size=nS)\n",
        "\n",
        "begin = time.time()\n",
        "count = 0\n",
        "while True:\n",
        "    v = eval_policy(nS, P, R, gamma, pi)\n",
        "    pi_next = greedy(P, R, gamma, v)\n",
        "    if (pi_next == pi).all():\n",
        "        break\n",
        "    pi = pi_next\n",
        "    count += 1\n",
        "print('Policy iteration terminated in {} steps.'.format(count))\n",
        "v_pi = v\n",
        "print('Elapsed time = ', time.time() - begin, 'sec')"
      ],
      "metadata": {
        "id": "mQ8cW4otLBhD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_heatmap(v_vi, v_pi)"
      ],
      "metadata": {
        "id": "E-IfPJNWLBko"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}