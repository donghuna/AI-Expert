{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/donghuna/AI-Expert/blob/main/%EC%96%91%EC%9D%B8%EC%88%9C/q_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Day 2. Q-learning"
      ],
      "metadata": {
        "id": "tBlBRHsWdFLY"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFoqTA-WPoz6"
      },
      "source": [
        "If you run in jupyter, turn\n",
        "\n",
        "```\n",
        "colab = False\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "apg1TJg_ODA1"
      },
      "outputs": [],
      "source": [
        "colab = True\n",
        "if colab:\n",
        "    !pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "    !apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "    !apt-get update > /dev/null 2>&1\n",
        "    !apt-get install cmake > /dev/null 2>&1\n",
        "    !pip install --upgrade setuptools 2>&1\n",
        "    !pip install ez_setup > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r2b4pUKfNSPK"
      },
      "outputs": [],
      "source": [
        "if colab:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    %cd /content/drive/MyDrive/day2_q_learning\n",
        "    !ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZ-kDzNINOPh"
      },
      "source": [
        "# Tabular Q-learning Practice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EQbv3yRcNOPj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from gym.envs.registration import register\n",
        "from discrete_pendulum import DiscretePendulumEnv\n",
        "register(\n",
        "    id=\"DiscretePendulum-v1\",\n",
        "    entry_point=\"discrete_pendulum:DiscretePendulumEnv\",\n",
        "    max_episode_steps=200,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CgkfEVcNOPk"
      },
      "source": [
        "# 0. Basic definitions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dm52tSrONOPk"
      },
      "source": [
        "## 0.0. Q-table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3gHbbASFNOPk"
      },
      "outputs": [],
      "source": [
        "class QTable:\n",
        "    def __init__(self, num_states, num_actions, gamma=0.99, pth=None):\n",
        "        self.gamma = gamma\n",
        "        if pth is None:\n",
        "            self.Q = -300. * np.ones(shape=(num_states, num_actions))\n",
        "        else:\n",
        "            self.Q = np.load(pth, allow_pickle=True)\n",
        "\n",
        "    def update(self, state, action, reward, next_state, alpha):\n",
        "        # update Q-table according to the following update rule:\n",
        "        # Q(s, a) <- Q(s, a) + alpha * (r + gamma * max_a' {Q(s', a')} - Q(s, a))\n",
        "        # TODO_1 : Implement update target of Q-function, r + gamma * max_a' {Q(s', a')} - Q(s, a).\n",
        "        target =\n",
        "        # TODO_2 : Build incremental update of Q-function.\n",
        "\n",
        "\n",
        "    def act(self, state):\n",
        "        # TODO_3 : Using Q-table, choose a greedy action.\n",
        "        return\n",
        "\n",
        "    def save(self, pth=None):\n",
        "        if pth is None:\n",
        "            pth = './table.npy'\n",
        "        np.save(pth, self.Q)\n",
        "\n",
        "    @property\n",
        "    def value_ftn(self):\n",
        "        return np.max(self.Q, axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loCG8x6RNOPl"
      },
      "source": [
        "## 0.1. Stepsize rule & Exploration Schedule"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BSgydQcENOPl"
      },
      "outputs": [],
      "source": [
        "class LinearExplorationSchedule:\n",
        "    def __init__(self, rollout_len, initial_epsilon=1., final_epsilon=0.02):\n",
        "        # linear exploration schedule\n",
        "        self.decrement = (initial_epsilon - final_epsilon) / rollout_len\n",
        "        self.initial_epsilon = initial_epsilon\n",
        "        self.final_epsilon = final_epsilon\n",
        "\n",
        "    def __call__(self, t):\n",
        "        # Define this as callable object so that the schedule is stateless.\n",
        "        return max(self.initial_epsilon - t * self.decrement, self.final_epsilon)\n",
        "\n",
        "# TODO_optional : try exponentially decaying schedule!\n",
        "class ExponentialExplorationSchedule:\n",
        "    def __init__(self, decay_rate, initial_epsilon=1., final_epsilon=0.02):\n",
        "        self.decay_rate = decay_rate\n",
        "        self.initial_epsilon = initial_epsilon\n",
        "        self.final_epsilon = final_epsilon\n",
        "\n",
        "    def __call__(self, t):\n",
        "        epsilon = self.decay_rate ** t * self.initial_epsilon\n",
        "        return max(epsilon, self.final_epsilon)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ag8alOj2NOPm"
      },
      "source": [
        "# 1. Q-learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "97_DojYsNOPm"
      },
      "outputs": [],
      "source": [
        "env = DiscretePendulumEnv()\n",
        "num_states = env.observation_space.n\n",
        "num_actions = env.action_space.n\n",
        "gamma = 0.99\n",
        "\n",
        "learner = QTable(num_states=env.observation_space.n, num_actions=env.action_space.n, gamma=gamma)\n",
        "rollout_len = 4000000\n",
        "\n",
        "\n",
        "epsilon_schedule = LinearExplorationSchedule(rollout_len, final_epsilon=0.5)\n",
        "# TODO_optional : alternative schedule\n",
        "# epsilon_schedule = ExponentialExplorationSchedule(decay_rate=0.99, final_epsilon=0.02)\n",
        "\n",
        "checkpoint_interval = rollout_len // 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OgmnuF0dNOPn"
      },
      "outputs": [],
      "source": [
        "class VisitCountStepsizeSchedule:\n",
        "    def __init__(self, deg=1.0):\n",
        "        # polynomial stepsize schedule : $\\Theta(N_t(s, a)^{-d})$\n",
        "        # $N_t(s, a)$ is the number of visits of (s, a)-pair until step t\n",
        "        # to satisfy Robbins-Monro condition, d must satisfy $d \\in (1/2, 1]$\n",
        "        assert .5 < deg <= 1\n",
        "        self.deg = deg\n",
        "\n",
        "    def __call__(self, n):\n",
        "        return 1. / ((n + 1.) ** self.deg)\n",
        "\n",
        "visit_count = np.zeros(shape=(num_states, num_actions))     # save visit counts N(s, a) of all state-action pairs\n",
        "alpha_schedule = VisitCountStepsizeSchedule(deg=0.5001)\n",
        "\n",
        "s = env.reset()\n",
        "for t in tqdm(range(rollout_len + 1)):\n",
        "    u = np.random.rand()    # sampling from uniform distribution [0, 1)\n",
        "    # TODO_4 : Implement epsilon-greedy algorithm (random action with probability epsilon, greedy action with probability 1 - epsilon).\n",
        "    if :\n",
        "        a =\n",
        "    else:\n",
        "        a =\n",
        "\n",
        "    # TODO_5 : Using the chosen action, get the next state and reward.\n",
        "\n",
        "\n",
        "    n = visit_count[s, a]\n",
        "    # TODO_6 : Update Q-table with QTable.update().\n",
        "\n",
        "\n",
        "    visit_count[s, a] += 1\n",
        "    s = s_next\n",
        "\n",
        "    if t % checkpoint_interval == 0:\n",
        "        learner.save()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDdUvKVlNOPo"
      },
      "source": [
        "# 2. Let's see if the Q-function is learned properly!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0E-fnx9bNOPo"
      },
      "source": [
        "## 2.0. Visualization"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "from gym.wrappers.record_video import RecordVideo\n",
        "import os\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode"
      ],
      "metadata": {
        "id": "i8QSBlhLrvG3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Install classical control environment\n",
        "!pip install gym[classic_control]"
      ],
      "metadata": {
        "id": "Mq2o9MKRCIL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YpdGqLaFNOPp"
      },
      "outputs": [],
      "source": [
        "ep_len = 400\n",
        "# test learned result!\n",
        "trajectory = np.zeros((ep_len, 2))      # store continuous states\n",
        "reward = 0.\n",
        "\n",
        "np.random.seed(2023)\n",
        "\n",
        "# Make directory of video record.\n",
        "os.makedirs('./video',exist_ok=True)\n",
        "\n",
        "# TODO_7 : Make gym environment with id 'DiscretePendulum-v1'.\n",
        "env =\n",
        "env = RecordVideo(env=env, video_folder='./video', name_prefix='pendulum_trained_control')\n",
        "\n",
        "s = env.reset(deterministic=True)\n",
        "\n",
        "env.start_video_recorder()\n",
        "for t in range(ep_len):\n",
        "    trajectory[t] = np.copy(env.x)\n",
        "    # TODO_8 : Execute only greedy action at test time.\n",
        "    a =\n",
        "    s, r, _, _ = env.step(a)\n",
        "    # TODO_9 : Compute cumulative reward = sum_t {gamma^t * r}.\n",
        "\n",
        "print('total reward =', reward)\n",
        "\n",
        "# Close environment and video record.\n",
        "env.close_video_recorder()\n",
        "\n",
        "mp4 = open('./video/pendulum_trained_control-episode-0.mp4','rb').read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "HTML(\"\"\"\n",
        "<video width=400 controls>\n",
        "      <source src=\"%s\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\" % data_url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vro4h79fNOPp"
      },
      "source": [
        "## 2.1. Trajectory Plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kltxc7mZNOPp",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(2, 1, figsize=(8, 10), sharex=True)\n",
        "x = env.dt * np.arange(ep_len)\n",
        "ylabels = [r'$\\theta$ (rad)', r'$\\dot\\theta$ (rad/s)']\n",
        "ax[1].set_xlabel(r'$t$ (s)', fontsize=20)\n",
        "ax[0].set_ylim(-np.pi, np.pi)\n",
        "ax[1].set_ylim(-8., 8.)\n",
        "ax[0].set_yticks([-np.pi, -np.pi / 2, 0, np.pi / 2, np.pi])\n",
        "ax[0].set_yticklabels([r'$-\\pi$', r'$-\\pi/2$', r'$0$', r'$\\pi/2$', r'$\\pi$'])\n",
        "ax[1].set_yticks([-8, -4, 0, 4, 8])\n",
        "for i in range(2):\n",
        "    ax[i].plot(x, trajectory[:, i])\n",
        "    ax[i].set_xlim(0, x[-1])\n",
        "    ax[i].grid(True)\n",
        "    ax[i].set_ylabel(ylabels[i], fontsize=20)\n",
        "    ax[i].tick_params(axis='both', which='major', labelsize=18)\n",
        "fig.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHpvCABNNOPq"
      },
      "source": [
        "# 2.2. Q-learning Result vs Optimal Solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kgBIGzVfNOPq"
      },
      "outputs": [],
      "source": [
        "optimal_agent = QTable(num_states=env.observation_space.n, num_actions=env.action_space.n, pth='./optimum.npy')\n",
        "\n",
        "env = gym.make('DiscretePendulum-v1')\n",
        "env = RecordVideo(env=env, video_folder='./video', name_prefix='pendulum_optimal_control')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_cetZFcNOPq"
      },
      "outputs": [],
      "source": [
        "ep_len = 400\n",
        "# test learned result!\n",
        "trajectory = np.zeros((ep_len, 2))      # store continuous states\n",
        "reward = 0.\n",
        "\n",
        "s = env.reset(deterministic=True)\n",
        "env.start_video_recorder()\n",
        "for t in range(ep_len):\n",
        "    trajectory[t] = np.copy(env.x)\n",
        "    a = optimal_agent.act(s)\n",
        "    s, r, _, _ = env.step(a)\n",
        "    reward += (gamma ** t) * r\n",
        "\n",
        "print('total reward =', reward)\n",
        "env.close_video_recorder()\n",
        "\n",
        "mp4 = open('./video/pendulum_optimal_control-episode-0.mp4','rb').read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "HTML(\"\"\"\n",
        "<video width=400 controls>\n",
        "      <source src=\"%s\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\" % data_url)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}